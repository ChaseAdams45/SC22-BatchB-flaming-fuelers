{"backend_state":"running","connection_file":"/projects/49811120-694c-43f1-9267-605bd2af9ca9/.local/share/jupyter/runtime/kernel-cb8b60cb-f1bf-4201-b3f6-f43f8a3a2dc3.json","kernel":"cv_env","kernel_error":"","kernel_state":"idle","kernel_usage":{"cpu":0,"memory":0},"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"CV_Classification_skeletonized.ipynb","provenance":[]},"interpreter":{"hash":"f92c92cb1bbf690c4549d4feaa58ef087288670a9e11f4f2bc1ea394ea8a7720"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"},"widgets":{"application/vnd.jupyter.widget-state+json":{"09985f7b880f47509081c7b8f6e797a6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d71d754be2be425ca24b194a2aea125c","placeholder":"​","style":"IPY_MODEL_f6363bc40f524f2ea0ecb199cd4ad414","value":""}},"0c32afe2a35c48dc81e26f43832e5528":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_80c324bd3a774d9fb3474fea9a2f37a1","IPY_MODEL_d994ef5592444f078eb0b38adbb24176","IPY_MODEL_bd1bcd5f50a248a896285f4a8e154c6c"],"layout":"IPY_MODEL_86fbff3d1b4f4fe98ef4f5713c5c6014"}},"0dd8ab643bd24b2e949e4e3b4096d4be":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"14bbc869fe8d452692a6f7cdb38b075b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"161da13e65934d7f9ee9e5cd9f35a46f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"20182c64d4de4aff830441867cfd9d78":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2cdcdd80797f42659fb44a03e3bbd269":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"35a8bff49c89404ba666f0181606edad":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"37a3453775974ecd9176a3c96dde77df":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3e3ac696c1ae4030970a291d0b55a9c3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"499b378fe04d48aa937c21de79966c4b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a41d6fc422ee47e189be9fde8a0fa2ff","max":5148,"min":0,"orientation":"horizontal","style":"IPY_MODEL_20182c64d4de4aff830441867cfd9d78","value":5148}},"4b5790f98e3b44ec915349b854c9064f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"52dd7ed840c244059a7d69252fb20a0f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_76b8caf8fcbb48779f5dea6d2329ff4d","placeholder":"​","style":"IPY_MODEL_37a3453775974ecd9176a3c96dde77df","value":" 4422656/? [00:01&lt;00:00, 3960059.28it/s]"}},"53e1b8f46e04430c8e19fb66ec674d28":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"58493c17d71b43fc876c0cc4a3093120":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5f4d3dce10394194943134f856a40b5e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"63bba93a02c141d18b53328db8173d67":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"67167b99eabd407d8a7900319b48c67f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2cdcdd80797f42659fb44a03e3bbd269","max":4422102,"min":0,"orientation":"horizontal","style":"IPY_MODEL_71df07ea74d045ea93bba3d60dcb800c","value":4422102}},"683e7c41d782493ba3cb93d0c288ade1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"68fbbf59d93340c7be0b57869ac17b15":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"701e7e3c57174353aa7190b10b10da27":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d4b6729efbed4c48aa19ae019aa1ee43","placeholder":"​","style":"IPY_MODEL_c1528e5e1be64a93928ad41707b0dec0","value":" 6144/? [00:00&lt;00:00, 198807.33it/s]"}},"7151b8796109496f9a0213d2ca556296":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_58493c17d71b43fc876c0cc4a3093120","max":29515,"min":0,"orientation":"horizontal","style":"IPY_MODEL_161da13e65934d7f9ee9e5cd9f35a46f","value":29515}},"71df07ea74d045ea93bba3d60dcb800c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7280aea30eff49bf94a4217647cd70f2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fd2449e472514181a355fc1c315acfa2","placeholder":"​","style":"IPY_MODEL_b37cbc32dcbc49aca73006ba0f900a17","value":""}},"76b8caf8fcbb48779f5dea6d2329ff4d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"80c324bd3a774d9fb3474fea9a2f37a1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_35a8bff49c89404ba666f0181606edad","placeholder":"​","style":"IPY_MODEL_3e3ac696c1ae4030970a291d0b55a9c3","value":""}},"822c855d65d64f0a9112308d2389ff7c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_63bba93a02c141d18b53328db8173d67","placeholder":"​","style":"IPY_MODEL_f90246acab4b45f3b540772689ac3657","value":""}},"86fbff3d1b4f4fe98ef4f5713c5c6014":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9ddf7176f4d54ae4bca347af22041b7b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_822c855d65d64f0a9112308d2389ff7c","IPY_MODEL_67167b99eabd407d8a7900319b48c67f","IPY_MODEL_52dd7ed840c244059a7d69252fb20a0f"],"layout":"IPY_MODEL_14bbc869fe8d452692a6f7cdb38b075b"}},"9e48ad9913664c5f9f53f3a815ba9c85":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_09985f7b880f47509081c7b8f6e797a6","IPY_MODEL_499b378fe04d48aa937c21de79966c4b","IPY_MODEL_701e7e3c57174353aa7190b10b10da27"],"layout":"IPY_MODEL_53e1b8f46e04430c8e19fb66ec674d28"}},"a0acaa8f8ff04fa4aa8efef789ea6094":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a41d6fc422ee47e189be9fde8a0fa2ff":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b37cbc32dcbc49aca73006ba0f900a17":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bd1bcd5f50a248a896285f4a8e154c6c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_68fbbf59d93340c7be0b57869ac17b15","placeholder":"​","style":"IPY_MODEL_fcbfca25f1a14e15b0fa0f6088bf40f1","value":" 26422272/? [00:03&lt;00:00, 11817352.68it/s]"}},"c1528e5e1be64a93928ad41707b0dec0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d4b6729efbed4c48aa19ae019aa1ee43":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d58ad2180cae45eab7addb2417b50a03":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7280aea30eff49bf94a4217647cd70f2","IPY_MODEL_7151b8796109496f9a0213d2ca556296","IPY_MODEL_d841001138284dbe8628700df81bc404"],"layout":"IPY_MODEL_683e7c41d782493ba3cb93d0c288ade1"}},"d71d754be2be425ca24b194a2aea125c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d841001138284dbe8628700df81bc404":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a0acaa8f8ff04fa4aa8efef789ea6094","placeholder":"​","style":"IPY_MODEL_4b5790f98e3b44ec915349b854c9064f","value":" 29696/? [00:00&lt;00:00, 72057.25it/s]"}},"d994ef5592444f078eb0b38adbb24176":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0dd8ab643bd24b2e949e4e3b4096d4be","max":26421880,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5f4d3dce10394194943134f856a40b5e","value":26421880}},"f6363bc40f524f2ea0ecb199cd4ad414":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f90246acab4b45f3b540772689ac3657":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fcbfca25f1a14e15b0fa0f6088bf40f1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fd2449e472514181a355fc1c315acfa2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"trust":true,"type":"settings"}
{"cell_type":"code","end":1657048870008,"exec_count":1,"id":"0b0011","input":"\n%pip install torch torchvision\nimport torch\nimport torchvision\nimport torch.optim as optim\nimport torchvision.transforms as transforms\nimport torch.nn as nn\nimport torch.nn.functional as F\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Convert Data to Tensors and normalize\ntransform = transforms.Compose(\n    [transforms.ToTensor(),\n     transforms.Normalize((0.5), (0.5))])\n\n# hyperparameters\ninput_size = 28 * 28 # 28 * 28 images\nnum_classes = 10 # 10 possible classifications/outputs for the model\nnum_epochs = 10 # 10 iterations over the dataset\nbatch_size = 8 # 8 images passed at a time into the model\n\n#the names of all the possible classifications\nclasses = (\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n           \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\")\n\n# training set\ntrain_dataset = torchvision.datasets.FashionMNIST(\n    root = \"./data\",\n    train = True,\n    transform = transform,\n    download = True\n)\n\n# EXERCISE: do the same thing for the testing dataset and call it test_dataset\n\ntest_dataset = torchvision.datasets.FashionMNIST(\n    root = \"./data\",\n    train = False,\n    transform = transform,\n    download = True\n)\n\n# Get batched Dataloaders\ntrainloader = torch.utils.data.DataLoader(\n    dataset = train_dataset,\n    batch_size = batch_size,\n    shuffle = True # optional\n)\n\n#EXERCISE: Create a dataloader for the testing dataset called testloader\ntestloader = torch.utils.data.DataLoader(\n    dataset = test_dataset,\n    batch_size = batch_size,\n    shuffle = True # optional\n)","kernel":"cv_env","metadata":{"id":"UA-UYjkpbYT3"},"output":{"0":{"name":"stdout","text":"Requirement already satisfied: torch in /projects/49811120-694c-43f1-9267-605bd2af9ca9/miniconda3/envs/cv_env/lib/python3.8/site-packages (1.11.0)\r\nRequirement already satisfied: torchvision in /projects/49811120-694c-43f1-9267-605bd2af9ca9/miniconda3/envs/cv_env/lib/python3.8/site-packages (0.12.0)\r\nRequirement already satisfied: typing-extensions in /projects/49811120-694c-43f1-9267-605bd2af9ca9/miniconda3/envs/cv_env/lib/python3.8/site-packages (from torch) (4.2.0)\r\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /projects/49811120-694c-43f1-9267-605bd2af9ca9/miniconda3/envs/cv_env/lib/python3.8/site-packages (from torchvision) (9.1.1)\r\nRequirement already satisfied: requests in /projects/49811120-694c-43f1-9267-605bd2af9ca9/miniconda3/envs/cv_env/lib/python3.8/site-packages (from torchvision) (2.27.1)\r\nRequirement already satisfied: numpy in /projects/49811120-694c-43f1-9267-605bd2af9ca9/miniconda3/envs/cv_env/lib/python3.8/site-packages (from torchvision) (1.22.3)\r\n"},"1":{"name":"stdout","text":"Requirement already satisfied: charset-normalizer~=2.0.0 in /projects/49811120-694c-43f1-9267-605bd2af9ca9/miniconda3/envs/cv_env/lib/python3.8/site-packages (from requests->torchvision) (2.0.12)\r\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /projects/49811120-694c-43f1-9267-605bd2af9ca9/miniconda3/envs/cv_env/lib/python3.8/site-packages (from requests->torchvision) (1.26.9)\r\nRequirement already satisfied: idna<4,>=2.5 in /projects/49811120-694c-43f1-9267-605bd2af9ca9/miniconda3/envs/cv_env/lib/python3.8/site-packages (from requests->torchvision) (3.3)\r\nRequirement already satisfied: certifi>=2017.4.17 in /projects/49811120-694c-43f1-9267-605bd2af9ca9/miniconda3/envs/cv_env/lib/python3.8/site-packages (from requests->torchvision) (2021.10.8)\r\n"},"2":{"name":"stdout","text":"Note: you may need to restart the kernel to use updated packages.\n"}},"pos":15,"start":1657048866891,"state":"done","type":"cell"}
{"cell_type":"code","end":1657049026170,"exec_count":2,"id":"0ceecc","input":"\n%pip install torch torchvision\nimport torch\nimport torchvision\nimport torch.optim as optim\nimport torchvision.transforms as transforms\nimport torch.nn as nn\nimport torch.nn.functional as F\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Convert Data to Tensors and normalize\ntransform = transforms.Compose(\n    [transforms.ToTensor(),\n     transforms.Normalize((0.5), (0.5))])\n\n# hyperparameters\ninput_size = 28 * 28 # 28 * 28 images\nnum_classes = 10 # 10 possible classifications/outputs for the model\nnum_epochs = 10 # 10 iterations over the dataset\nbatch_size = 8 # 8 images passed at a time into the model\n\n#the names of all the possible classifications\nclasses = (\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n           \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\")\n\n# training set\ntrain_dataset = torchvision.datasets.FashionMNIST(\n    root = \"./data\",\n    train = True,\n    transform = transform,\n    download = True\n)\n\n# EXERCISE: do the same thing for the testing dataset and call it test_dataset\n\ntest_dataset = torchvision.datasets.FashionMNIST(\n    root = \"./data\",\n    train = False,\n    transform = transform,\n    download = True\n)\n\n# Get batched Dataloaders\ntrainloader = torch.utils.data.DataLoader(\n    dataset = train_dataset,\n    batch_size = batch_size,\n    shuffle = True # optional\n)\n\n#EXERCISE: Create a dataloader for the testing dataset called testloader\ntestloader = torch.utils.data.DataLoader(\n    dataset = test_dataset,\n    batch_size = batch_size,\n    shuffle = True # optional\n)\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n#imshow wrapper function to display image\ndef imshow(img):\n    # EXERCISE: reverse normalization transform. Go up and see which operations were performed and do the opposite\n\n    img = img/2 + 0.5\n    npimg = img.numpy() #turn the image tensor into a numpy array\n    plt.imshow(np.transpose(npimg, (1, 2, 0))) #transpose the array to  3x28x28 instead of 28x28x3\n    plt.show()\n\n\n# get some random training images\ndataiter = iter(trainloader)\nimages, labels = dataiter.next() #get the next batch of images from the iterator dataloader\n\n# show images\nimshow(torchvision.utils.make_grid(images)) #turn the batch of images into one image grid\n# print labels\nprint(' '.join(f'{classes[labels[j]]:5s}' for j in range(batch_size))) #print out all the classifications for each image","kernel":"cv_env","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":112},"id":"s2DfOJeRLbwk","outputId":"02498f6a-0dc8-41bc-80ad-84d0fa6fe42e"},"output":{"0":{"name":"stdout","text":"Requirement already satisfied: torch in /projects/49811120-694c-43f1-9267-605bd2af9ca9/miniconda3/envs/cv_env/lib/python3.8/site-packages (1.11.0)\r\nRequirement already satisfied: torchvision in /projects/49811120-694c-43f1-9267-605bd2af9ca9/miniconda3/envs/cv_env/lib/python3.8/site-packages (0.12.0)\r\nRequirement already satisfied: typing-extensions in /projects/49811120-694c-43f1-9267-605bd2af9ca9/miniconda3/envs/cv_env/lib/python3.8/site-packages (from torch) (4.2.0)\r\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /projects/49811120-694c-43f1-9267-605bd2af9ca9/miniconda3/envs/cv_env/lib/python3.8/site-packages (from torchvision) (9.1.1)\r\nRequirement already satisfied: numpy in /projects/49811120-694c-43f1-9267-605bd2af9ca9/miniconda3/envs/cv_env/lib/python3.8/site-packages (from torchvision) (1.22.3)\r\nRequirement already satisfied: requests in /projects/49811120-694c-43f1-9267-605bd2af9ca9/miniconda3/envs/cv_env/lib/python3.8/site-packages (from torchvision) (2.27.1)\r\n"},"1":{"name":"stdout","text":"Requirement already satisfied: urllib3<1.27,>=1.21.1 in /projects/49811120-694c-43f1-9267-605bd2af9ca9/miniconda3/envs/cv_env/lib/python3.8/site-packages (from requests->torchvision) (1.26.9)\r\nRequirement already satisfied: idna<4,>=2.5 in /projects/49811120-694c-43f1-9267-605bd2af9ca9/miniconda3/envs/cv_env/lib/python3.8/site-packages (from requests->torchvision) (3.3)\r\nRequirement already satisfied: certifi>=2017.4.17 in /projects/49811120-694c-43f1-9267-605bd2af9ca9/miniconda3/envs/cv_env/lib/python3.8/site-packages (from requests->torchvision) (2021.10.8)\r\nRequirement already satisfied: charset-normalizer~=2.0.0 in /projects/49811120-694c-43f1-9267-605bd2af9ca9/miniconda3/envs/cv_env/lib/python3.8/site-packages (from requests->torchvision) (2.0.12)\r\n"},"2":{"name":"stdout","text":"Note: you may need to restart the kernel to use updated packages.\n"},"3":{"data":{"image/png":"d352ded61164bdc10f86cd4891baacb027ddd017","text/plain":"<Figure size 432x288 with 1 Axes>"},"metadata":{"needs_background":"light"}},"4":{"name":"stdout","text":"Dress Pullover Trouser Pullover Trouser Ankle boot Trouser Dress\n"}},"pos":19,"start":1657049022543,"state":"done","type":"cell"}
{"cell_type":"code","end":1657050157326,"exec_count":3,"id":"4f5f37","input":"\n%pip install torch torchvision\nimport torch\nimport torchvision\nimport torch.optim as optim\nimport torchvision.transforms as transforms\nimport torch.nn as nn\nimport torch.nn.functional as F\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Convert Data to Tensors and normalize\ntransform = transforms.Compose(\n    [transforms.ToTensor(),\n     transforms.Normalize((0.5), (0.5))])\n\n# hyperparameters\ninput_size = 28 * 28 # 28 * 28 images\nnum_classes = 10 # 10 possible classifications/outputs for the model\nnum_epochs = 10 # 10 iterations over the dataset\nbatch_size = 8 # 8 images passed at a time into the model\n\n#the names of all the possible classifications\nclasses = (\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n           \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\")\n\n# training set\ntrain_dataset = torchvision.datasets.FashionMNIST(\n    root = \"./data\",\n    train = True,\n    transform = transform,\n    download = True\n)\n\n# EXERCISE: do the same thing for the testing dataset and call it test_dataset\n\ntest_dataset = torchvision.datasets.FashionMNIST(\n    root = \"./data\",\n    train = False,\n    transform = transform,\n    download = True\n)\n\n# Get batched Dataloaders\ntrainloader = torch.utils.data.DataLoader(\n    dataset = train_dataset,\n    batch_size = batch_size,\n    shuffle = True # optional\n)\n\n#EXERCISE: Create a dataloader for the testing dataset called testloader\ntestloader = torch.utils.data.DataLoader(\n    dataset = test_dataset,\n    batch_size = batch_size,\n    shuffle = True # optional\n)\n\nclass CNN(nn.Module):\n  def __init__(self):\n    super(CNN, self).__init__() \n    self.conv_layer_1 = nn.Sequential( # input.shape: [batch, 1, 28, 28]\n        nn.Conv2d(\n            in_channels=1, # grey scale, 3 otherwise if RGB   \n            out_channels=16, # 16 x 1(grey scale, 3 otherwise if RGB ) filters\n            kernel_size=5, # convolution extraction size\n            stride=1,\n            padding=2 # for convoluted output to be same size,\n                      # need padding=(kernel_size-1)/2 if stride = 1\n        ),\n        nn.ReLU(),\n        nn.MaxPool2d(kernel_size=2) # (2x2 maxpool), out.shape: [16, 14, 14]\n    )\n    #EXERCISE: Code second convolutional group\n    \n    \n    self.conv_layer_2 = nn.Sequential( # input.shape: [batch, 1, 28, 28]\n        nn.Conv2d(\n            in_channels=16, # grey scale, 3 otherwise if RGB   \n            out_channels=32, # 16 x 1(grey scale, 3 otherwise if RGB ) filters\n            kernel_size=5, # convolution extraction size\n            stride=1,\n            padding=2 # for convoluted output to be same size,\n                      # need padding=(kernel_size-1)/2 if stride = 1\n        ),\n        nn.ReLU(),\n        nn.MaxPool2d(kernel_size=2) # (2x2 maxpool), out.shape: [16, 14, 14]\n    )\n    \n    self.fc = nn.Linear(32*7*7, 10)\n\n  def forward(self, x):\n    # x.shape: [batch_size, 1, 28, 28] -> [batch_size, 16, 14, 14]\n    x = self.conv_layer_1(x)\n    # x.shape: [batch_size, 16, 14, 14] -> [batch_size, 32, 7, 7]\n    x = self.conv_layer_2(x)\n    x = x.view(x.shape[0], -1) # flatten: [batch_size, 32*7*7]\n    # x.shape: [batch_size, 32*7*7] -> [batch_size, 10]\n    out = self.fc(x)\n    return out","kernel":"cv_env","metadata":{"id":"UslclwUyIU6p"},"output":{"0":{"name":"stdout","text":"Requirement already satisfied: torch in /projects/49811120-694c-43f1-9267-605bd2af9ca9/miniconda3/envs/cv_env/lib/python3.8/site-packages (1.11.0)\r\nRequirement already satisfied: torchvision in /projects/49811120-694c-43f1-9267-605bd2af9ca9/miniconda3/envs/cv_env/lib/python3.8/site-packages (0.12.0)\r\nRequirement already satisfied: typing-extensions in /projects/49811120-694c-43f1-9267-605bd2af9ca9/miniconda3/envs/cv_env/lib/python3.8/site-packages (from torch) (4.2.0)\r\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /projects/49811120-694c-43f1-9267-605bd2af9ca9/miniconda3/envs/cv_env/lib/python3.8/site-packages (from torchvision) (9.1.1)\r\nRequirement already satisfied: numpy in /projects/49811120-694c-43f1-9267-605bd2af9ca9/miniconda3/envs/cv_env/lib/python3.8/site-packages (from torchvision) (1.22.3)\r\nRequirement already satisfied: requests in /projects/49811120-694c-43f1-9267-605bd2af9ca9/miniconda3/envs/cv_env/lib/python3.8/site-packages (from torchvision) (2.27.1)\r\n"},"1":{"name":"stdout","text":"Requirement already satisfied: charset-normalizer~=2.0.0 in /projects/49811120-694c-43f1-9267-605bd2af9ca9/miniconda3/envs/cv_env/lib/python3.8/site-packages (from requests->torchvision) (2.0.12)\r\nRequirement already satisfied: idna<4,>=2.5 in /projects/49811120-694c-43f1-9267-605bd2af9ca9/miniconda3/envs/cv_env/lib/python3.8/site-packages (from requests->torchvision) (3.3)\r\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /projects/49811120-694c-43f1-9267-605bd2af9ca9/miniconda3/envs/cv_env/lib/python3.8/site-packages (from requests->torchvision) (1.26.9)\r\nRequirement already satisfied: certifi>=2017.4.17 in /projects/49811120-694c-43f1-9267-605bd2af9ca9/miniconda3/envs/cv_env/lib/python3.8/site-packages (from requests->torchvision) (2021.10.8)\r\n"},"2":{"name":"stdout","text":"Note: you may need to restart the kernel to use updated packages.\n"}},"pos":26,"start":1657050155270,"state":"done","type":"cell"}
{"cell_type":"code","end":1657051233073,"exec_count":1,"id":"5262d6","input":"%pip install torch torchvision\nimport torch\nimport torchvision\nimport torch.optim as optim\nimport torchvision.transforms as transforms\nimport torch.nn as nn\nimport torch.nn.functional as F\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Convert Data to Tensors and normalize\ntransform = transforms.Compose(\n    [transforms.ToTensor(),\n     transforms.Normalize((0.5), (0.5))])\n\n# hyperparameters\ninput_size = 28 * 28 # 28 * 28 images\nnum_classes = 10 # 10 possible classifications/outputs for the model\nnum_epochs = 10 # 10 iterations over the dataset\nbatch_size = 8 # 8 images passed at a time into the model\n\n#the names of all the possible classifications\nclasses = (\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n           \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\")\n\n# training set\ntrain_dataset = torchvision.datasets.FashionMNIST(\n    root = \"./data\",\n    train = True,\n    transform = transform,\n    download = True\n)\n\n# EXERCISE: do the same thing for the testing dataset and call it test_dataset\n\ntest_dataset = torchvision.datasets.FashionMNIST(\n    root = \"./data\",\n    train = False,\n    transform = transform,\n    download = True\n)\n\n# Get batched Dataloaders\ntrainloader = torch.utils.data.DataLoader(\n    dataset = train_dataset,\n    batch_size = batch_size,\n    shuffle = True # optional\n)\n\n#EXERCISE: Create a dataloader for the testing dataset called testloader\ntestloader = torch.utils.data.DataLoader(\n    dataset = test_dataset,\n    batch_size = batch_size,\n    shuffle = True # optional\n)\n\nclass CNN(nn.Module):\n  def __init__(self):\n    super(CNN, self).__init__() \n    self.conv_layer_1 = nn.Sequential( # input.shape: [batch, 1, 28, 28]\n        nn.Conv2d(\n            in_channels=1, # grey scale, 3 otherwise if RGB   \n            out_channels=16, # 16 x 1(grey scale, 3 otherwise if RGB ) filters\n            kernel_size=5, # convolution extraction size\n            stride=1,\n            padding=2 # for convoluted output to be same size,\n                      # need padding=(kernel_size-1)/2 if stride = 1\n        ),\n        nn.ReLU(),\n        nn.MaxPool2d(kernel_size=2) # (2x2 maxpool), out.shape: [16, 14, 14]\n    )\n    #EXERCISE: Code second convolutional group\n    \n    \n    self.conv_layer_2 = nn.Sequential( # input.shape: [batch, 1, 28, 28]\n        nn.Conv2d(\n            in_channels=16, # grey scale, 3 otherwise if RGB   \n            out_channels=32, # 16 x 1(grey scale, 3 otherwise if RGB ) filters\n            kernel_size=5, # convolution extraction size\n            stride=1,\n            padding=2 # for convoluted output to be same size,\n                      # need padding=(kernel_size-1)/2 if stride = 1\n        ),\n        nn.ReLU(),\n        nn.MaxPool2d(kernel_size=2) # (2x2 maxpool), out.shape: [16, 14, 14]\n    )\n    \n    self.fc = nn.Linear(32*7*7, 10)\n\n  def forward(self, x):\n    # x.shape: [batch_size, 1, 28, 28] -> [batch_size, 16, 14, 14]\n    x = self.conv_layer_1(x)\n    # x.shape: [batch_size, 16, 14, 14] -> [batch_size, 32, 7, 7]\n    x = self.conv_layer_2(x)\n    x = x.view(x.shape[0], -1) # flatten: [batch_size, 32*7*7]\n    # x.shape: [batch_size, 32*7*7] -> [batch_size, 10]\n    out = self.fc(x)\n    return out\n\nmodel = CNN()\nmodel.to(device) # cast model to device\n\ncriterion = nn.CrossEntropyLoss()\n#EXERCISE: transfer the loss function over to the GPU. \n\ncriterion.to(device)\noptimizer = optim.Adam(model.parameters(), lr=0.001)","kernel":"cv_env","metadata":{"id":"83YcVBTMNkE0"},"output":{"0":{"name":"stdout","text":"Requirement already satisfied: torch in /projects/49811120-694c-43f1-9267-605bd2af9ca9/miniconda3/envs/cv_env/lib/python3.8/site-packages (1.11.0)\r\nRequirement already satisfied: torchvision in /projects/49811120-694c-43f1-9267-605bd2af9ca9/miniconda3/envs/cv_env/lib/python3.8/site-packages (0.12.0)\r\nRequirement already satisfied: typing-extensions in /projects/49811120-694c-43f1-9267-605bd2af9ca9/miniconda3/envs/cv_env/lib/python3.8/site-packages (from torch) (4.2.0)\r\nRequirement already satisfied: requests in /projects/49811120-694c-43f1-9267-605bd2af9ca9/miniconda3/envs/cv_env/lib/python3.8/site-packages (from torchvision) (2.27.1)\r\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /projects/49811120-694c-43f1-9267-605bd2af9ca9/miniconda3/envs/cv_env/lib/python3.8/site-packages (from torchvision) (9.1.1)\r\nRequirement already satisfied: numpy in /projects/49811120-694c-43f1-9267-605bd2af9ca9/miniconda3/envs/cv_env/lib/python3.8/site-packages (from torchvision) (1.22.3)\r\n"},"1":{"name":"stdout","text":"Requirement already satisfied: charset-normalizer~=2.0.0 in /projects/49811120-694c-43f1-9267-605bd2af9ca9/miniconda3/envs/cv_env/lib/python3.8/site-packages (from requests->torchvision) (2.0.12)\r\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /projects/49811120-694c-43f1-9267-605bd2af9ca9/miniconda3/envs/cv_env/lib/python3.8/site-packages (from requests->torchvision) (1.26.9)\r\nRequirement already satisfied: certifi>=2017.4.17 in /projects/49811120-694c-43f1-9267-605bd2af9ca9/miniconda3/envs/cv_env/lib/python3.8/site-packages (from requests->torchvision) (2021.10.8)\r\nRequirement already satisfied: idna<4,>=2.5 in /projects/49811120-694c-43f1-9267-605bd2af9ca9/miniconda3/envs/cv_env/lib/python3.8/site-packages (from requests->torchvision) (3.3)\r\n"},"2":{"name":"stdout","text":"Note: you may need to restart the kernel to use updated packages.\n"}},"pos":29,"start":1657051229958,"state":"done","type":"cell"}
{"cell_type":"code","end":1657051571506,"exec_count":2,"id":"7d3f85","input":"\n%pip install torch torchvision\nimport torch\nimport torchvision\nimport torch.optim as optim\nimport torchvision.transforms as transforms\nimport torch.nn as nn\nimport torch.nn.functional as F\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Convert Data to Tensors and normalize\ntransform = transforms.Compose(\n    [transforms.ToTensor(),\n     transforms.Normalize((0.5), (0.5))])\n\n# hyperparameters\ninput_size = 28 * 28 # 28 * 28 images\nnum_classes = 10 # 10 possible classifications/outputs for the model\nnum_epochs = 10 # 10 iterations over the dataset\nbatch_size = 8 # 8 images passed at a time into the model\n\n#the names of all the possible classifications\nclasses = (\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n           \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\")\n\n# training set\ntrain_dataset = torchvision.datasets.FashionMNIST(\n    root = \"./data\",\n    train = True,\n    transform = transform,\n    download = True\n)\n\n# EXERCISE: do the same thing for the testing dataset and call it test_dataset\n\ntest_dataset = torchvision.datasets.FashionMNIST(\n    root = \"./data\",\n    train = False,\n    transform = transform,\n    download = True\n)\n\n# Get batched Dataloaders\ntrainloader = torch.utils.data.DataLoader(\n    dataset = train_dataset,\n    batch_size = batch_size,\n    shuffle = True # optional\n)\n\n#EXERCISE: Create a dataloader for the testing dataset called testloader\ntestloader = torch.utils.data.DataLoader(\n    dataset = test_dataset,\n    batch_size = batch_size,\n    shuffle = True # optional\n)\n\nclass CNN(nn.Module):\n  def __init__(self):\n    super(CNN, self).__init__() \n    self.conv_layer_1 = nn.Sequential( # input.shape: [batch, 1, 28, 28]\n        nn.Conv2d(\n            in_channels=1, # grey scale, 3 otherwise if RGB   \n            out_channels=16, # 16 x 1(grey scale, 3 otherwise if RGB ) filters\n            kernel_size=5, # convolution extraction size\n            stride=1,\n            padding=2 # for convoluted output to be same size,\n                      # need padding=(kernel_size-1)/2 if stride = 1\n        ),\n        nn.ReLU(),\n        nn.MaxPool2d(kernel_size=2) # (2x2 maxpool), out.shape: [16, 14, 14]\n    )\n    #EXERCISE: Code second convolutional group\n    \n    \n    self.conv_layer_2 = nn.Sequential( # input.shape: [batch, 1, 28, 28]\n        nn.Conv2d(\n            in_channels=16, # grey scale, 3 otherwise if RGB   \n            out_channels=32, # 16 x 1(grey scale, 3 otherwise if RGB ) filters\n            kernel_size=5, # convolution extraction size\n            stride=1,\n            padding=2 # for convoluted output to be same size,\n                      # need padding=(kernel_size-1)/2 if stride = 1\n        ),\n        nn.ReLU(),\n        nn.MaxPool2d(kernel_size=2) # (2x2 maxpool), out.shape: [16, 14, 14]\n    )\n    \n    self.fc = nn.Linear(32*7*7, 10)\n\n  def forward(self, x):\n    # x.shape: [batch_size, 1, 28, 28] -> [batch_size, 16, 14, 14]\n    x = self.conv_layer_1(x)\n    # x.shape: [batch_size, 16, 14, 14] -> [batch_size, 32, 7, 7]\n    x = self.conv_layer_2(x)\n    x = x.view(x.shape[0], -1) # flatten: [batch_size, 32*7*7]\n    # x.shape: [batch_size, 32*7*7] -> [batch_size, 10]\n    out = self.fc(x)\n    return out\n\nmodel = CNN()\nmodel.to(device) # cast model to device\n\ncriterion = nn.CrossEntropyLoss()\n#EXERCISE: transfer the loss function over to the GPU. \n\ncriterion.to(device)\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n#-----------\nfor epoch in range(3):  # loop over the dataset multiple times\n    running_loss = 0.0\n    for i, data in enumerate(trainloader):\n        # get the inputs; data is a list of [inputs, labels]\n        inputs, labels = data\n        inputs = inputs.to(device) # put input image onto gpu\n        labels = labels.to(device) # put label onto gpu\n\n        # zero the parameter gradients\n        optimizer.zero_grad()\n\n        # forward + backward + optimize\n        outputs = model(inputs)\n        #EXERCISE: Calculate \"loss\" by passing the outputs and labels into the loss function\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        # print statistics\n        running_loss += loss.item()\n        if i % 3750 == 3749:    # print every 3750 mini-batches\n            #EXERCISE: print and reset total loss\n\n            print('Finished Training')","kernel":"cv_env","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Jm4C52vBNxBi","outputId":"7c1ac7da-1d31-483c-ff84-63eccb56d49b"},"output":{"0":{"name":"stdout","text":"Requirement already satisfied: torch in /projects/49811120-694c-43f1-9267-605bd2af9ca9/miniconda3/envs/cv_env/lib/python3.8/site-packages (1.11.0)\r\nRequirement already satisfied: torchvision in /projects/49811120-694c-43f1-9267-605bd2af9ca9/miniconda3/envs/cv_env/lib/python3.8/site-packages (0.12.0)\r\nRequirement already satisfied: typing-extensions in /projects/49811120-694c-43f1-9267-605bd2af9ca9/miniconda3/envs/cv_env/lib/python3.8/site-packages (from torch) (4.2.0)\r\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /projects/49811120-694c-43f1-9267-605bd2af9ca9/miniconda3/envs/cv_env/lib/python3.8/site-packages (from torchvision) (9.1.1)\r\nRequirement already satisfied: requests in /projects/49811120-694c-43f1-9267-605bd2af9ca9/miniconda3/envs/cv_env/lib/python3.8/site-packages (from torchvision) (2.27.1)\r\nRequirement already satisfied: numpy in /projects/49811120-694c-43f1-9267-605bd2af9ca9/miniconda3/envs/cv_env/lib/python3.8/site-packages (from torchvision) (1.22.3)\r\n"},"1":{"name":"stdout","text":"Requirement already satisfied: certifi>=2017.4.17 in /projects/49811120-694c-43f1-9267-605bd2af9ca9/miniconda3/envs/cv_env/lib/python3.8/site-packages (from requests->torchvision) (2021.10.8)\r\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /projects/49811120-694c-43f1-9267-605bd2af9ca9/miniconda3/envs/cv_env/lib/python3.8/site-packages (from requests->torchvision) (1.26.9)\r\nRequirement already satisfied: idna<4,>=2.5 in /projects/49811120-694c-43f1-9267-605bd2af9ca9/miniconda3/envs/cv_env/lib/python3.8/site-packages (from requests->torchvision) (3.3)\r\nRequirement already satisfied: charset-normalizer~=2.0.0 in /projects/49811120-694c-43f1-9267-605bd2af9ca9/miniconda3/envs/cv_env/lib/python3.8/site-packages (from requests->torchvision) (2.0.12)\r\n"},"2":{"name":"stdout","text":"Note: you may need to restart the kernel to use updated packages.\n"},"3":{"name":"stdout","text":"Finished Training\n"},"4":{"name":"stdout","text":"Finished Training\n"},"5":{"name":"stdout","text":"Finished Training\n"},"6":{"name":"stdout","text":"Finished Training\n"},"7":{"name":"stdout","text":"Finished Training\n"},"8":{"name":"stdout","text":"Finished Training\n"}},"pos":31,"start":1657051443853,"state":"done","type":"cell"}
{"cell_type":"code","end":1657051703226,"id":"9e8d8f","input":"\n%pip install torch torchvision\nimport torch\nimport torchvision\nimport torch.optim as optim\nimport torchvision.transforms as transforms\nimport torch.nn as nn\nimport torch.nn.functional as F\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Convert Data to Tensors and normalize\ntransform = transforms.Compose(\n    [transforms.ToTensor(),\n     transforms.Normalize((0.5), (0.5))])\n\n# hyperparameters\ninput_size = 28 * 28 # 28 * 28 images\nnum_classes = 10 # 10 possible classifications/outputs for the model\nnum_epochs = 10 # 10 iterations over the dataset\nbatch_size = 8 # 8 images passed at a time into the model\n\n#the names of all the possible classifications\nclasses = (\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n           \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\")\n\n# training set\ntrain_dataset = torchvision.datasets.FashionMNIST(\n    root = \"./data\",\n    train = True,\n    transform = transform,\n    download = True\n)\n\n# EXERCISE: do the same thing for the testing dataset and call it test_dataset\n\ntest_dataset = torchvision.datasets.FashionMNIST(\n    root = \"./data\",\n    train = False,\n    transform = transform,\n    download = True\n)\n\n# Get batched Dataloaders\ntrainloader = torch.utils.data.DataLoader(\n    dataset = train_dataset,\n    batch_size = batch_size,\n    shuffle = True # optional\n)\n\n#EXERCISE: Create a dataloader for the testing dataset called testloader\ntestloader = torch.utils.data.DataLoader(\n    dataset = test_dataset,\n    batch_size = batch_size,\n    shuffle = True # optional\n)\n\nclass CNN(nn.Module):\n  def __init__(self):\n    super(CNN, self).__init__() \n    self.conv_layer_1 = nn.Sequential( # input.shape: [batch, 1, 28, 28]\n        nn.Conv2d(\n            in_channels=1, # grey scale, 3 otherwise if RGB   \n            out_channels=16, # 16 x 1(grey scale, 3 otherwise if RGB ) filters\n            kernel_size=5, # convolution extraction size\n            stride=1,\n            padding=2 # for convoluted output to be same size,\n                      # need padding=(kernel_size-1)/2 if stride = 1\n        ),\n        nn.ReLU(),\n        nn.MaxPool2d(kernel_size=2) # (2x2 maxpool), out.shape: [16, 14, 14]\n    )\n    #EXERCISE: Code second convolutional group\n    \n    \n    self.conv_layer_2 = nn.Sequential( # input.shape: [batch, 1, 28, 28]\n        nn.Conv2d(\n            in_channels=16, # grey scale, 3 otherwise if RGB   \n            out_channels=32, # 16 x 1(grey scale, 3 otherwise if RGB ) filters\n            kernel_size=5, # convolution extraction size\n            stride=1,\n            padding=2 # for convoluted output to be same size,\n                      # need padding=(kernel_size-1)/2 if stride = 1\n        ),\n        nn.ReLU(),\n        nn.MaxPool2d(kernel_size=2) # (2x2 maxpool), out.shape: [16, 14, 14]\n    )\n    \n    self.fc = nn.Linear(32*7*7, 10)\n\n  def forward(self, x):\n    # x.shape: [batch_size, 1, 28, 28] -> [batch_size, 16, 14, 14]\n    x = self.conv_layer_1(x)\n    # x.shape: [batch_size, 16, 14, 14] -> [batch_size, 32, 7, 7]\n    x = self.conv_layer_2(x)\n    x = x.view(x.shape[0], -1) # flatten: [batch_size, 32*7*7]\n    # x.shape: [batch_size, 32*7*7] -> [batch_size, 10]\n    out = self.fc(x)\n    return out\n\nmodel = CNN()\nmodel.to(device) # cast model to device\n\ncriterion = nn.CrossEntropyLoss()\n#EXERCISE: transfer the loss function over to the GPU. \n\ncriterion.to(device)\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\nfor epoch in range(3):  # loop over the dataset multiple times\n    running_loss = 0.0\n    for i, data in enumerate(trainloader):\n        # get the inputs; data is a list of [inputs, labels]\n        inputs, labels = data\n        inputs = inputs.to(device) # put input image onto gpu\n        labels = labels.to(device) # put label onto gpu\n\n        # zero the parameter gradients\n        optimizer.zero_grad()\n\n        # forward + backward + optimize\n        outputs = model(inputs)\n        #EXERCISE: Calculate \"loss\" by passing the outputs and labels into the loss function\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        # print statistics\n        running_loss += loss.item()\n        if i % 3750 == 3749:    # print every 3750 mini-batches\n            #EXERCISE: print and reset total loss\n\n            print('Finished Training')\n            \n#------------------------------\n\n#EXERCISE: turn the test dataloader (called testloader) into an iterator and get the next image batch\n\ndataiter = iter(testloader)\n\nimages, labels = dataiter.next()\n\n# print images\n#EXERCISE: Display the batch using torch.util.make_grid\nimshow(torchvision.utils.make_grid(images))\n\nprint('GroundTruth: ', ' '.join(f'{classes[labels[j]]:5s}' for j in range(batch_size)))\n\n\n#-------------------------------------------\n\ncorrect = 0\ntotal = 0\n# since we're not training, we don't need to calculate the gradients for our outputs\nwith torch.no_grad():\n    for data in testloader:\n        #EXERCISE: Split the data into image and labels as done in the training loop\n        #EXERCISE: Transfer the inputs and labels to the GPU\n        #EXERCISE: calculate outputs by running images through the network\n        \n        \n        images,labels = data\n        images = images.to(device)\n        labels = labels.to(device)\n        outputs = model(images)\n        # the class with the highest energy is what we choose as prediction\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\nprint(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')","kernel":"cv_env","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I6XAn4mNr8_W","outputId":"3a47fecd-91ad-4274-ef90-beb63ee1d07e"},"pos":35,"start":1657051703226,"state":"done","type":"cell"}
{"cell_type":"code","end":1657051703227,"exec_count":3,"id":"11b128","input":"\n%pip install torch torchvision\nimport torch\nimport torchvision\nimport torch.optim as optim\nimport torchvision.transforms as transforms\nimport torch.nn as nn\nimport torch.nn.functional as F\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Convert Data to Tensors and normalize\ntransform = transforms.Compose(\n    [transforms.ToTensor(),\n     transforms.Normalize((0.5), (0.5))])\n\n# hyperparameters\ninput_size = 28 * 28 # 28 * 28 images\nnum_classes = 10 # 10 possible classifications/outputs for the model\nnum_epochs = 10 # 10 iterations over the dataset\nbatch_size = 8 # 8 images passed at a time into the model\n\n#the names of all the possible classifications\nclasses = (\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n           \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\")\n\n# training set\ntrain_dataset = torchvision.datasets.FashionMNIST(\n    root = \"./data\",\n    train = True,\n    transform = transform,\n    download = True\n)\n\n# EXERCISE: do the same thing for the testing dataset and call it test_dataset\n\ntest_dataset = torchvision.datasets.FashionMNIST(\n    root = \"./data\",\n    train = False,\n    transform = transform,\n    download = True\n)\n\n# Get batched Dataloaders\ntrainloader = torch.utils.data.DataLoader(\n    dataset = train_dataset,\n    batch_size = batch_size,\n    shuffle = True # optional\n)\n\n#EXERCISE: Create a dataloader for the testing dataset called testloader\ntestloader = torch.utils.data.DataLoader(\n    dataset = test_dataset,\n    batch_size = batch_size,\n    shuffle = True # optional\n)\n\nclass CNN(nn.Module):\n  def __init__(self):\n    super(CNN, self).__init__() \n    self.conv_layer_1 = nn.Sequential( # input.shape: [batch, 1, 28, 28]\n        nn.Conv2d(\n            in_channels=1, # grey scale, 3 otherwise if RGB   \n            out_channels=16, # 16 x 1(grey scale, 3 otherwise if RGB ) filters\n            kernel_size=5, # convolution extraction size\n            stride=1,\n            padding=2 # for convoluted output to be same size,\n                      # need padding=(kernel_size-1)/2 if stride = 1\n        ),\n        nn.ReLU(),\n        nn.MaxPool2d(kernel_size=2) # (2x2 maxpool), out.shape: [16, 14, 14]\n    )\n    #EXERCISE: Code second convolutional group\n    \n    \n    self.conv_layer_2 = nn.Sequential( # input.shape: [batch, 1, 28, 28]\n        nn.Conv2d(\n            in_channels=16, # grey scale, 3 otherwise if RGB   \n            out_channels=32, # 16 x 1(grey scale, 3 otherwise if RGB ) filters\n            kernel_size=5, # convolution extraction size\n            stride=1,\n            padding=2 # for convoluted output to be same size,\n                      # need padding=(kernel_size-1)/2 if stride = 1\n        ),\n        nn.ReLU(),\n        nn.MaxPool2d(kernel_size=2) # (2x2 maxpool), out.shape: [16, 14, 14]\n    )\n    \n    self.fc = nn.Linear(32*7*7, 10)\n\n  def forward(self, x):\n    # x.shape: [batch_size, 1, 28, 28] -> [batch_size, 16, 14, 14]\n    x = self.conv_layer_1(x)\n    # x.shape: [batch_size, 16, 14, 14] -> [batch_size, 32, 7, 7]\n    x = self.conv_layer_2(x)\n    x = x.view(x.shape[0], -1) # flatten: [batch_size, 32*7*7]\n    # x.shape: [batch_size, 32*7*7] -> [batch_size, 10]\n    out = self.fc(x)\n    return out\n\nmodel = CNN()\nmodel.to(device) # cast model to device\n\ncriterion = nn.CrossEntropyLoss()\n#EXERCISE: transfer the loss function over to the GPU. \n\ncriterion.to(device)\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\nfor epoch in range(3):  # loop over the dataset multiple times\n    running_loss = 0.0\n    for i, data in enumerate(trainloader):\n        # get the inputs; data is a list of [inputs, labels]\n        inputs, labels = data\n        inputs = inputs.to(device) # put input image onto gpu\n        labels = labels.to(device) # put label onto gpu\n\n        # zero the parameter gradients\n        optimizer.zero_grad()\n\n        # forward + backward + optimize\n        outputs = model(inputs)\n        #EXERCISE: Calculate \"loss\" by passing the outputs and labels into the loss function\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        # print statistics\n        running_loss += loss.item()\n        if i % 3750 == 3749:    # print every 3750 mini-batches\n            #EXERCISE: print and reset total loss\n\n            print('Finished Training')\n            \n#------------------------------\n\n#EXERCISE: turn the test dataloader (called testloader) into an iterator and get the next image batch\n\ndataiter = iter(testloader)\n\nimages, labels = dataiter.next()\n\n# print images\n#EXERCISE: Display the batch using torch.util.make_grid\nimshow(torchvision.utils.make_grid(images))\n\nprint('GroundTruth: ', ' '.join(f'{classes[labels[j]]:5s}' for j in range(batch_size)))","kernel":"cv_env","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":112},"id":"kZAICucFPRXx","outputId":"ddbe07f7-8dfc-46b0-cd04-6df452512447"},"output":{"0":{"name":"stdout","text":"Requirement already satisfied: torch in /projects/49811120-694c-43f1-9267-605bd2af9ca9/miniconda3/envs/cv_env/lib/python3.8/site-packages (1.11.0)\r\nRequirement already satisfied: torchvision in /projects/49811120-694c-43f1-9267-605bd2af9ca9/miniconda3/envs/cv_env/lib/python3.8/site-packages (0.12.0)\r\nRequirement already satisfied: typing-extensions in /projects/49811120-694c-43f1-9267-605bd2af9ca9/miniconda3/envs/cv_env/lib/python3.8/site-packages (from torch) (4.2.0)\r\nRequirement already satisfied: numpy in /projects/49811120-694c-43f1-9267-605bd2af9ca9/miniconda3/envs/cv_env/lib/python3.8/site-packages (from torchvision) (1.22.3)\r\nRequirement already satisfied: requests in /projects/49811120-694c-43f1-9267-605bd2af9ca9/miniconda3/envs/cv_env/lib/python3.8/site-packages (from torchvision) (2.27.1)\r\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /projects/49811120-694c-43f1-9267-605bd2af9ca9/miniconda3/envs/cv_env/lib/python3.8/site-packages (from torchvision) (9.1.1)\r\n"},"1":{"name":"stdout","text":"Requirement already satisfied: idna<4,>=2.5 in /projects/49811120-694c-43f1-9267-605bd2af9ca9/miniconda3/envs/cv_env/lib/python3.8/site-packages (from requests->torchvision) (3.3)\r\nRequirement already satisfied: charset-normalizer~=2.0.0 in /projects/49811120-694c-43f1-9267-605bd2af9ca9/miniconda3/envs/cv_env/lib/python3.8/site-packages (from requests->torchvision) (2.0.12)\r\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /projects/49811120-694c-43f1-9267-605bd2af9ca9/miniconda3/envs/cv_env/lib/python3.8/site-packages (from requests->torchvision) (1.26.9)\r\nRequirement already satisfied: certifi>=2017.4.17 in /projects/49811120-694c-43f1-9267-605bd2af9ca9/miniconda3/envs/cv_env/lib/python3.8/site-packages (from requests->torchvision) (2021.10.8)\r\n"},"2":{"name":"stdout","text":"Note: you may need to restart the kernel to use updated packages.\n"},"3":{"name":"stdout","text":"Finished Training\n"},"4":{"name":"stdout","text":"Finished Training\n"},"5":{"name":"stdout","text":"Finished Training\n"},"6":{"name":"stdout","text":"Finished Training\n"},"7":{"name":"stdout","text":"Finished Training\n"},"8":{"name":"stdout","text":"Finished Training\n"},"9":{"ename":"NameError","evalue":"name 'imshow' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 144>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    140\u001b[0m images, labels \u001b[38;5;241m=\u001b[39m dataiter\u001b[38;5;241m.\u001b[39mnext()\n\u001b[1;32m    142\u001b[0m \u001b[38;5;66;03m# print images\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;66;03m#EXERCISE: Display the batch using torch.util.make_grid\u001b[39;00m\n\u001b[0;32m--> 144\u001b[0m \u001b[43mimshow\u001b[49m(torchvision\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mmake_grid(images))\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGroundTruth: \u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclasses[labels[j]]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m5s\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(batch_size)))\n","\u001b[0;31mNameError\u001b[0m: name 'imshow' is not defined"]}},"pos":33,"start":1657051571600,"state":"done","type":"cell"}
{"cell_type":"code","exec_count":0,"id":"074df3","input":"# hyperparameters\ninput_size = 28 * 28 # 28 * 28 images\nnum_classes = 10 # 10 possible classifications/outputs for the model\nnum_epochs = 10 # 10 iterations over the dataset\nbatch_size = 8 # 8 images passed at a time into the model\n\n#the names of all the possible classifications\nclasses = (\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n           \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\")","metadata":{"id":"Zd-szzHtHCYb"},"pos":11,"state":"done","type":"cell"}
{"cell_type":"code","exec_count":0,"id":"31a888","input":"%pip install torch torchvision\nimport torch\nimport torchvision\nimport torch.optim as optim\nimport torchvision.transforms as transforms\nimport torch.nn as nn\nimport torch.nn.functional as F","metadata":{"id":"ptEQDylSP4SP"},"pos":2,"state":"done","type":"cell"}
{"cell_type":"code","exec_count":0,"id":"5c4f18","input":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"id":"8hk0-8XDGnpd"},"pos":6,"state":"done","type":"cell"}
{"cell_type":"code","exec_count":0,"id":"a4646a","input":"with torch.no_grad():\n    # cast input to device\n    # [(1), 28, 28] -> [(1), (1), 28, 28]\n    image = image.to(device)\n\n    log_preds = model(image.unsqueeze(0))\n\n# post processes the image into more usable numbers\n#   Math: probabilites were natural logged, so torch.exp() performs e^(log_preds)\npreds = torch.exp(log_preds)\nprobab = list(preds.cpu().numpy()[0])\npred_label = probab.index(max(probab)) # get index of max num (highest probability)\n\nprint(f\"Prediction: {classes[pred_label]}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rgE35PGOtMun","outputId":"56fb78f9-eaa4-4610-b407-7ecb6401208f"},"pos":38,"state":"done","type":"cell"}
{"cell_type":"code","exec_count":0,"id":"d8526d","input":"# Convert Data to Tensors and normalize\ntransform = transforms.Compose(\n    [transforms.ToTensor(),\n     transforms.Normalize((0.5), (0.5))])","metadata":{"id":"mgxEe11MLiry"},"pos":9,"state":"done","type":"cell"}
{"cell_type":"code","exec_count":0,"id":"f15416","input":"import random\nrand = random.randint(0,9999)\nimage = test_dataset[rand][0] # shape: [(1) batch_size, 28, 28]\nplt.subplot()\nplt.axis('off')\n# [1, 28, 28] -> [28, 28]\nplt.imshow(image.squeeze(0))\nprint(f\"Ground Truth: {classes[test_dataset[rand][1]]}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":265},"id":"Ra_kXL0EV2vf","outputId":"d91906ff-bf30-4466-b6c1-c3f63ec80dd3"},"pos":37,"state":"done","type":"cell"}
{"cell_type":"code","exec_count":0,"id":"f40c05","input":"","metadata":{"id":"TOpENCdFtSRZ"},"pos":39,"state":"done","type":"cell"}
{"cell_type":"code","exec_count":1,"id":"4b1141","input":"%pip install torch torchvision\nimport torch\nimport torchvision\nimport torch.optim as optim\nimport torchvision.transforms as transforms\nimport torch.nn as nn\nimport torch.nn.functional as F\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Convert Data to Tensors and normalize\ntransform = transforms.Compose(\n    [transforms.ToTensor(),\n     transforms.Normalize((0.5), (0.5))])\n\n# hyperparameters\ninput_size = 28 * 28 # 28 * 28 images\nnum_classes = 10 # 10 possible classifications/outputs for the model\nnum_epochs = 10 # 10 iterations over the dataset\nbatch_size = 8 # 8 images passed at a time into the model\n\n#the names of all the possible classifications\nclasses = (\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n           \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\")\n\n# training set\ntrain_dataset = torchvision.datasets.FashionMNIST(\n    root = \"./data\",\n    train = True,\n    transform = transform,\n    download = True\n)\n\n# EXERCISE: do the same thing for the testing dataset and call it test_dataset\n\ntest_dataset = torchvision.datasets.FashionMNIST(\n    root = \"./data\",\n    train = False,\n    transform = transform,\n    download = True\n)","kernel":"cv_env","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":423,"referenced_widgets":["0c32afe2a35c48dc81e26f43832e5528","80c324bd3a774d9fb3474fea9a2f37a1","d994ef5592444f078eb0b38adbb24176","bd1bcd5f50a248a896285f4a8e154c6c","86fbff3d1b4f4fe98ef4f5713c5c6014","35a8bff49c89404ba666f0181606edad","3e3ac696c1ae4030970a291d0b55a9c3","0dd8ab643bd24b2e949e4e3b4096d4be","5f4d3dce10394194943134f856a40b5e","68fbbf59d93340c7be0b57869ac17b15","fcbfca25f1a14e15b0fa0f6088bf40f1","d58ad2180cae45eab7addb2417b50a03","7280aea30eff49bf94a4217647cd70f2","7151b8796109496f9a0213d2ca556296","d841001138284dbe8628700df81bc404","683e7c41d782493ba3cb93d0c288ade1","fd2449e472514181a355fc1c315acfa2","b37cbc32dcbc49aca73006ba0f900a17","58493c17d71b43fc876c0cc4a3093120","161da13e65934d7f9ee9e5cd9f35a46f","a0acaa8f8ff04fa4aa8efef789ea6094","4b5790f98e3b44ec915349b854c9064f","9ddf7176f4d54ae4bca347af22041b7b","822c855d65d64f0a9112308d2389ff7c","67167b99eabd407d8a7900319b48c67f","52dd7ed840c244059a7d69252fb20a0f","14bbc869fe8d452692a6f7cdb38b075b","63bba93a02c141d18b53328db8173d67","f90246acab4b45f3b540772689ac3657","2cdcdd80797f42659fb44a03e3bbd269","71df07ea74d045ea93bba3d60dcb800c","76b8caf8fcbb48779f5dea6d2329ff4d","37a3453775974ecd9176a3c96dde77df","9e48ad9913664c5f9f53f3a815ba9c85","09985f7b880f47509081c7b8f6e797a6","499b378fe04d48aa937c21de79966c4b","701e7e3c57174353aa7190b10b10da27","53e1b8f46e04430c8e19fb66ec674d28","d71d754be2be425ca24b194a2aea125c","f6363bc40f524f2ea0ecb199cd4ad414","a41d6fc422ee47e189be9fde8a0fa2ff","20182c64d4de4aff830441867cfd9d78","d4b6729efbed4c48aa19ae019aa1ee43","c1528e5e1be64a93928ad41707b0dec0"]},"id":"dU6akcMoXxb3","outputId":"f2b71f90-d1c0-4798-9e2d-c83709954e8d"},"output":{"0":{"name":"stdout","text":"Requirement already satisfied: torch in /projects/49811120-694c-43f1-9267-605bd2af9ca9/miniconda3/envs/cv_env/lib/python3.8/site-packages (1.11.0)\r\nRequirement already satisfied: torchvision in /projects/49811120-694c-43f1-9267-605bd2af9ca9/miniconda3/envs/cv_env/lib/python3.8/site-packages (0.12.0)\r\nRequirement already satisfied: typing-extensions in /projects/49811120-694c-43f1-9267-605bd2af9ca9/miniconda3/envs/cv_env/lib/python3.8/site-packages (from torch) (4.2.0)\r\nRequirement already satisfied: requests in /projects/49811120-694c-43f1-9267-605bd2af9ca9/miniconda3/envs/cv_env/lib/python3.8/site-packages (from torchvision) (2.27.1)\r\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /projects/49811120-694c-43f1-9267-605bd2af9ca9/miniconda3/envs/cv_env/lib/python3.8/site-packages (from torchvision) (9.1.1)\r\nRequirement already satisfied: numpy in /projects/49811120-694c-43f1-9267-605bd2af9ca9/miniconda3/envs/cv_env/lib/python3.8/site-packages (from torchvision) (1.22.3)\r\n"},"1":{"name":"stdout","text":"Requirement already satisfied: idna<4,>=2.5 in /projects/49811120-694c-43f1-9267-605bd2af9ca9/miniconda3/envs/cv_env/lib/python3.8/site-packages (from requests->torchvision) (3.3)\r\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /projects/49811120-694c-43f1-9267-605bd2af9ca9/miniconda3/envs/cv_env/lib/python3.8/site-packages (from requests->torchvision) (1.26.9)\r\nRequirement already satisfied: charset-normalizer~=2.0.0 in /projects/49811120-694c-43f1-9267-605bd2af9ca9/miniconda3/envs/cv_env/lib/python3.8/site-packages (from requests->torchvision) (2.0.12)\r\nRequirement already satisfied: certifi>=2017.4.17 in /projects/49811120-694c-43f1-9267-605bd2af9ca9/miniconda3/envs/cv_env/lib/python3.8/site-packages (from requests->torchvision) (2021.10.8)\r\n"},"10":{"name":"stderr","text":"\r0.0%"},"100":{"name":"stderr","text":"\r0.4%"},"1000":{"name":"stderr","text":"\r3.9%"},"1001":{"name":"stderr","text":"\r3.9%"},"1002":{"name":"stderr","text":"\r3.9%"},"1003":{"name":"stderr","text":"\r3.9%"},"1004":{"name":"stderr","text":"\r3.9%"},"1005":{"name":"stderr","text":"\r3.9%"},"1006":{"name":"stderr","text":"\r3.9%"},"1007":{"name":"stderr","text":"\r3.9%"},"1008":{"name":"stderr","text":"\r3.9%"},"1009":{"name":"stderr","text":"\r3.9%"},"101":{"name":"stderr","text":"\r0.4%"},"1010":{"name":"stderr","text":"\r3.9%"},"1011":{"name":"stderr","text":"\r3.9%"},"1012":{"name":"stderr","text":"\r3.9%"},"1013":{"name":"stderr","text":"\r3.9%"},"1014":{"name":"stderr","text":"\r3.9%"},"1015":{"name":"stderr","text":"\r3.9%"},"1016":{"name":"stderr","text":"\r3.9%"},"1017":{"name":"stderr","text":"\r3.9%"},"1018":{"name":"stderr","text":"\r3.9%"},"1019":{"name":"stderr","text":"\r3.9%"},"102":{"name":"stderr","text":"\r0.4%"},"1020":{"name":"stderr","text":"\r3.9%"},"1021":{"name":"stderr","text":"\r3.9%"},"1022":{"name":"stderr","text":"\r3.9%"},"1023":{"name":"stderr","text":"\r3.9%"},"1024":{"name":"stderr","text":"\r4.0%"},"1025":{"name":"stderr","text":"\r4.0%"},"1026":{"name":"stderr","text":"\r4.0%"},"1027":{"name":"stderr","text":"\r4.0%"},"1028":{"name":"stderr","text":"\r4.0%"},"1029":{"name":"stderr","text":"\r4.0%"},"103":{"name":"stderr","text":"\r0.4%"},"1030":{"name":"stderr","text":"\r4.0%"},"1031":{"name":"stderr","text":"\r4.0%"},"1032":{"name":"stderr","text":"\r4.0%"},"1033":{"name":"stderr","text":"\r4.0%"},"1034":{"name":"stderr","text":"\r4.0%"},"1035":{"name":"stderr","text":"\r4.0%"},"1036":{"name":"stderr","text":"\r4.0%"},"1037":{"name":"stderr","text":"\r4.0%"},"1038":{"name":"stderr","text":"\r4.0%"},"1039":{"name":"stderr","text":"\r4.0%"},"104":{"name":"stderr","text":"\r0.4%"},"1040":{"name":"stderr","text":"\r4.0%"},"1041":{"name":"stderr","text":"\r4.0%"},"1042":{"name":"stderr","text":"\r4.0%"},"1043":{"name":"stderr","text":"\r4.0%"},"1044":{"name":"stderr","text":"\r4.0%"},"1045":{"name":"stderr","text":"\r4.0%"},"1046":{"name":"stderr","text":"\r4.0%"},"1047":{"name":"stderr","text":"\r4.0%"},"1048":{"name":"stderr","text":"\r4.0%"},"1049":{"name":"stderr","text":"\r4.0%"},"105":{"name":"stderr","text":"\r0.4%"},"1050":{"name":"stderr","text":"\r4.1%"},"1051":{"name":"stderr","text":"\r4.1%"},"1052":{"name":"stderr","text":"\r4.1%"},"1053":{"name":"stderr","text":"\r4.1%"},"1054":{"name":"stderr","text":"\r4.1%"},"1055":{"name":"stderr","text":"\r4.1%"},"1056":{"name":"stderr","text":"\r4.1%"},"1057":{"name":"stderr","text":"\r4.1%"},"1058":{"name":"stderr","text":"\r4.1%"},"1059":{"name":"stderr","text":"\r4.1%"},"106":{"name":"stderr","text":"\r0.4%"},"1060":{"name":"stderr","text":"\r4.1%"},"1061":{"name":"stderr","text":"\r4.1%"},"1062":{"name":"stderr","text":"\r4.1%"},"1063":{"name":"stderr","text":"\r4.1%"},"1064":{"name":"stderr","text":"\r4.1%"},"1065":{"name":"stderr","text":"\r4.1%"},"1066":{"name":"stderr","text":"\r4.1%"},"1067":{"name":"stderr","text":"\r4.1%"},"1068":{"name":"stderr","text":"\r4.1%"},"1069":{"name":"stderr","text":"\r4.1%"},"107":{"name":"stderr","text":"\r0.4%"},"1070":{"name":"stderr","text":"\r4.1%"},"1071":{"name":"stderr","text":"\r4.1%"},"1072":{"name":"stderr","text":"\r4.1%"},"1073":{"name":"stderr","text":"\r4.1%"},"1074":{"name":"stderr","text":"\r4.1%"},"1075":{"name":"stderr","text":"\r4.2%"},"1076":{"name":"stderr","text":"\r4.2%"},"1077":{"name":"stderr","text":"\r4.2%"},"1078":{"name":"stderr","text":"\r4.2%"},"1079":{"name":"stderr","text":"\r4.2%"},"108":{"name":"stderr","text":"\r0.4%"},"1080":{"name":"stderr","text":"\r4.2%"},"1081":{"name":"stderr","text":"\r4.2%"},"1082":{"name":"stderr","text":"\r4.2%"},"1083":{"name":"stderr","text":"\r4.2%"},"1084":{"name":"stderr","text":"\r4.2%"},"1085":{"name":"stderr","text":"\r4.2%"},"1086":{"name":"stderr","text":"\r4.2%"},"1087":{"name":"stderr","text":"\r4.2%"},"1088":{"name":"stderr","text":"\r4.2%"},"1089":{"name":"stderr","text":"\r4.2%"},"109":{"name":"stderr","text":"\r0.4%"},"1090":{"name":"stderr","text":"\r4.2%"},"1091":{"name":"stderr","text":"\r4.2%"},"1092":{"name":"stderr","text":"\r4.2%"},"1093":{"name":"stderr","text":"\r4.2%"},"1094":{"name":"stderr","text":"\r4.2%"},"1095":{"name":"stderr","text":"\r4.2%"},"1096":{"name":"stderr","text":"\r4.2%"},"1097":{"name":"stderr","text":"\r4.2%"},"1098":{"name":"stderr","text":"\r4.2%"},"1099":{"name":"stderr","text":"\r4.2%"},"11":{"name":"stderr","text":"\r0.0%"},"110":{"name":"stderr","text":"\r0.4%"},"1100":{"name":"stderr","text":"\r4.2%"},"1101":{"name":"stderr","text":"\r4.3%"},"1102":{"name":"stderr","text":"\r4.3%"},"1103":{"name":"stderr","text":"\r4.3%"},"1104":{"name":"stderr","text":"\r4.3%"},"1105":{"name":"stderr","text":"\r4.3%"},"1106":{"name":"stderr","text":"\r4.3%"},"1107":{"name":"stderr","text":"\r4.3%"},"1108":{"name":"stderr","text":"\r4.3%"},"1109":{"name":"stderr","text":"\r4.3%"},"111":{"name":"stderr","text":"\r0.4%"},"1110":{"name":"stderr","text":"\r4.3%"},"1111":{"name":"stderr","text":"\r4.3%"},"1112":{"name":"stderr","text":"\r4.3%"},"1113":{"name":"stderr","text":"\r4.3%"},"1114":{"name":"stderr","text":"\r4.3%"},"1115":{"name":"stderr","text":"\r4.3%"},"1116":{"name":"stderr","text":"\r4.3%"},"1117":{"name":"stderr","text":"\r4.3%"},"1118":{"name":"stderr","text":"\r4.3%"},"1119":{"name":"stderr","text":"\r4.3%"},"112":{"name":"stderr","text":"\r0.4%"},"1120":{"name":"stderr","text":"\r4.3%"},"1121":{"name":"stderr","text":"\r4.3%"},"1122":{"name":"stderr","text":"\r4.3%"},"1123":{"name":"stderr","text":"\r4.3%"},"1124":{"name":"stderr","text":"\r4.3%"},"1125":{"name":"stderr","text":"\r4.3%"},"1126":{"name":"stderr","text":"\r4.3%"},"1127":{"name":"stderr","text":"\r4.4%"},"1128":{"name":"stderr","text":"\r4.4%"},"1129":{"name":"stderr","text":"\r4.4%"},"113":{"name":"stderr","text":"\r0.4%"},"1130":{"name":"stderr","text":"\r4.4%"},"1131":{"name":"stderr","text":"\r4.4%"},"1132":{"name":"stderr","text":"\r4.4%"},"1133":{"name":"stderr","text":"\r4.4%"},"1134":{"name":"stderr","text":"\r4.4%"},"1135":{"name":"stderr","text":"\r4.4%"},"1136":{"name":"stderr","text":"\r4.4%"},"1137":{"name":"stderr","text":"\r4.4%"},"1138":{"name":"stderr","text":"\r4.4%"},"1139":{"name":"stderr","text":"\r4.4%"},"114":{"name":"stderr","text":"\r0.4%"},"1140":{"name":"stderr","text":"\r4.4%"},"1141":{"name":"stderr","text":"\r4.4%"},"1142":{"name":"stderr","text":"\r4.4%"},"1143":{"name":"stderr","text":"\r4.4%"},"1144":{"name":"stderr","text":"\r4.4%"},"1145":{"name":"stderr","text":"\r4.4%"},"1146":{"name":"stderr","text":"\r4.4%"},"1147":{"name":"stderr","text":"\r4.4%"},"1148":{"name":"stderr","text":"\r4.4%"},"1149":{"name":"stderr","text":"\r4.4%"},"115":{"name":"stderr","text":"\r0.4%"},"1150":{"name":"stderr","text":"\r4.4%"},"1151":{"name":"stderr","text":"\r4.4%"},"1152":{"name":"stderr","text":"\r4.4%"},"1153":{"name":"stderr","text":"\r4.5%"},"1154":{"name":"stderr","text":"\r4.5%"},"1155":{"name":"stderr","text":"\r4.5%"},"1156":{"name":"stderr","text":"\r4.5%"},"1157":{"name":"stderr","text":"\r4.5%"},"1158":{"name":"stderr","text":"\r4.5%"},"1159":{"name":"stderr","text":"\r4.5%"},"116":{"name":"stderr","text":"\r0.4%"},"1160":{"name":"stderr","text":"\r4.5%"},"1161":{"name":"stderr","text":"\r4.5%"},"1162":{"name":"stderr","text":"\r4.5%"},"1163":{"name":"stderr","text":"\r4.5%"},"1164":{"name":"stderr","text":"\r4.5%"},"1165":{"name":"stderr","text":"\r4.5%"},"1166":{"name":"stderr","text":"\r4.5%"},"1167":{"name":"stderr","text":"\r4.5%"},"1168":{"name":"stderr","text":"\r4.5%"},"1169":{"name":"stderr","text":"\r4.5%"},"117":{"name":"stderr","text":"\r0.4%"},"1170":{"name":"stderr","text":"\r4.5%"},"1171":{"name":"stderr","text":"\r4.5%"},"1172":{"name":"stderr","text":"\r4.5%"},"1173":{"name":"stderr","text":"\r4.5%"},"1174":{"name":"stderr","text":"\r4.5%"},"1175":{"name":"stderr","text":"\r4.5%"},"1176":{"name":"stderr","text":"\r4.5%"},"1177":{"name":"stderr","text":"\r4.5%"},"1178":{"name":"stderr","text":"\r4.5%"},"1179":{"name":"stderr","text":"\r4.6%"},"118":{"name":"stderr","text":"\r0.4%"},"1180":{"name":"stderr","text":"\r4.6%"},"1181":{"name":"stderr","text":"\r4.6%"},"1182":{"name":"stderr","text":"\r4.6%"},"1183":{"name":"stderr","text":"\r4.6%"},"1184":{"name":"stderr","text":"\r4.6%"},"1185":{"name":"stderr","text":"\r4.6%"},"1186":{"name":"stderr","text":"\r4.6%"},"1187":{"name":"stderr","text":"\r4.6%"},"1188":{"name":"stderr","text":"\r4.6%"},"1189":{"name":"stderr","text":"\r4.6%"},"119":{"name":"stderr","text":"\r0.4%"},"1190":{"name":"stderr","text":"\r4.6%"},"1191":{"name":"stderr","text":"\r4.6%"},"1192":{"name":"stderr","text":"\r4.6%"},"1193":{"name":"stderr","text":"\r4.6%"},"1194":{"name":"stderr","text":"\r4.6%"},"1195":{"name":"stderr","text":"\r4.6%"},"1196":{"name":"stderr","text":"\r4.6%"},"1197":{"name":"stderr","text":"\r4.6%"},"1198":{"name":"stderr","text":"\r4.6%"},"1199":{"name":"stderr","text":"\r4.6%"},"12":{"name":"stderr","text":"\r0.0%"},"120":{"name":"stderr","text":"\r0.4%"},"1200":{"name":"stderr","text":"\r4.6%"},"1201":{"name":"stderr","text":"\r4.6%"},"1202":{"name":"stderr","text":"\r4.6%"},"1203":{"name":"stderr","text":"\r4.6%"},"1204":{"name":"stderr","text":"\r4.7%"},"1205":{"name":"stderr","text":"\r4.7%"},"1206":{"name":"stderr","text":"\r4.7%"},"1207":{"name":"stderr","text":"\r4.7%"},"1208":{"name":"stderr","text":"\r4.7%"},"1209":{"name":"stderr","text":"\r4.7%"},"121":{"name":"stderr","text":"\r0.5%"},"1210":{"name":"stderr","text":"\r4.7%"},"1211":{"name":"stderr","text":"\r4.7%"},"1212":{"name":"stderr","text":"\r4.7%"},"1213":{"name":"stderr","text":"\r4.7%"},"1214":{"name":"stderr","text":"\r4.7%"},"1215":{"name":"stderr","text":"\r4.7%"},"1216":{"name":"stderr","text":"\r4.7%"},"1217":{"name":"stderr","text":"\r4.7%"},"1218":{"name":"stderr","text":"\r4.7%"},"1219":{"name":"stderr","text":"\r4.7%"},"122":{"name":"stderr","text":"\r0.5%"},"1220":{"name":"stderr","text":"\r4.7%"},"1221":{"name":"stderr","text":"\r4.7%"},"1222":{"name":"stderr","text":"\r4.7%"},"1223":{"name":"stderr","text":"\r4.7%"},"1224":{"name":"stderr","text":"\r4.7%"},"1225":{"name":"stderr","text":"\r4.7%"},"1226":{"name":"stderr","text":"\r4.7%"},"1227":{"name":"stderr","text":"\r4.7%"},"1228":{"name":"stderr","text":"\r4.7%"},"1229":{"name":"stderr","text":"\r4.7%"},"123":{"name":"stderr","text":"\r0.5%"},"1230":{"name":"stderr","text":"\r4.8%"},"1231":{"name":"stderr","text":"\r4.8%"},"1232":{"name":"stderr","text":"\r4.8%"},"1233":{"name":"stderr","text":"\r4.8%"},"1234":{"name":"stderr","text":"\r4.8%"},"1235":{"name":"stderr","text":"\r4.8%"},"1236":{"name":"stderr","text":"\r4.8%"},"1237":{"name":"stderr","text":"\r4.8%"},"1238":{"name":"stderr","text":"\r4.8%"},"1239":{"name":"stderr","text":"\r4.8%"},"124":{"name":"stderr","text":"\r0.5%"},"1240":{"name":"stderr","text":"\r4.8%"},"1241":{"name":"stderr","text":"\r4.8%"},"1242":{"name":"stderr","text":"\r4.8%"},"1243":{"name":"stderr","text":"\r4.8%"},"1244":{"name":"stderr","text":"\r4.8%"},"1245":{"name":"stderr","text":"\r4.8%"},"1246":{"name":"stderr","text":"\r4.8%"},"1247":{"name":"stderr","text":"\r4.8%"},"1248":{"name":"stderr","text":"\r4.8%"},"1249":{"name":"stderr","text":"\r4.8%"},"125":{"name":"stderr","text":"\r0.5%"},"1250":{"name":"stderr","text":"\r4.8%"},"1251":{"name":"stderr","text":"\r4.8%"},"1252":{"name":"stderr","text":"\r4.8%"},"1253":{"name":"stderr","text":"\r4.8%"},"1254":{"name":"stderr","text":"\r4.8%"},"1255":{"name":"stderr","text":"\r4.8%"},"1256":{"name":"stderr","text":"\r4.9%"},"1257":{"name":"stderr","text":"\r4.9%"},"1258":{"name":"stderr","text":"\r4.9%"},"1259":{"name":"stderr","text":"\r4.9%"},"126":{"name":"stderr","text":"\r0.5%"},"1260":{"name":"stderr","text":"\r4.9%"},"1261":{"name":"stderr","text":"\r4.9%"},"1262":{"name":"stderr","text":"\r4.9%"},"1263":{"name":"stderr","text":"\r4.9%"},"1264":{"name":"stderr","text":"\r4.9%"},"1265":{"name":"stderr","text":"\r4.9%"},"1266":{"name":"stderr","text":"\r4.9%"},"1267":{"name":"stderr","text":"\r4.9%"},"1268":{"name":"stderr","text":"\r4.9%"},"1269":{"name":"stderr","text":"\r4.9%"},"127":{"name":"stderr","text":"\r0.5%"},"1270":{"name":"stderr","text":"\r4.9%"},"1271":{"name":"stderr","text":"\r4.9%"},"1272":{"name":"stderr","text":"\r4.9%"},"1273":{"name":"stderr","text":"\r4.9%"},"1274":{"name":"stderr","text":"\r4.9%"},"1275":{"name":"stderr","text":"\r4.9%"},"1276":{"name":"stderr","text":"\r4.9%"},"1277":{"name":"stderr","text":"\r4.9%"},"1278":{"name":"stderr","text":"\r4.9%"},"1279":{"name":"stderr","text":"\r4.9%"},"128":{"name":"stderr","text":"\r0.5%"},"1280":{"name":"stderr","text":"\r4.9%"},"1281":{"name":"stderr","text":"\r4.9%"},"1282":{"name":"stderr","text":"\r5.0%"},"1283":{"name":"stderr","text":"\r5.0%"},"1284":{"name":"stderr","text":"\r5.0%"},"1285":{"name":"stderr","text":"\r5.0%"},"1286":{"name":"stderr","text":"\r5.0%"},"1287":{"name":"stderr","text":"\r5.0%"},"1288":{"name":"stderr","text":"\r5.0%"},"1289":{"name":"stderr","text":"\r5.0%"},"129":{"name":"stderr","text":"\r0.5%"},"1290":{"name":"stderr","text":"\r5.0%"},"1291":{"name":"stderr","text":"\r5.0%"},"1292":{"name":"stderr","text":"\r5.0%"},"1293":{"name":"stderr","text":"\r5.0%"},"1294":{"name":"stderr","text":"\r5.0%"},"1295":{"name":"stderr","text":"\r5.0%"},"1296":{"name":"stderr","text":"\r5.0%"},"1297":{"name":"stderr","text":"\r5.0%"},"1298":{"name":"stderr","text":"\r5.0%"},"1299":{"name":"stderr","text":"\r5.0%"},"13":{"name":"stderr","text":"\r0.0%"},"130":{"name":"stderr","text":"\r0.5%"},"1300":{"name":"stderr","text":"\r5.0%"},"1301":{"name":"stderr","text":"\r5.0%"},"1302":{"name":"stderr","text":"\r5.0%"},"1303":{"name":"stderr","text":"\r5.0%"},"1304":{"name":"stderr","text":"\r5.0%"},"1305":{"name":"stderr","text":"\r5.0%"},"1306":{"name":"stderr","text":"\r5.0%"},"1307":{"name":"stderr","text":"\r5.0%"},"1308":{"name":"stderr","text":"\r5.1%"},"1309":{"name":"stderr","text":"\r5.1%"},"131":{"name":"stderr","text":"\r0.5%"},"1310":{"name":"stderr","text":"\r5.1%"},"1311":{"name":"stderr","text":"\r5.1%"},"1312":{"name":"stderr","text":"\r5.1%"},"1313":{"name":"stderr","text":"\r5.1%"},"1314":{"name":"stderr","text":"\r5.1%"},"1315":{"name":"stderr","text":"\r5.1%"},"1316":{"name":"stderr","text":"\r5.1%"},"1317":{"name":"stderr","text":"\r5.1%"},"1318":{"name":"stderr","text":"\r5.1%"},"1319":{"name":"stderr","text":"\r5.1%"},"132":{"name":"stderr","text":"\r0.5%"},"1320":{"name":"stderr","text":"\r5.1%"},"1321":{"name":"stderr","text":"\r5.1%"},"1322":{"name":"stderr","text":"\r5.1%"},"1323":{"name":"stderr","text":"\r5.1%"},"1324":{"name":"stderr","text":"\r5.1%"},"1325":{"name":"stderr","text":"\r5.1%"},"1326":{"name":"stderr","text":"\r5.1%"},"1327":{"name":"stderr","text":"\r5.1%"},"1328":{"name":"stderr","text":"\r5.1%"},"1329":{"name":"stderr","text":"\r5.1%"},"133":{"name":"stderr","text":"\r0.5%"},"1330":{"name":"stderr","text":"\r5.1%"},"1331":{"name":"stderr","text":"\r5.1%"},"1332":{"name":"stderr","text":"\r5.1%"},"1333":{"name":"stderr","text":"\r5.2%"},"1334":{"name":"stderr","text":"\r5.2%"},"1335":{"name":"stderr","text":"\r5.2%"},"1336":{"name":"stderr","text":"\r5.2%"},"1337":{"name":"stderr","text":"\r5.2%"},"1338":{"name":"stderr","text":"\r5.2%"},"1339":{"name":"stderr","text":"\r5.2%"},"134":{"name":"stderr","text":"\r0.5%"},"1340":{"name":"stderr","text":"\r5.2%"},"1341":{"name":"stderr","text":"\r5.2%"},"1342":{"name":"stderr","text":"\r5.2%"},"1343":{"name":"stderr","text":"\r5.2%"},"1344":{"name":"stderr","text":"\r5.2%"},"1345":{"name":"stderr","text":"\r5.2%"},"1346":{"name":"stderr","text":"\r5.2%"},"1347":{"name":"stderr","text":"\r5.2%"},"1348":{"name":"stderr","text":"\r5.2%"},"1349":{"name":"stderr","text":"\r5.2%"},"135":{"name":"stderr","text":"\r0.5%"},"1350":{"name":"stderr","text":"\r5.2%"},"1351":{"name":"stderr","text":"\r5.2%"},"1352":{"name":"stderr","text":"\r5.2%"},"1353":{"name":"stderr","text":"\r5.2%"},"1354":{"name":"stderr","text":"\r5.2%"},"1355":{"name":"stderr","text":"\r5.2%"},"1356":{"name":"stderr","text":"\r5.2%"},"1357":{"name":"stderr","text":"\r5.2%"},"1358":{"name":"stderr","text":"\r5.2%"},"1359":{"name":"stderr","text":"\r5.3%"},"136":{"name":"stderr","text":"\r0.5%"},"1360":{"name":"stderr","text":"\r5.3%"},"1361":{"name":"stderr","text":"\r5.3%"},"1362":{"name":"stderr","text":"\r5.3%"},"1363":{"name":"stderr","text":"\r5.3%"},"1364":{"name":"stderr","text":"\r5.3%"},"1365":{"name":"stderr","text":"\r5.3%"},"1366":{"name":"stderr","text":"\r5.3%"},"1367":{"name":"stderr","text":"\r5.3%"},"1368":{"name":"stderr","text":"\r5.3%"},"1369":{"name":"stderr","text":"\r5.3%"},"137":{"name":"stderr","text":"\r0.5%"},"1370":{"name":"stderr","text":"\r5.3%"},"1371":{"name":"stderr","text":"\r5.3%"},"1372":{"name":"stderr","text":"\r5.3%"},"1373":{"name":"stderr","text":"\r5.3%"},"1374":{"name":"stderr","text":"\r5.3%"},"1375":{"name":"stderr","text":"\r5.3%"},"1376":{"name":"stderr","text":"\r5.3%"},"1377":{"name":"stderr","text":"\r5.3%"},"1378":{"name":"stderr","text":"\r5.3%"},"1379":{"name":"stderr","text":"\r5.3%"},"138":{"name":"stderr","text":"\r0.5%"},"1380":{"name":"stderr","text":"\r5.3%"},"1381":{"name":"stderr","text":"\r5.3%"},"1382":{"name":"stderr","text":"\r5.3%"},"1383":{"name":"stderr","text":"\r5.3%"},"1384":{"name":"stderr","text":"\r5.3%"},"1385":{"name":"stderr","text":"\r5.4%"},"1386":{"name":"stderr","text":"\r5.4%"},"1387":{"name":"stderr","text":"\r5.4%"},"1388":{"name":"stderr","text":"\r5.4%"},"1389":{"name":"stderr","text":"\r5.4%"},"139":{"name":"stderr","text":"\r0.5%"},"1390":{"name":"stderr","text":"\r5.4%"},"1391":{"name":"stderr","text":"\r5.4%"},"1392":{"name":"stderr","text":"\r5.4%"},"1393":{"name":"stderr","text":"\r5.4%"},"1394":{"name":"stderr","text":"\r5.4%"},"1395":{"name":"stderr","text":"\r5.4%"},"1396":{"name":"stderr","text":"\r5.4%"},"1397":{"name":"stderr","text":"\r5.4%"},"1398":{"name":"stderr","text":"\r5.4%"},"1399":{"name":"stderr","text":"\r5.4%"},"14":{"name":"stderr","text":"\r0.0%"},"140":{"name":"stderr","text":"\r0.5%"},"1400":{"name":"stderr","text":"\r5.4%"},"1401":{"name":"stderr","text":"\r5.4%"},"1402":{"name":"stderr","text":"\r5.4%"},"1403":{"name":"stderr","text":"\r5.4%"},"1404":{"name":"stderr","text":"\r5.4%"},"1405":{"name":"stderr","text":"\r5.4%"},"1406":{"name":"stderr","text":"\r5.4%"},"1407":{"name":"stderr","text":"\r5.4%"},"1408":{"name":"stderr","text":"\r5.4%"},"1409":{"name":"stderr","text":"\r5.4%"},"141":{"name":"stderr","text":"\r0.5%"},"1410":{"name":"stderr","text":"\r5.4%"},"1411":{"name":"stderr","text":"\r5.5%"},"1412":{"name":"stderr","text":"\r5.5%"},"1413":{"name":"stderr","text":"\r5.5%"},"1414":{"name":"stderr","text":"\r5.5%"},"1415":{"name":"stderr","text":"\r5.5%"},"1416":{"name":"stderr","text":"\r5.5%"},"1417":{"name":"stderr","text":"\r5.5%"},"1418":{"name":"stderr","text":"\r5.5%"},"1419":{"name":"stderr","text":"\r5.5%"},"142":{"name":"stderr","text":"\r0.5%"},"1420":{"name":"stderr","text":"\r5.5%"},"1421":{"name":"stderr","text":"\r5.5%"},"1422":{"name":"stderr","text":"\r5.5%"},"1423":{"name":"stderr","text":"\r5.5%"},"1424":{"name":"stderr","text":"\r5.5%"},"1425":{"name":"stderr","text":"\r5.5%"},"1426":{"name":"stderr","text":"\r5.5%"},"1427":{"name":"stderr","text":"\r5.5%"},"1428":{"name":"stderr","text":"\r5.5%"},"1429":{"name":"stderr","text":"\r5.5%"},"143":{"name":"stderr","text":"\r0.5%"},"1430":{"name":"stderr","text":"\r5.5%"},"1431":{"name":"stderr","text":"\r5.5%"},"1432":{"name":"stderr","text":"\r5.5%"},"1433":{"name":"stderr","text":"\r5.5%"},"1434":{"name":"stderr","text":"\r5.5%"},"1435":{"name":"stderr","text":"\r5.5%"},"1436":{"name":"stderr","text":"\r5.5%"},"1437":{"name":"stderr","text":"\r5.6%"},"1438":{"name":"stderr","text":"\r5.6%"},"1439":{"name":"stderr","text":"\r5.6%"},"144":{"name":"stderr","text":"\r0.5%"},"1440":{"name":"stderr","text":"\r5.6%"},"1441":{"name":"stderr","text":"\r5.6%"},"1442":{"name":"stderr","text":"\r5.6%"},"1443":{"name":"stderr","text":"\r5.6%"},"1444":{"name":"stderr","text":"\r5.6%"},"1445":{"name":"stderr","text":"\r5.6%"},"1446":{"name":"stderr","text":"\r5.6%"},"1447":{"name":"stderr","text":"\r5.6%"},"1448":{"name":"stderr","text":"\r5.6%"},"1449":{"name":"stderr","text":"\r5.6%"},"145":{"name":"stderr","text":"\r0.5%"},"1450":{"name":"stderr","text":"\r5.6%"},"1451":{"name":"stderr","text":"\r5.6%"},"1452":{"name":"stderr","text":"\r5.6%"},"1453":{"name":"stderr","text":"\r5.6%"},"1454":{"name":"stderr","text":"\r5.6%"},"1455":{"name":"stderr","text":"\r5.6%"},"1456":{"name":"stderr","text":"\r5.6%"},"1457":{"name":"stderr","text":"\r5.6%"},"1458":{"name":"stderr","text":"\r5.6%"},"1459":{"name":"stderr","text":"\r5.6%"},"146":{"name":"stderr","text":"\r0.6%"},"1460":{"name":"stderr","text":"\r5.6%"},"1461":{"name":"stderr","text":"\r5.6%"},"1462":{"name":"stderr","text":"\r5.7%"},"1463":{"name":"stderr","text":"\r5.7%"},"1464":{"name":"stderr","text":"\r5.7%"},"1465":{"name":"stderr","text":"\r5.7%"},"1466":{"name":"stderr","text":"\r5.7%"},"1467":{"name":"stderr","text":"\r5.7%"},"1468":{"name":"stderr","text":"\r5.7%"},"1469":{"name":"stderr","text":"\r5.7%"},"147":{"name":"stderr","text":"\r0.6%"},"1470":{"name":"stderr","text":"\r5.7%"},"1471":{"name":"stderr","text":"\r5.7%"},"1472":{"name":"stderr","text":"\r5.7%"},"1473":{"name":"stderr","text":"\r5.7%"},"1474":{"name":"stderr","text":"\r5.7%"},"1475":{"name":"stderr","text":"\r5.7%"},"1476":{"name":"stderr","text":"\r5.7%"},"1477":{"name":"stderr","text":"\r5.7%"},"1478":{"name":"stderr","text":"\r5.7%"},"1479":{"name":"stderr","text":"\r5.7%"},"148":{"name":"stderr","text":"\r0.6%"},"1480":{"name":"stderr","text":"\r5.7%"},"1481":{"name":"stderr","text":"\r5.7%"},"1482":{"name":"stderr","text":"\r5.7%"},"1483":{"name":"stderr","text":"\r5.7%"},"1484":{"name":"stderr","text":"\r5.7%"},"1485":{"name":"stderr","text":"\r5.7%"},"1486":{"name":"stderr","text":"\r5.7%"},"1487":{"name":"stderr","text":"\r5.7%"},"1488":{"name":"stderr","text":"\r5.8%"},"1489":{"name":"stderr","text":"\r5.8%"},"149":{"name":"stderr","text":"\r0.6%"},"1490":{"name":"stderr","text":"\r5.8%"},"1491":{"name":"stderr","text":"\r5.8%"},"1492":{"name":"stderr","text":"\r5.8%"},"1493":{"name":"stderr","text":"\r5.8%"},"1494":{"name":"stderr","text":"\r5.8%"},"1495":{"name":"stderr","text":"\r5.8%"},"1496":{"name":"stderr","text":"\r5.8%"},"1497":{"name":"stderr","text":"\r5.8%"},"1498":{"name":"stderr","text":"\r5.8%"},"1499":{"name":"stderr","text":"\r5.8%"},"15":{"name":"stderr","text":"\r0.0%"},"150":{"name":"stderr","text":"\r0.6%"},"1500":{"name":"stderr","text":"\r5.8%"},"1501":{"name":"stderr","text":"\r5.8%"},"1502":{"name":"stderr","text":"\r5.8%"},"1503":{"name":"stderr","text":"\r5.8%"},"1504":{"name":"stderr","text":"\r5.8%"},"1505":{"name":"stderr","text":"\r5.8%"},"1506":{"name":"stderr","text":"\r5.8%"},"1507":{"name":"stderr","text":"\r5.8%"},"1508":{"name":"stderr","text":"\r5.8%"},"1509":{"name":"stderr","text":"\r5.8%"},"151":{"name":"stderr","text":"\r0.6%"},"1510":{"name":"stderr","text":"\r5.8%"},"1511":{"name":"stderr","text":"\r5.8%"},"1512":{"name":"stderr","text":"\r5.8%"},"1513":{"name":"stderr","text":"\r5.8%"},"1514":{"name":"stderr","text":"\r5.9%"},"1515":{"name":"stderr","text":"\r5.9%"},"1516":{"name":"stderr","text":"\r5.9%"},"1517":{"name":"stderr","text":"\r5.9%"},"1518":{"name":"stderr","text":"\r5.9%"},"1519":{"name":"stderr","text":"\r5.9%"},"152":{"name":"stderr","text":"\r0.6%"},"1520":{"name":"stderr","text":"\r5.9%"},"1521":{"name":"stderr","text":"\r5.9%"},"1522":{"name":"stderr","text":"\r5.9%"},"1523":{"name":"stderr","text":"\r5.9%"},"1524":{"name":"stderr","text":"\r5.9%"},"1525":{"name":"stderr","text":"\r5.9%"},"1526":{"name":"stderr","text":"\r5.9%"},"1527":{"name":"stderr","text":"\r5.9%"},"1528":{"name":"stderr","text":"\r5.9%"},"1529":{"name":"stderr","text":"\r5.9%"},"153":{"name":"stderr","text":"\r0.6%"},"1530":{"name":"stderr","text":"\r5.9%"},"1531":{"name":"stderr","text":"\r5.9%"},"1532":{"name":"stderr","text":"\r5.9%"},"1533":{"name":"stderr","text":"\r5.9%"},"1534":{"name":"stderr","text":"\r5.9%"},"1535":{"name":"stderr","text":"\r5.9%"},"1536":{"name":"stderr","text":"\r5.9%"},"1537":{"name":"stderr","text":"\r5.9%"},"1538":{"name":"stderr","text":"\r5.9%"},"1539":{"name":"stderr","text":"\r5.9%"},"154":{"name":"stderr","text":"\r0.6%"},"1540":{"name":"stderr","text":"\r6.0%"},"1541":{"name":"stderr","text":"\r6.0%"},"1542":{"name":"stderr","text":"\r6.0%"},"1543":{"name":"stderr","text":"\r6.0%"},"1544":{"name":"stderr","text":"\r6.0%"},"1545":{"name":"stderr","text":"\r6.0%"},"1546":{"name":"stderr","text":"\r6.0%"},"1547":{"name":"stderr","text":"\r6.0%"},"1548":{"name":"stderr","text":"\r6.0%"},"1549":{"name":"stderr","text":"\r6.0%"},"155":{"name":"stderr","text":"\r0.6%"},"1550":{"name":"stderr","text":"\r6.0%"},"1551":{"name":"stderr","text":"\r6.0%"},"1552":{"name":"stderr","text":"\r6.0%"},"1553":{"name":"stderr","text":"\r6.0%"},"1554":{"name":"stderr","text":"\r6.0%"},"1555":{"name":"stderr","text":"\r6.0%"},"1556":{"name":"stderr","text":"\r6.0%"},"1557":{"name":"stderr","text":"\r6.0%"},"1558":{"name":"stderr","text":"\r6.0%"},"1559":{"name":"stderr","text":"\r6.0%"},"156":{"name":"stderr","text":"\r0.6%"},"1560":{"name":"stderr","text":"\r6.0%"},"1561":{"name":"stderr","text":"\r6.0%"},"1562":{"name":"stderr","text":"\r6.0%"},"1563":{"name":"stderr","text":"\r6.0%"},"1564":{"name":"stderr","text":"\r6.0%"},"1565":{"name":"stderr","text":"\r6.0%"},"1566":{"name":"stderr","text":"\r6.1%"},"1567":{"name":"stderr","text":"\r6.1%"},"1568":{"name":"stderr","text":"\r6.1%"},"1569":{"name":"stderr","text":"\r6.1%"},"157":{"name":"stderr","text":"\r0.6%"},"1570":{"name":"stderr","text":"\r6.1%"},"1571":{"name":"stderr","text":"\r6.1%"},"1572":{"name":"stderr","text":"\r6.1%"},"1573":{"name":"stderr","text":"\r6.1%"},"1574":{"name":"stderr","text":"\r6.1%"},"1575":{"name":"stderr","text":"\r6.1%"},"1576":{"name":"stderr","text":"\r6.1%"},"1577":{"name":"stderr","text":"\r6.1%"},"1578":{"name":"stderr","text":"\r6.1%"},"1579":{"name":"stderr","text":"\r6.1%"},"158":{"name":"stderr","text":"\r0.6%"},"1580":{"name":"stderr","text":"\r6.1%"},"1581":{"name":"stderr","text":"\r6.1%"},"1582":{"name":"stderr","text":"\r6.1%"},"1583":{"name":"stderr","text":"\r6.1%"},"1584":{"name":"stderr","text":"\r6.1%"},"1585":{"name":"stderr","text":"\r6.1%"},"1586":{"name":"stderr","text":"\r6.1%"},"1587":{"name":"stderr","text":"\r6.1%"},"1588":{"name":"stderr","text":"\r6.1%"},"1589":{"name":"stderr","text":"\r6.1%"},"159":{"name":"stderr","text":"\r0.6%"},"1590":{"name":"stderr","text":"\r6.1%"},"1591":{"name":"stderr","text":"\r6.2%"},"1592":{"name":"stderr","text":"\r6.2%"},"1593":{"name":"stderr","text":"\r6.2%"},"1594":{"name":"stderr","text":"\r6.2%"},"1595":{"name":"stderr","text":"\r6.2%"},"1596":{"name":"stderr","text":"\r6.2%"},"1597":{"name":"stderr","text":"\r6.2%"},"1598":{"name":"stderr","text":"\r6.2%"},"1599":{"name":"stderr","text":"\r6.2%"},"16":{"name":"stderr","text":"\r0.0%"},"160":{"name":"stderr","text":"\r0.6%"},"1600":{"name":"stderr","text":"\r6.2%"},"1601":{"name":"stderr","text":"\r6.2%"},"1602":{"name":"stderr","text":"\r6.2%"},"1603":{"name":"stderr","text":"\r6.2%"},"1604":{"name":"stderr","text":"\r6.2%"},"1605":{"name":"stderr","text":"\r6.2%"},"1606":{"name":"stderr","text":"\r6.2%"},"1607":{"name":"stderr","text":"\r6.2%"},"1608":{"name":"stderr","text":"\r6.2%"},"1609":{"name":"stderr","text":"\r6.2%"},"161":{"name":"stderr","text":"\r0.6%"},"1610":{"name":"stderr","text":"\r6.2%"},"1611":{"name":"stderr","text":"\r6.2%"},"1612":{"name":"stderr","text":"\r6.2%"},"1613":{"name":"stderr","text":"\r6.2%"},"1614":{"name":"stderr","text":"\r6.2%"},"1615":{"name":"stderr","text":"\r6.2%"},"1616":{"name":"stderr","text":"\r6.2%"},"1617":{"name":"stderr","text":"\r6.3%"},"1618":{"name":"stderr","text":"\r6.3%"},"1619":{"name":"stderr","text":"\r6.3%"},"162":{"name":"stderr","text":"\r0.6%"},"1620":{"name":"stderr","text":"\r6.3%"},"1621":{"name":"stderr","text":"\r6.3%"},"1622":{"name":"stderr","text":"\r6.3%"},"1623":{"name":"stderr","text":"\r6.3%"},"1624":{"name":"stderr","text":"\r6.3%"},"1625":{"name":"stderr","text":"\r6.3%"},"1626":{"name":"stderr","text":"\r6.3%"},"1627":{"name":"stderr","text":"\r6.3%"},"1628":{"name":"stderr","text":"\r6.3%"},"1629":{"name":"stderr","text":"\r6.3%"},"163":{"name":"stderr","text":"\r0.6%"},"1630":{"name":"stderr","text":"\r6.3%"},"1631":{"name":"stderr","text":"\r6.3%"},"1632":{"name":"stderr","text":"\r6.3%"},"1633":{"name":"stderr","text":"\r6.3%"},"1634":{"name":"stderr","text":"\r6.3%"},"1635":{"name":"stderr","text":"\r6.3%"},"1636":{"name":"stderr","text":"\r6.3%"},"1637":{"name":"stderr","text":"\r6.3%"},"1638":{"name":"stderr","text":"\r6.3%"},"1639":{"name":"stderr","text":"\r6.3%"},"164":{"name":"stderr","text":"\r0.6%"},"1640":{"name":"stderr","text":"\r6.3%"},"1641":{"name":"stderr","text":"\r6.3%"},"1642":{"name":"stderr","text":"\r6.3%"},"1643":{"name":"stderr","text":"\r6.4%"},"1644":{"name":"stderr","text":"\r6.4%"},"1645":{"name":"stderr","text":"\r6.4%"},"1646":{"name":"stderr","text":"\r6.4%"},"1647":{"name":"stderr","text":"\r6.4%"},"1648":{"name":"stderr","text":"\r6.4%"},"1649":{"name":"stderr","text":"\r6.4%"},"165":{"name":"stderr","text":"\r0.6%"},"1650":{"name":"stderr","text":"\r6.4%"},"1651":{"name":"stderr","text":"\r6.4%"},"1652":{"name":"stderr","text":"\r6.4%"},"1653":{"name":"stderr","text":"\r6.4%"},"1654":{"name":"stderr","text":"\r6.4%"},"1655":{"name":"stderr","text":"\r6.4%"},"1656":{"name":"stderr","text":"\r6.4%"},"1657":{"name":"stderr","text":"\r6.4%"},"1658":{"name":"stderr","text":"\r6.4%"},"1659":{"name":"stderr","text":"\r6.4%"},"166":{"name":"stderr","text":"\r0.6%"},"1660":{"name":"stderr","text":"\r6.4%"},"1661":{"name":"stderr","text":"\r6.4%"},"1662":{"name":"stderr","text":"\r6.4%"},"1663":{"name":"stderr","text":"\r6.4%"},"1664":{"name":"stderr","text":"\r6.4%"},"1665":{"name":"stderr","text":"\r6.4%"},"1666":{"name":"stderr","text":"\r6.4%"},"1667":{"name":"stderr","text":"\r6.4%"},"1668":{"name":"stderr","text":"\r6.4%"},"1669":{"name":"stderr","text":"\r6.5%"},"167":{"name":"stderr","text":"\r0.6%"},"1670":{"name":"stderr","text":"\r6.5%"},"1671":{"name":"stderr","text":"\r6.5%"},"1672":{"name":"stderr","text":"\r6.5%"},"1673":{"name":"stderr","text":"\r6.5%"},"1674":{"name":"stderr","text":"\r6.5%"},"1675":{"name":"stderr","text":"\r6.5%"},"1676":{"name":"stderr","text":"\r6.5%"},"1677":{"name":"stderr","text":"\r6.5%"},"1678":{"name":"stderr","text":"\r6.5%"},"1679":{"name":"stderr","text":"\r6.5%"},"168":{"name":"stderr","text":"\r0.6%"},"1680":{"name":"stderr","text":"\r6.5%"},"1681":{"name":"stderr","text":"\r6.5%"},"1682":{"name":"stderr","text":"\r6.5%"},"1683":{"name":"stderr","text":"\r6.5%"},"1684":{"name":"stderr","text":"\r6.5%"},"1685":{"name":"stderr","text":"\r6.5%"},"1686":{"name":"stderr","text":"\r6.5%"},"1687":{"name":"stderr","text":"\r6.5%"},"1688":{"name":"stderr","text":"\r6.5%"},"1689":{"name":"stderr","text":"\r6.5%"},"169":{"name":"stderr","text":"\r0.6%"},"1690":{"name":"stderr","text":"\r6.5%"},"1691":{"name":"stderr","text":"\r6.5%"},"1692":{"name":"stderr","text":"\r6.5%"},"1693":{"name":"stderr","text":"\r6.5%"},"1694":{"name":"stderr","text":"\r6.5%"},"1695":{"name":"stderr","text":"\r6.6%"},"1696":{"name":"stderr","text":"\r6.6%"},"1697":{"name":"stderr","text":"\r6.6%"},"1698":{"name":"stderr","text":"\r6.6%"},"1699":{"name":"stderr","text":"\r6.6%"},"17":{"name":"stderr","text":"\r0.1%"},"170":{"name":"stderr","text":"\r0.6%"},"1700":{"name":"stderr","text":"\r6.6%"},"1701":{"name":"stderr","text":"\r6.6%"},"1702":{"name":"stderr","text":"\r6.6%"},"1703":{"name":"stderr","text":"\r6.6%"},"1704":{"name":"stderr","text":"\r6.6%"},"1705":{"name":"stderr","text":"\r6.6%"},"1706":{"name":"stderr","text":"\r6.6%"},"1707":{"name":"stderr","text":"\r6.6%"},"1708":{"name":"stderr","text":"\r6.6%"},"1709":{"name":"stderr","text":"\r6.6%"},"171":{"name":"stderr","text":"\r0.6%"},"1710":{"name":"stderr","text":"\r6.6%"},"1711":{"name":"stderr","text":"\r6.6%"},"1712":{"name":"stderr","text":"\r6.6%"},"1713":{"name":"stderr","text":"\r6.6%"},"1714":{"name":"stderr","text":"\r6.6%"},"1715":{"name":"stderr","text":"\r6.6%"},"1716":{"name":"stderr","text":"\r6.6%"},"1717":{"name":"stderr","text":"\r6.6%"},"1718":{"name":"stderr","text":"\r6.6%"},"1719":{"name":"stderr","text":"\r6.6%"},"172":{"name":"stderr","text":"\r0.7%"},"1720":{"name":"stderr","text":"\r6.7%"},"1721":{"name":"stderr","text":"\r6.7%"},"1722":{"name":"stderr","text":"\r6.7%"},"1723":{"name":"stderr","text":"\r6.7%"},"1724":{"name":"stderr","text":"\r6.7%"},"1725":{"name":"stderr","text":"\r6.7%"},"1726":{"name":"stderr","text":"\r6.7%"},"1727":{"name":"stderr","text":"\r6.7%"},"1728":{"name":"stderr","text":"\r6.7%"},"1729":{"name":"stderr","text":"\r6.7%"},"173":{"name":"stderr","text":"\r0.7%"},"1730":{"name":"stderr","text":"\r6.7%"},"1731":{"name":"stderr","text":"\r6.7%"},"1732":{"name":"stderr","text":"\r6.7%"},"1733":{"name":"stderr","text":"\r6.7%"},"1734":{"name":"stderr","text":"\r6.7%"},"1735":{"name":"stderr","text":"\r6.7%"},"1736":{"name":"stderr","text":"\r6.7%"},"1737":{"name":"stderr","text":"\r6.7%"},"1738":{"name":"stderr","text":"\r6.7%"},"1739":{"name":"stderr","text":"\r6.7%"},"174":{"name":"stderr","text":"\r0.7%"},"1740":{"name":"stderr","text":"\r6.7%"},"1741":{"name":"stderr","text":"\r6.7%"},"1742":{"name":"stderr","text":"\r6.7%"},"1743":{"name":"stderr","text":"\r6.7%"},"1744":{"name":"stderr","text":"\r6.7%"},"1745":{"name":"stderr","text":"\r6.7%"},"1746":{"name":"stderr","text":"\r6.8%"},"1747":{"name":"stderr","text":"\r6.8%"},"1748":{"name":"stderr","text":"\r6.8%"},"1749":{"name":"stderr","text":"\r6.8%"},"175":{"name":"stderr","text":"\r0.7%"},"1750":{"name":"stderr","text":"\r6.8%"},"1751":{"name":"stderr","text":"\r6.8%"},"1752":{"name":"stderr","text":"\r6.8%"},"1753":{"name":"stderr","text":"\r6.8%"},"1754":{"name":"stderr","text":"\r6.8%"},"1755":{"name":"stderr","text":"\r6.8%"},"1756":{"name":"stderr","text":"\r6.8%"},"1757":{"name":"stderr","text":"\r6.8%"},"1758":{"name":"stderr","text":"\r6.8%"},"1759":{"name":"stderr","text":"\r6.8%"},"176":{"name":"stderr","text":"\r0.7%"},"1760":{"name":"stderr","text":"\r6.8%"},"1761":{"name":"stderr","text":"\r6.8%"},"1762":{"name":"stderr","text":"\r6.8%"},"1763":{"name":"stderr","text":"\r6.8%"},"1764":{"name":"stderr","text":"\r6.8%"},"1765":{"name":"stderr","text":"\r6.8%"},"1766":{"name":"stderr","text":"\r6.8%"},"1767":{"name":"stderr","text":"\r6.8%"},"1768":{"name":"stderr","text":"\r6.8%"},"1769":{"name":"stderr","text":"\r6.8%"},"177":{"name":"stderr","text":"\r0.7%"},"1770":{"name":"stderr","text":"\r6.8%"},"1771":{"name":"stderr","text":"\r6.8%"},"1772":{"name":"stderr","text":"\r6.9%"},"1773":{"name":"stderr","text":"\r6.9%"},"1774":{"name":"stderr","text":"\r6.9%"},"1775":{"name":"stderr","text":"\r6.9%"},"1776":{"name":"stderr","text":"\r6.9%"},"1777":{"name":"stderr","text":"\r6.9%"},"1778":{"name":"stderr","text":"\r6.9%"},"1779":{"name":"stderr","text":"\r6.9%"},"178":{"name":"stderr","text":"\r0.7%"},"1780":{"name":"stderr","text":"\r6.9%"},"1781":{"name":"stderr","text":"\r6.9%"},"1782":{"name":"stderr","text":"\r6.9%"},"1783":{"name":"stderr","text":"\r6.9%"},"1784":{"name":"stderr","text":"\r6.9%"},"1785":{"name":"stderr","text":"\r6.9%"},"1786":{"name":"stderr","text":"\r6.9%"},"1787":{"name":"stderr","text":"\r6.9%"},"1788":{"name":"stderr","text":"\r6.9%"},"1789":{"name":"stderr","text":"\r6.9%"},"179":{"name":"stderr","text":"\r0.7%"},"1790":{"name":"stderr","text":"\r6.9%"},"1791":{"name":"stderr","text":"\r6.9%"},"1792":{"name":"stderr","text":"\r6.9%"},"1793":{"name":"stderr","text":"\r6.9%"},"1794":{"name":"stderr","text":"\r6.9%"},"1795":{"name":"stderr","text":"\r6.9%"},"1796":{"name":"stderr","text":"\r6.9%"},"1797":{"name":"stderr","text":"\r6.9%"},"1798":{"name":"stderr","text":"\r7.0%"},"1799":{"name":"stderr","text":"\r7.0%"},"18":{"name":"stderr","text":"\r0.1%"},"180":{"name":"stderr","text":"\r0.7%"},"1800":{"name":"stderr","text":"\r7.0%"},"1801":{"name":"stderr","text":"\r7.0%"},"1802":{"name":"stderr","text":"\r7.0%"},"1803":{"name":"stderr","text":"\r7.0%"},"1804":{"name":"stderr","text":"\r7.0%"},"1805":{"name":"stderr","text":"\r7.0%"},"1806":{"name":"stderr","text":"\r7.0%"},"1807":{"name":"stderr","text":"\r7.0%"},"1808":{"name":"stderr","text":"\r7.0%"},"1809":{"name":"stderr","text":"\r7.0%"},"181":{"name":"stderr","text":"\r0.7%"},"1810":{"name":"stderr","text":"\r7.0%"},"1811":{"name":"stderr","text":"\r7.0%"},"1812":{"name":"stderr","text":"\r7.0%"},"1813":{"name":"stderr","text":"\r7.0%"},"1814":{"name":"stderr","text":"\r7.0%"},"1815":{"name":"stderr","text":"\r7.0%"},"1816":{"name":"stderr","text":"\r7.0%"},"1817":{"name":"stderr","text":"\r7.0%"},"1818":{"name":"stderr","text":"\r7.0%"},"1819":{"name":"stderr","text":"\r7.0%"},"182":{"name":"stderr","text":"\r0.7%"},"1820":{"name":"stderr","text":"\r7.0%"},"1821":{"name":"stderr","text":"\r7.0%"},"1822":{"name":"stderr","text":"\r7.0%"},"1823":{"name":"stderr","text":"\r7.0%"},"1824":{"name":"stderr","text":"\r7.1%"},"1825":{"name":"stderr","text":"\r7.1%"},"1826":{"name":"stderr","text":"\r7.1%"},"1827":{"name":"stderr","text":"\r7.1%"},"1828":{"name":"stderr","text":"\r7.1%"},"1829":{"name":"stderr","text":"\r7.1%"},"183":{"name":"stderr","text":"\r0.7%"},"1830":{"name":"stderr","text":"\r7.1%"},"1831":{"name":"stderr","text":"\r7.1%"},"1832":{"name":"stderr","text":"\r7.1%"},"1833":{"name":"stderr","text":"\r7.1%"},"1834":{"name":"stderr","text":"\r7.1%"},"1835":{"name":"stderr","text":"\r7.1%"},"1836":{"name":"stderr","text":"\r7.1%"},"1837":{"name":"stderr","text":"\r7.1%"},"1838":{"name":"stderr","text":"\r7.1%"},"1839":{"name":"stderr","text":"\r7.1%"},"184":{"name":"stderr","text":"\r0.7%"},"1840":{"name":"stderr","text":"\r7.1%"},"1841":{"name":"stderr","text":"\r7.1%"},"1842":{"name":"stderr","text":"\r7.1%"},"1843":{"name":"stderr","text":"\r7.1%"},"1844":{"name":"stderr","text":"\r7.1%"},"1845":{"name":"stderr","text":"\r7.1%"},"1846":{"name":"stderr","text":"\r7.1%"},"1847":{"name":"stderr","text":"\r7.1%"},"1848":{"name":"stderr","text":"\r7.1%"},"1849":{"name":"stderr","text":"\r7.2%"},"185":{"name":"stderr","text":"\r0.7%"},"1850":{"name":"stderr","text":"\r7.2%"},"1851":{"name":"stderr","text":"\r7.2%"},"1852":{"name":"stderr","text":"\r7.2%"},"1853":{"name":"stderr","text":"\r7.2%"},"1854":{"name":"stderr","text":"\r7.2%"},"1855":{"name":"stderr","text":"\r7.2%"},"1856":{"name":"stderr","text":"\r7.2%"},"1857":{"name":"stderr","text":"\r7.2%"},"1858":{"name":"stderr","text":"\r7.2%"},"1859":{"name":"stderr","text":"\r7.2%"},"186":{"name":"stderr","text":"\r0.7%"},"1860":{"name":"stderr","text":"\r7.2%"},"1861":{"name":"stderr","text":"\r7.2%"},"1862":{"name":"stderr","text":"\r7.2%"},"1863":{"name":"stderr","text":"\r7.2%"},"1864":{"name":"stderr","text":"\r7.2%"},"1865":{"name":"stderr","text":"\r7.2%"},"1866":{"name":"stderr","text":"\r7.2%"},"1867":{"name":"stderr","text":"\r7.2%"},"1868":{"name":"stderr","text":"\r7.2%"},"1869":{"name":"stderr","text":"\r7.2%"},"187":{"name":"stderr","text":"\r0.7%"},"1870":{"name":"stderr","text":"\r7.2%"},"1871":{"name":"stderr","text":"\r7.2%"},"1872":{"name":"stderr","text":"\r7.2%"},"1873":{"name":"stderr","text":"\r7.2%"},"1874":{"name":"stderr","text":"\r7.2%"},"1875":{"name":"stderr","text":"\r7.3%"},"1876":{"name":"stderr","text":"\r7.3%"},"1877":{"name":"stderr","text":"\r7.3%"},"1878":{"name":"stderr","text":"\r7.3%"},"1879":{"name":"stderr","text":"\r7.3%"},"188":{"name":"stderr","text":"\r0.7%"},"1880":{"name":"stderr","text":"\r7.3%"},"1881":{"name":"stderr","text":"\r7.3%"},"1882":{"name":"stderr","text":"\r7.3%"},"1883":{"name":"stderr","text":"\r7.3%"},"1884":{"name":"stderr","text":"\r7.3%"},"1885":{"name":"stderr","text":"\r7.3%"},"1886":{"name":"stderr","text":"\r7.3%"},"1887":{"name":"stderr","text":"\r7.3%"},"1888":{"name":"stderr","text":"\r7.3%"},"1889":{"name":"stderr","text":"\r7.3%"},"189":{"name":"stderr","text":"\r0.7%"},"1890":{"name":"stderr","text":"\r7.3%"},"1891":{"name":"stderr","text":"\r7.3%"},"1892":{"name":"stderr","text":"\r7.3%"},"1893":{"name":"stderr","text":"\r7.3%"},"1894":{"name":"stderr","text":"\r7.3%"},"1895":{"name":"stderr","text":"\r7.3%"},"1896":{"name":"stderr","text":"\r7.3%"},"1897":{"name":"stderr","text":"\r7.3%"},"1898":{"name":"stderr","text":"\r7.3%"},"1899":{"name":"stderr","text":"\r7.3%"},"19":{"name":"stderr","text":"\r0.1%"},"190":{"name":"stderr","text":"\r0.7%"},"1900":{"name":"stderr","text":"\r7.3%"},"1901":{"name":"stderr","text":"\r7.4%"},"1902":{"name":"stderr","text":"\r7.4%"},"1903":{"name":"stderr","text":"\r7.4%"},"1904":{"name":"stderr","text":"\r7.4%"},"1905":{"name":"stderr","text":"\r7.4%"},"1906":{"name":"stderr","text":"\r7.4%"},"1907":{"name":"stderr","text":"\r7.4%"},"1908":{"name":"stderr","text":"\r7.4%"},"1909":{"name":"stderr","text":"\r7.4%"},"191":{"name":"stderr","text":"\r0.7%"},"1910":{"name":"stderr","text":"\r7.4%"},"1911":{"name":"stderr","text":"\r7.4%"},"1912":{"name":"stderr","text":"\r7.4%"},"1913":{"name":"stderr","text":"\r7.4%"},"1914":{"name":"stderr","text":"\r7.4%"},"1915":{"name":"stderr","text":"\r7.4%"},"1916":{"name":"stderr","text":"\r7.4%"},"1917":{"name":"stderr","text":"\r7.4%"},"1918":{"name":"stderr","text":"\r7.4%"},"1919":{"name":"stderr","text":"\r7.4%"},"192":{"name":"stderr","text":"\r0.7%"},"1920":{"name":"stderr","text":"\r7.4%"},"1921":{"name":"stderr","text":"\r7.4%"},"1922":{"name":"stderr","text":"\r7.4%"},"1923":{"name":"stderr","text":"\r7.4%"},"1924":{"name":"stderr","text":"\r7.4%"},"1925":{"name":"stderr","text":"\r7.4%"},"1926":{"name":"stderr","text":"\r7.4%"},"1927":{"name":"stderr","text":"\r7.5%"},"1928":{"name":"stderr","text":"\r7.5%"},"1929":{"name":"stderr","text":"\r7.5%"},"193":{"name":"stderr","text":"\r0.7%"},"1930":{"name":"stderr","text":"\r7.5%"},"1931":{"name":"stderr","text":"\r7.5%"},"1932":{"name":"stderr","text":"\r7.5%"},"1933":{"name":"stderr","text":"\r7.5%"},"1934":{"name":"stderr","text":"\r7.5%"},"1935":{"name":"stderr","text":"\r7.5%"},"1936":{"name":"stderr","text":"\r7.5%"},"1937":{"name":"stderr","text":"\r7.5%"},"1938":{"name":"stderr","text":"\r7.5%"},"1939":{"name":"stderr","text":"\r7.5%"},"194":{"name":"stderr","text":"\r0.7%"},"1940":{"name":"stderr","text":"\r7.5%"},"1941":{"name":"stderr","text":"\r7.5%"},"1942":{"name":"stderr","text":"\r7.5%"},"1943":{"name":"stderr","text":"\r7.5%"},"1944":{"name":"stderr","text":"\r7.5%"},"1945":{"name":"stderr","text":"\r7.5%"},"1946":{"name":"stderr","text":"\r7.5%"},"1947":{"name":"stderr","text":"\r7.5%"},"1948":{"name":"stderr","text":"\r7.5%"},"1949":{"name":"stderr","text":"\r7.5%"},"195":{"name":"stderr","text":"\r0.7%"},"1950":{"name":"stderr","text":"\r7.5%"},"1951":{"name":"stderr","text":"\r7.5%"},"1952":{"name":"stderr","text":"\r7.5%"},"1953":{"name":"stderr","text":"\r7.6%"},"1954":{"name":"stderr","text":"\r7.6%"},"1955":{"name":"stderr","text":"\r7.6%"},"1956":{"name":"stderr","text":"\r7.6%"},"1957":{"name":"stderr","text":"\r7.6%"},"1958":{"name":"stderr","text":"\r7.6%"},"1959":{"name":"stderr","text":"\r7.6%"},"196":{"name":"stderr","text":"\r0.7%"},"1960":{"name":"stderr","text":"\r7.6%"},"1961":{"name":"stderr","text":"\r7.6%"},"1962":{"name":"stderr","text":"\r7.6%"},"1963":{"name":"stderr","text":"\r7.6%"},"1964":{"name":"stderr","text":"\r7.6%"},"1965":{"name":"stderr","text":"\r7.6%"},"1966":{"name":"stderr","text":"\r7.6%"},"1967":{"name":"stderr","text":"\r7.6%"},"1968":{"name":"stderr","text":"\r7.6%"},"1969":{"name":"stderr","text":"\r7.6%"},"197":{"name":"stderr","text":"\r0.7%"},"1970":{"name":"stderr","text":"\r7.6%"},"1971":{"name":"stderr","text":"\r7.6%"},"1972":{"name":"stderr","text":"\r7.6%"},"1973":{"name":"stderr","text":"\r7.6%"},"1974":{"name":"stderr","text":"\r7.6%"},"1975":{"name":"stderr","text":"\r7.6%"},"1976":{"name":"stderr","text":"\r7.6%"},"1977":{"name":"stderr","text":"\r7.6%"},"1978":{"name":"stderr","text":"\r7.7%"},"1979":{"name":"stderr","text":"\r7.7%"},"198":{"name":"stderr","text":"\r0.8%"},"1980":{"name":"stderr","text":"\r7.7%"},"1981":{"name":"stderr","text":"\r7.7%"},"1982":{"name":"stderr","text":"\r7.7%"},"1983":{"name":"stderr","text":"\r7.7%"},"1984":{"name":"stderr","text":"\r7.7%"},"1985":{"name":"stderr","text":"\r7.7%"},"1986":{"name":"stderr","text":"\r7.7%"},"1987":{"name":"stderr","text":"\r7.7%"},"1988":{"name":"stderr","text":"\r7.7%"},"1989":{"name":"stderr","text":"\r7.7%"},"199":{"name":"stderr","text":"\r0.8%"},"1990":{"name":"stderr","text":"\r7.7%"},"1991":{"name":"stderr","text":"\r7.7%"},"1992":{"name":"stderr","text":"\r7.7%"},"1993":{"name":"stderr","text":"\r7.7%"},"1994":{"name":"stderr","text":"\r7.7%"},"1995":{"name":"stderr","text":"\r7.7%"},"1996":{"name":"stderr","text":"\r7.7%"},"1997":{"name":"stderr","text":"\r7.7%"},"1998":{"name":"stderr","text":"\r7.7%"},"1999":{"name":"stderr","text":"\r7.7%"},"2":{"name":"stdout","text":"Note: you may need to restart the kernel to use updated packages.\n"},"20":{"name":"stderr","text":"\r0.1%"},"200":{"name":"stderr","text":"\r0.8%"},"2000":{"name":"stderr","text":"\r7.7%"},"2001":{"name":"stderr","text":"\r7.7%"},"2002":{"name":"stderr","text":"\r7.7%"},"2003":{"name":"stderr","text":"\r7.7%"},"2004":{"name":"stderr","text":"\r7.8%"},"2005":{"name":"stderr","text":"\r7.8%"},"2006":{"name":"stderr","text":"\r7.8%"},"2007":{"name":"stderr","text":"\r7.8%"},"2008":{"name":"stderr","text":"\r7.8%"},"2009":{"name":"stderr","text":"\r7.8%"},"201":{"name":"stderr","text":"\r0.8%"},"2010":{"name":"stderr","text":"\r7.8%"},"2011":{"name":"stderr","text":"\r7.8%"},"2012":{"name":"stderr","text":"\r7.8%"},"2013":{"name":"stderr","text":"\r7.8%"},"2014":{"name":"stderr","text":"\r7.8%"},"2015":{"name":"stderr","text":"\r7.8%"},"2016":{"name":"stderr","text":"\r7.8%"},"2017":{"name":"stderr","text":"\r7.8%"},"2018":{"name":"stderr","text":"\r7.8%"},"2019":{"name":"stderr","text":"\r7.8%"},"202":{"name":"stderr","text":"\r0.8%"},"2020":{"name":"stderr","text":"\r7.8%"},"2021":{"name":"stderr","text":"\r7.8%"},"2022":{"name":"stderr","text":"\r7.8%"},"2023":{"name":"stderr","text":"\r7.8%"},"2024":{"name":"stderr","text":"\r7.8%"},"2025":{"name":"stderr","text":"\r7.8%"},"2026":{"name":"stderr","text":"\r7.8%"},"2027":{"name":"stderr","text":"\r7.8%"},"2028":{"name":"stderr","text":"\r7.8%"},"2029":{"name":"stderr","text":"\r7.8%"},"203":{"name":"stderr","text":"\r0.8%"},"2030":{"name":"stderr","text":"\r7.9%"},"2031":{"name":"stderr","text":"\r7.9%"},"2032":{"name":"stderr","text":"\r7.9%"},"2033":{"name":"stderr","text":"\r7.9%"},"2034":{"name":"stderr","text":"\r7.9%"},"2035":{"name":"stderr","text":"\r7.9%"},"2036":{"name":"stderr","text":"\r7.9%"},"2037":{"name":"stderr","text":"\r7.9%"},"2038":{"name":"stderr","text":"\r7.9%"},"2039":{"name":"stderr","text":"\r7.9%"},"204":{"name":"stderr","text":"\r0.8%"},"2040":{"name":"stderr","text":"\r7.9%"},"2041":{"name":"stderr","text":"\r7.9%"},"2042":{"name":"stderr","text":"\r7.9%"},"2043":{"name":"stderr","text":"\r7.9%"},"2044":{"name":"stderr","text":"\r7.9%"},"2045":{"name":"stderr","text":"\r7.9%"},"2046":{"name":"stderr","text":"\r7.9%"},"2047":{"name":"stderr","text":"\r7.9%"},"2048":{"name":"stderr","text":"\r7.9%"},"2049":{"name":"stderr","text":"\r7.9%"},"205":{"name":"stderr","text":"\r0.8%"},"2050":{"name":"stderr","text":"\r7.9%"},"2051":{"name":"stderr","text":"\r7.9%"},"2052":{"name":"stderr","text":"\r7.9%"},"2053":{"name":"stderr","text":"\r7.9%"},"2054":{"name":"stderr","text":"\r7.9%"},"2055":{"name":"stderr","text":"\r7.9%"},"2056":{"name":"stderr","text":"\r8.0%"},"2057":{"name":"stderr","text":"\r8.0%"},"2058":{"name":"stderr","text":"\r8.0%"},"2059":{"name":"stderr","text":"\r8.0%"},"206":{"name":"stderr","text":"\r0.8%"},"2060":{"name":"stderr","text":"\r8.0%"},"2061":{"name":"stderr","text":"\r8.0%"},"2062":{"name":"stderr","text":"\r8.0%"},"2063":{"name":"stderr","text":"\r8.0%"},"2064":{"name":"stderr","text":"\r8.0%"},"2065":{"name":"stderr","text":"\r8.0%"},"2066":{"name":"stderr","text":"\r8.0%"},"2067":{"name":"stderr","text":"\r8.0%"},"2068":{"name":"stderr","text":"\r8.0%"},"2069":{"name":"stderr","text":"\r8.0%"},"207":{"name":"stderr","text":"\r0.8%"},"2070":{"name":"stderr","text":"\r8.0%"},"2071":{"name":"stderr","text":"\r8.0%"},"2072":{"name":"stderr","text":"\r8.0%"},"2073":{"name":"stderr","text":"\r8.0%"},"2074":{"name":"stderr","text":"\r8.0%"},"2075":{"name":"stderr","text":"\r8.0%"},"2076":{"name":"stderr","text":"\r8.0%"},"2077":{"name":"stderr","text":"\r8.0%"},"2078":{"name":"stderr","text":"\r8.0%"},"2079":{"name":"stderr","text":"\r8.0%"},"208":{"name":"stderr","text":"\r0.8%"},"2080":{"name":"stderr","text":"\r8.0%"},"2081":{"name":"stderr","text":"\r8.0%"},"2082":{"name":"stderr","text":"\r8.1%"},"2083":{"name":"stderr","text":"\r8.1%"},"2084":{"name":"stderr","text":"\r8.1%"},"2085":{"name":"stderr","text":"\r8.1%"},"2086":{"name":"stderr","text":"\r8.1%"},"2087":{"name":"stderr","text":"\r8.1%"},"2088":{"name":"stderr","text":"\r8.1%"},"2089":{"name":"stderr","text":"\r8.1%"},"209":{"name":"stderr","text":"\r0.8%"},"2090":{"name":"stderr","text":"\r8.1%"},"2091":{"name":"stderr","text":"\r8.1%"},"2092":{"name":"stderr","text":"\r8.1%"},"2093":{"name":"stderr","text":"\r8.1%"},"2094":{"name":"stderr","text":"\r8.1%"},"2095":{"name":"stderr","text":"\r8.1%"},"2096":{"name":"stderr","text":"\r8.1%"},"2097":{"name":"stderr","text":"\r8.1%"},"2098":{"name":"stderr","text":"\r8.1%"},"2099":{"name":"stderr","text":"\r8.1%"},"21":{"name":"stderr","text":"\r0.1%"},"210":{"name":"stderr","text":"\r0.8%"},"2100":{"name":"stderr","text":"\r8.1%"},"2101":{"name":"stderr","text":"\r8.1%"},"2102":{"name":"stderr","text":"\r8.1%"},"2103":{"name":"stderr","text":"\r8.1%"},"2104":{"name":"stderr","text":"\r8.1%"},"2105":{"name":"stderr","text":"\r8.1%"},"2106":{"name":"stderr","text":"\r8.1%"},"2107":{"name":"stderr","text":"\r8.2%"},"2108":{"name":"stderr","text":"\r8.2%"},"2109":{"name":"stderr","text":"\r8.2%"},"211":{"name":"stderr","text":"\r0.8%"},"2110":{"name":"stderr","text":"\r8.2%"},"2111":{"name":"stderr","text":"\r8.2%"},"2112":{"name":"stderr","text":"\r8.2%"},"2113":{"name":"stderr","text":"\r8.2%"},"2114":{"name":"stderr","text":"\r8.2%"},"2115":{"name":"stderr","text":"\r8.2%"},"2116":{"name":"stderr","text":"\r8.2%"},"2117":{"name":"stderr","text":"\r8.2%"},"2118":{"name":"stderr","text":"\r8.2%"},"2119":{"name":"stderr","text":"\r8.2%"},"212":{"name":"stderr","text":"\r0.8%"},"2120":{"name":"stderr","text":"\r8.2%"},"2121":{"name":"stderr","text":"\r8.2%"},"2122":{"name":"stderr","text":"\r8.2%"},"2123":{"name":"stderr","text":"\r8.2%"},"2124":{"name":"stderr","text":"\r8.2%"},"2125":{"name":"stderr","text":"\r8.2%"},"2126":{"name":"stderr","text":"\r8.2%"},"2127":{"name":"stderr","text":"\r8.2%"},"2128":{"name":"stderr","text":"\r8.2%"},"2129":{"name":"stderr","text":"\r8.2%"},"213":{"name":"stderr","text":"\r0.8%"},"2130":{"name":"stderr","text":"\r8.2%"},"2131":{"name":"stderr","text":"\r8.2%"},"2132":{"name":"stderr","text":"\r8.2%"},"2133":{"name":"stderr","text":"\r8.3%"},"2134":{"name":"stderr","text":"\r8.3%"},"2135":{"name":"stderr","text":"\r8.3%"},"2136":{"name":"stderr","text":"\r8.3%"},"2137":{"name":"stderr","text":"\r8.3%"},"2138":{"name":"stderr","text":"\r8.3%"},"2139":{"name":"stderr","text":"\r8.3%"},"214":{"name":"stderr","text":"\r0.8%"},"2140":{"name":"stderr","text":"\r8.3%"},"2141":{"name":"stderr","text":"\r8.3%"},"2142":{"name":"stderr","text":"\r8.3%"},"2143":{"name":"stderr","text":"\r8.3%"},"2144":{"name":"stderr","text":"\r8.3%"},"2145":{"name":"stderr","text":"\r8.3%"},"2146":{"name":"stderr","text":"\r8.3%"},"2147":{"name":"stderr","text":"\r8.3%"},"2148":{"name":"stderr","text":"\r8.3%"},"2149":{"name":"stderr","text":"\r8.3%"},"215":{"name":"stderr","text":"\r0.8%"},"2150":{"name":"stderr","text":"\r8.3%"},"2151":{"name":"stderr","text":"\r8.3%"},"2152":{"name":"stderr","text":"\r8.3%"},"2153":{"name":"stderr","text":"\r8.3%"},"2154":{"name":"stderr","text":"\r8.3%"},"2155":{"name":"stderr","text":"\r8.3%"},"2156":{"name":"stderr","text":"\r8.3%"},"2157":{"name":"stderr","text":"\r8.3%"},"2158":{"name":"stderr","text":"\r8.3%"},"2159":{"name":"stderr","text":"\r8.4%"},"216":{"name":"stderr","text":"\r0.8%"},"2160":{"name":"stderr","text":"\r8.4%"},"2161":{"name":"stderr","text":"\r8.4%"},"2162":{"name":"stderr","text":"\r8.4%"},"2163":{"name":"stderr","text":"\r8.4%"},"2164":{"name":"stderr","text":"\r8.4%"},"2165":{"name":"stderr","text":"\r8.4%"},"2166":{"name":"stderr","text":"\r8.4%"},"2167":{"name":"stderr","text":"\r8.4%"},"2168":{"name":"stderr","text":"\r8.4%"},"2169":{"name":"stderr","text":"\r8.4%"},"217":{"name":"stderr","text":"\r0.8%"},"2170":{"name":"stderr","text":"\r8.4%"},"2171":{"name":"stderr","text":"\r8.4%"},"2172":{"name":"stderr","text":"\r8.4%"},"2173":{"name":"stderr","text":"\r8.4%"},"2174":{"name":"stderr","text":"\r8.4%"},"2175":{"name":"stderr","text":"\r8.4%"},"2176":{"name":"stderr","text":"\r8.4%"},"2177":{"name":"stderr","text":"\r8.4%"},"2178":{"name":"stderr","text":"\r8.4%"},"2179":{"name":"stderr","text":"\r8.4%"},"218":{"name":"stderr","text":"\r0.8%"},"2180":{"name":"stderr","text":"\r8.4%"},"2181":{"name":"stderr","text":"\r8.4%"},"2182":{"name":"stderr","text":"\r8.4%"},"2183":{"name":"stderr","text":"\r8.4%"},"2184":{"name":"stderr","text":"\r8.4%"},"2185":{"name":"stderr","text":"\r8.5%"},"2186":{"name":"stderr","text":"\r8.5%"},"2187":{"name":"stderr","text":"\r8.5%"},"2188":{"name":"stderr","text":"\r8.5%"},"2189":{"name":"stderr","text":"\r8.5%"},"219":{"name":"stderr","text":"\r0.8%"},"2190":{"name":"stderr","text":"\r8.5%"},"2191":{"name":"stderr","text":"\r8.5%"},"2192":{"name":"stderr","text":"\r8.5%"},"2193":{"name":"stderr","text":"\r8.5%"},"2194":{"name":"stderr","text":"\r8.5%"},"2195":{"name":"stderr","text":"\r8.5%"},"2196":{"name":"stderr","text":"\r8.5%"},"2197":{"name":"stderr","text":"\r8.5%"},"2198":{"name":"stderr","text":"\r8.5%"},"2199":{"name":"stderr","text":"\r8.5%"},"22":{"name":"stderr","text":"\r0.1%"},"220":{"name":"stderr","text":"\r0.8%"},"2200":{"name":"stderr","text":"\r8.5%"},"2201":{"name":"stderr","text":"\r8.5%"},"2202":{"name":"stderr","text":"\r8.5%"},"2203":{"name":"stderr","text":"\r8.5%"},"2204":{"name":"stderr","text":"\r8.5%"},"2205":{"name":"stderr","text":"\r8.5%"},"2206":{"name":"stderr","text":"\r8.5%"},"2207":{"name":"stderr","text":"\r8.5%"},"2208":{"name":"stderr","text":"\r8.5%"},"2209":{"name":"stderr","text":"\r8.5%"},"221":{"name":"stderr","text":"\r0.8%"},"2210":{"name":"stderr","text":"\r8.5%"},"2211":{"name":"stderr","text":"\r8.6%"},"2212":{"name":"stderr","text":"\r8.6%"},"2213":{"name":"stderr","text":"\r8.6%"},"2214":{"name":"stderr","text":"\r8.6%"},"2215":{"name":"stderr","text":"\r8.6%"},"2216":{"name":"stderr","text":"\r8.6%"},"2217":{"name":"stderr","text":"\r8.6%"},"2218":{"name":"stderr","text":"\r8.6%"},"2219":{"name":"stderr","text":"\r8.6%"},"222":{"name":"stderr","text":"\r0.8%"},"2220":{"name":"stderr","text":"\r8.6%"},"2221":{"name":"stderr","text":"\r8.6%"},"2222":{"name":"stderr","text":"\r8.6%"},"2223":{"name":"stderr","text":"\r8.6%"},"2224":{"name":"stderr","text":"\r8.6%"},"2225":{"name":"stderr","text":"\r8.6%"},"2226":{"name":"stderr","text":"\r8.6%"},"2227":{"name":"stderr","text":"\r8.6%"},"2228":{"name":"stderr","text":"\r8.6%"},"2229":{"name":"stderr","text":"\r8.6%"},"223":{"name":"stderr","text":"\r0.8%"},"2230":{"name":"stderr","text":"\r8.6%"},"2231":{"name":"stderr","text":"\r8.6%"},"2232":{"name":"stderr","text":"\r8.6%"},"2233":{"name":"stderr","text":"\r8.6%"},"2234":{"name":"stderr","text":"\r8.6%"},"2235":{"name":"stderr","text":"\r8.6%"},"2236":{"name":"stderr","text":"\r8.7%"},"2237":{"name":"stderr","text":"\r8.7%"},"2238":{"name":"stderr","text":"\r8.7%"},"2239":{"name":"stderr","text":"\r8.7%"},"224":{"name":"stderr","text":"\r0.9%"},"2240":{"name":"stderr","text":"\r8.7%"},"2241":{"name":"stderr","text":"\r8.7%"},"2242":{"name":"stderr","text":"\r8.7%"},"2243":{"name":"stderr","text":"\r8.7%"},"2244":{"name":"stderr","text":"\r8.7%"},"2245":{"name":"stderr","text":"\r8.7%"},"2246":{"name":"stderr","text":"\r8.7%"},"2247":{"name":"stderr","text":"\r8.7%"},"2248":{"name":"stderr","text":"\r8.7%"},"2249":{"name":"stderr","text":"\r8.7%"},"225":{"name":"stderr","text":"\r0.9%"},"2250":{"name":"stderr","text":"\r8.7%"},"2251":{"name":"stderr","text":"\r8.7%"},"2252":{"name":"stderr","text":"\r8.7%"},"2253":{"name":"stderr","text":"\r8.7%"},"2254":{"name":"stderr","text":"\r8.7%"},"2255":{"name":"stderr","text":"\r8.7%"},"2256":{"name":"stderr","text":"\r8.7%"},"2257":{"name":"stderr","text":"\r8.7%"},"2258":{"name":"stderr","text":"\r8.7%"},"2259":{"name":"stderr","text":"\r8.7%"},"226":{"name":"stderr","text":"\r0.9%"},"2260":{"name":"stderr","text":"\r8.7%"},"2261":{"name":"stderr","text":"\r8.7%"},"2262":{"name":"stderr","text":"\r8.8%"},"2263":{"name":"stderr","text":"\r8.8%"},"2264":{"name":"stderr","text":"\r8.8%"},"2265":{"name":"stderr","text":"\r8.8%"},"2266":{"name":"stderr","text":"\r8.8%"},"2267":{"name":"stderr","text":"\r8.8%"},"2268":{"name":"stderr","text":"\r8.8%"},"2269":{"name":"stderr","text":"\r8.8%"},"227":{"name":"stderr","text":"\r0.9%"},"2270":{"name":"stderr","text":"\r8.8%"},"2271":{"name":"stderr","text":"\r8.8%"},"2272":{"name":"stderr","text":"\r8.8%"},"2273":{"name":"stderr","text":"\r8.8%"},"2274":{"name":"stderr","text":"\r8.8%"},"2275":{"name":"stderr","text":"\r8.8%"},"2276":{"name":"stderr","text":"\r8.8%"},"2277":{"name":"stderr","text":"\r8.8%"},"2278":{"name":"stderr","text":"\r8.8%"},"2279":{"name":"stderr","text":"\r8.8%"},"228":{"name":"stderr","text":"\r0.9%"},"2280":{"name":"stderr","text":"\r8.8%"},"2281":{"name":"stderr","text":"\r8.8%"},"2282":{"name":"stderr","text":"\r8.8%"},"2283":{"name":"stderr","text":"\r8.8%"},"2284":{"name":"stderr","text":"\r8.8%"},"2285":{"name":"stderr","text":"\r8.8%"},"2286":{"name":"stderr","text":"\r8.8%"},"2287":{"name":"stderr","text":"\r8.8%"},"2288":{"name":"stderr","text":"\r8.9%"},"2289":{"name":"stderr","text":"\r8.9%"},"229":{"name":"stderr","text":"\r0.9%"},"2290":{"name":"stderr","text":"\r8.9%"},"2291":{"name":"stderr","text":"\r8.9%"},"2292":{"name":"stderr","text":"\r8.9%"},"2293":{"name":"stderr","text":"\r8.9%"},"2294":{"name":"stderr","text":"\r8.9%"},"2295":{"name":"stderr","text":"\r8.9%"},"2296":{"name":"stderr","text":"\r8.9%"},"2297":{"name":"stderr","text":"\r8.9%"},"2298":{"name":"stderr","text":"\r8.9%"},"2299":{"name":"stderr","text":"\r8.9%"},"23":{"name":"stderr","text":"\r0.1%"},"230":{"name":"stderr","text":"\r0.9%"},"2300":{"name":"stderr","text":"\r8.9%"},"2301":{"name":"stderr","text":"\r8.9%"},"2302":{"name":"stderr","text":"\r8.9%"},"2303":{"name":"stderr","text":"\r8.9%"},"2304":{"name":"stderr","text":"\r8.9%"},"2305":{"name":"stderr","text":"\r8.9%"},"2306":{"name":"stderr","text":"\r8.9%"},"2307":{"name":"stderr","text":"\r8.9%"},"2308":{"name":"stderr","text":"\r8.9%"},"2309":{"name":"stderr","text":"\r8.9%"},"231":{"name":"stderr","text":"\r0.9%"},"2310":{"name":"stderr","text":"\r8.9%"},"2311":{"name":"stderr","text":"\r8.9%"},"2312":{"name":"stderr","text":"\r8.9%"},"2313":{"name":"stderr","text":"\r8.9%"},"2314":{"name":"stderr","text":"\r9.0%"},"2315":{"name":"stderr","text":"\r9.0%"},"2316":{"name":"stderr","text":"\r9.0%"},"2317":{"name":"stderr","text":"\r9.0%"},"2318":{"name":"stderr","text":"\r9.0%"},"2319":{"name":"stderr","text":"\r9.0%"},"232":{"name":"stderr","text":"\r0.9%"},"2320":{"name":"stderr","text":"\r9.0%"},"2321":{"name":"stderr","text":"\r9.0%"},"2322":{"name":"stderr","text":"\r9.0%"},"2323":{"name":"stderr","text":"\r9.0%"},"2324":{"name":"stderr","text":"\r9.0%"},"2325":{"name":"stderr","text":"\r9.0%"},"2326":{"name":"stderr","text":"\r9.0%"},"2327":{"name":"stderr","text":"\r9.0%"},"2328":{"name":"stderr","text":"\r9.0%"},"2329":{"name":"stderr","text":"\r9.0%"},"233":{"name":"stderr","text":"\r0.9%"},"2330":{"name":"stderr","text":"\r9.0%"},"2331":{"name":"stderr","text":"\r9.0%"},"2332":{"name":"stderr","text":"\r9.0%"},"2333":{"name":"stderr","text":"\r9.0%"},"2334":{"name":"stderr","text":"\r9.0%"},"2335":{"name":"stderr","text":"\r9.0%"},"2336":{"name":"stderr","text":"\r9.0%"},"2337":{"name":"stderr","text":"\r9.0%"},"2338":{"name":"stderr","text":"\r9.0%"},"2339":{"name":"stderr","text":"\r9.0%"},"234":{"name":"stderr","text":"\r0.9%"},"2340":{"name":"stderr","text":"\r9.1%"},"2341":{"name":"stderr","text":"\r9.1%"},"2342":{"name":"stderr","text":"\r9.1%"},"2343":{"name":"stderr","text":"\r9.1%"},"2344":{"name":"stderr","text":"\r9.1%"},"2345":{"name":"stderr","text":"\r9.1%"},"2346":{"name":"stderr","text":"\r9.1%"},"2347":{"name":"stderr","text":"\r9.1%"},"2348":{"name":"stderr","text":"\r9.1%"},"2349":{"name":"stderr","text":"\r9.1%"},"235":{"name":"stderr","text":"\r0.9%"},"2350":{"name":"stderr","text":"\r9.1%"},"2351":{"name":"stderr","text":"\r9.1%"},"2352":{"name":"stderr","text":"\r9.1%"},"2353":{"name":"stderr","text":"\r9.1%"},"2354":{"name":"stderr","text":"\r9.1%"},"2355":{"name":"stderr","text":"\r9.1%"},"2356":{"name":"stderr","text":"\r9.1%"},"2357":{"name":"stderr","text":"\r9.1%"},"2358":{"name":"stderr","text":"\r9.1%"},"2359":{"name":"stderr","text":"\r9.1%"},"236":{"name":"stderr","text":"\r0.9%"},"2360":{"name":"stderr","text":"\r9.1%"},"2361":{"name":"stderr","text":"\r9.1%"},"2362":{"name":"stderr","text":"\r9.1%"},"2363":{"name":"stderr","text":"\r9.1%"},"2364":{"name":"stderr","text":"\r9.1%"},"2365":{"name":"stderr","text":"\r9.2%"},"2366":{"name":"stderr","text":"\r9.2%"},"2367":{"name":"stderr","text":"\r9.2%"},"2368":{"name":"stderr","text":"\r9.2%"},"2369":{"name":"stderr","text":"\r9.2%"},"237":{"name":"stderr","text":"\r0.9%"},"2370":{"name":"stderr","text":"\r9.2%"},"2371":{"name":"stderr","text":"\r9.2%"},"2372":{"name":"stderr","text":"\r9.2%"},"2373":{"name":"stderr","text":"\r9.2%"},"2374":{"name":"stderr","text":"\r9.2%"},"2375":{"name":"stderr","text":"\r9.2%"},"2376":{"name":"stderr","text":"\r9.2%"},"2377":{"name":"stderr","text":"\r9.2%"},"2378":{"name":"stderr","text":"\r9.2%"},"2379":{"name":"stderr","text":"\r9.2%"},"238":{"name":"stderr","text":"\r0.9%"},"2380":{"name":"stderr","text":"\r9.2%"},"2381":{"name":"stderr","text":"\r9.2%"},"2382":{"name":"stderr","text":"\r9.2%"},"2383":{"name":"stderr","text":"\r9.2%"},"2384":{"name":"stderr","text":"\r9.2%"},"2385":{"name":"stderr","text":"\r9.2%"},"2386":{"name":"stderr","text":"\r9.2%"},"2387":{"name":"stderr","text":"\r9.2%"},"2388":{"name":"stderr","text":"\r9.2%"},"2389":{"name":"stderr","text":"\r9.2%"},"239":{"name":"stderr","text":"\r0.9%"},"2390":{"name":"stderr","text":"\r9.2%"},"2391":{"name":"stderr","text":"\r9.3%"},"2392":{"name":"stderr","text":"\r9.3%"},"2393":{"name":"stderr","text":"\r9.3%"},"2394":{"name":"stderr","text":"\r9.3%"},"2395":{"name":"stderr","text":"\r9.3%"},"2396":{"name":"stderr","text":"\r9.3%"},"2397":{"name":"stderr","text":"\r9.3%"},"2398":{"name":"stderr","text":"\r9.3%"},"2399":{"name":"stderr","text":"\r9.3%"},"24":{"name":"stderr","text":"\r0.1%"},"240":{"name":"stderr","text":"\r0.9%"},"2400":{"name":"stderr","text":"\r9.3%"},"2401":{"name":"stderr","text":"\r9.3%"},"2402":{"name":"stderr","text":"\r9.3%"},"2403":{"name":"stderr","text":"\r9.3%"},"2404":{"name":"stderr","text":"\r9.3%"},"2405":{"name":"stderr","text":"\r9.3%"},"2406":{"name":"stderr","text":"\r9.3%"},"2407":{"name":"stderr","text":"\r9.3%"},"2408":{"name":"stderr","text":"\r9.3%"},"2409":{"name":"stderr","text":"\r9.3%"},"241":{"name":"stderr","text":"\r0.9%"},"2410":{"name":"stderr","text":"\r9.3%"},"2411":{"name":"stderr","text":"\r9.3%"},"2412":{"name":"stderr","text":"\r9.3%"},"2413":{"name":"stderr","text":"\r9.3%"},"2414":{"name":"stderr","text":"\r9.3%"},"2415":{"name":"stderr","text":"\r9.3%"},"2416":{"name":"stderr","text":"\r9.3%"},"2417":{"name":"stderr","text":"\r9.4%"},"2418":{"name":"stderr","text":"\r9.4%"},"2419":{"name":"stderr","text":"\r9.4%"},"242":{"name":"stderr","text":"\r0.9%"},"2420":{"name":"stderr","text":"\r9.4%"},"2421":{"name":"stderr","text":"\r9.4%"},"2422":{"name":"stderr","text":"\r9.4%"},"2423":{"name":"stderr","text":"\r9.4%"},"2424":{"name":"stderr","text":"\r9.4%"},"2425":{"name":"stderr","text":"\r9.4%"},"2426":{"name":"stderr","text":"\r9.4%"},"2427":{"name":"stderr","text":"\r9.4%"},"2428":{"name":"stderr","text":"\r9.4%"},"2429":{"name":"stderr","text":"\r9.4%"},"243":{"name":"stderr","text":"\r0.9%"},"2430":{"name":"stderr","text":"\r9.4%"},"2431":{"name":"stderr","text":"\r9.4%"},"2432":{"name":"stderr","text":"\r9.4%"},"2433":{"name":"stderr","text":"\r9.4%"},"2434":{"name":"stderr","text":"\r9.4%"},"2435":{"name":"stderr","text":"\r9.4%"},"2436":{"name":"stderr","text":"\r9.4%"},"2437":{"name":"stderr","text":"\r9.4%"},"2438":{"name":"stderr","text":"\r9.4%"},"2439":{"name":"stderr","text":"\r9.4%"},"244":{"name":"stderr","text":"\r0.9%"},"2440":{"name":"stderr","text":"\r9.4%"},"2441":{"name":"stderr","text":"\r9.4%"},"2442":{"name":"stderr","text":"\r9.4%"},"2443":{"name":"stderr","text":"\r9.5%"},"2444":{"name":"stderr","text":"\r9.5%"},"2445":{"name":"stderr","text":"\r9.5%"},"2446":{"name":"stderr","text":"\r9.5%"},"2447":{"name":"stderr","text":"\r9.5%"},"2448":{"name":"stderr","text":"\r9.5%"},"2449":{"name":"stderr","text":"\r9.5%"},"245":{"name":"stderr","text":"\r0.9%"},"2450":{"name":"stderr","text":"\r9.5%"},"2451":{"name":"stderr","text":"\r9.5%"},"2452":{"name":"stderr","text":"\r9.5%"},"2453":{"name":"stderr","text":"\r9.5%"},"2454":{"name":"stderr","text":"\r9.5%"},"2455":{"name":"stderr","text":"\r9.5%"},"2456":{"name":"stderr","text":"\r9.5%"},"2457":{"name":"stderr","text":"\r9.5%"},"2458":{"name":"stderr","text":"\r9.5%"},"2459":{"name":"stderr","text":"\r9.5%"},"246":{"name":"stderr","text":"\r0.9%"},"2460":{"name":"stderr","text":"\r9.5%"},"2461":{"name":"stderr","text":"\r9.5%"},"2462":{"name":"stderr","text":"\r9.5%"},"2463":{"name":"stderr","text":"\r9.5%"},"2464":{"name":"stderr","text":"\r9.5%"},"2465":{"name":"stderr","text":"\r9.5%"},"2466":{"name":"stderr","text":"\r9.5%"},"2467":{"name":"stderr","text":"\r9.5%"},"2468":{"name":"stderr","text":"\r9.5%"},"2469":{"name":"stderr","text":"\r9.6%"},"247":{"name":"stderr","text":"\r0.9%"},"2470":{"name":"stderr","text":"\r9.6%"},"2471":{"name":"stderr","text":"\r9.6%"},"2472":{"name":"stderr","text":"\r9.6%"},"2473":{"name":"stderr","text":"\r9.6%"},"2474":{"name":"stderr","text":"\r9.6%"},"2475":{"name":"stderr","text":"\r9.6%"},"2476":{"name":"stderr","text":"\r9.6%"},"2477":{"name":"stderr","text":"\r9.6%"},"2478":{"name":"stderr","text":"\r9.6%"},"2479":{"name":"stderr","text":"\r9.6%"},"248":{"name":"stderr","text":"\r0.9%"},"2480":{"name":"stderr","text":"\r9.6%"},"2481":{"name":"stderr","text":"\r9.6%"},"2482":{"name":"stderr","text":"\r9.6%"},"2483":{"name":"stderr","text":"\r9.6%"},"2484":{"name":"stderr","text":"\r9.6%"},"2485":{"name":"stderr","text":"\r9.6%"},"2486":{"name":"stderr","text":"\r9.6%"},"2487":{"name":"stderr","text":"\r9.6%"},"2488":{"name":"stderr","text":"\r9.6%"},"2489":{"name":"stderr","text":"\r9.6%"},"249":{"name":"stderr","text":"\r0.9%"},"2490":{"name":"stderr","text":"\r9.6%"},"2491":{"name":"stderr","text":"\r9.6%"},"2492":{"name":"stderr","text":"\r9.6%"},"2493":{"name":"stderr","text":"\r9.6%"},"2494":{"name":"stderr","text":"\r9.7%"},"2495":{"name":"stderr","text":"\r9.7%"},"2496":{"name":"stderr","text":"\r9.7%"},"2497":{"name":"stderr","text":"\r9.7%"},"2498":{"name":"stderr","text":"\r9.7%"},"2499":{"name":"stderr","text":"\r9.7%"},"25":{"name":"stderr","text":"\r0.1%"},"250":{"name":"stderr","text":"\r1.0%"},"2500":{"name":"stderr","text":"\r9.7%"},"2501":{"name":"stderr","text":"\r9.7%"},"2502":{"name":"stderr","text":"\r9.7%"},"2503":{"name":"stderr","text":"\r9.7%"},"2504":{"name":"stderr","text":"\r9.7%"},"2505":{"name":"stderr","text":"\r9.7%"},"2506":{"name":"stderr","text":"\r9.7%"},"2507":{"name":"stderr","text":"\r9.7%"},"2508":{"name":"stderr","text":"\r9.7%"},"2509":{"name":"stderr","text":"\r9.7%"},"251":{"name":"stderr","text":"\r1.0%"},"2510":{"name":"stderr","text":"\r9.7%"},"2511":{"name":"stderr","text":"\r9.7%"},"2512":{"name":"stderr","text":"\r9.7%"},"2513":{"name":"stderr","text":"\r9.7%"},"2514":{"name":"stderr","text":"\r9.7%"},"2515":{"name":"stderr","text":"\r9.7%"},"2516":{"name":"stderr","text":"\r9.7%"},"2517":{"name":"stderr","text":"\r9.7%"},"2518":{"name":"stderr","text":"\r9.7%"},"2519":{"name":"stderr","text":"\r9.7%"},"252":{"name":"stderr","text":"\r1.0%"},"2520":{"name":"stderr","text":"\r9.8%"},"2521":{"name":"stderr","text":"\r9.8%"},"2522":{"name":"stderr","text":"\r9.8%"},"2523":{"name":"stderr","text":"\r9.8%"},"2524":{"name":"stderr","text":"\r9.8%"},"2525":{"name":"stderr","text":"\r9.8%"},"2526":{"name":"stderr","text":"\r9.8%"},"2527":{"name":"stderr","text":"\r9.8%"},"2528":{"name":"stderr","text":"\r9.8%"},"2529":{"name":"stderr","text":"\r9.8%"},"253":{"name":"stderr","text":"\r1.0%"},"2530":{"name":"stderr","text":"\r9.8%"},"2531":{"name":"stderr","text":"\r9.8%"},"2532":{"name":"stderr","text":"\r9.8%"},"2533":{"name":"stderr","text":"\r9.8%"},"2534":{"name":"stderr","text":"\r9.8%"},"2535":{"name":"stderr","text":"\r9.8%"},"2536":{"name":"stderr","text":"\r9.8%"},"2537":{"name":"stderr","text":"\r9.8%"},"2538":{"name":"stderr","text":"\r9.8%"},"2539":{"name":"stderr","text":"\r9.8%"},"254":{"name":"stderr","text":"\r1.0%"},"2540":{"name":"stderr","text":"\r9.8%"},"2541":{"name":"stderr","text":"\r9.8%"},"2542":{"name":"stderr","text":"\r9.8%"},"2543":{"name":"stderr","text":"\r9.8%"},"2544":{"name":"stderr","text":"\r9.8%"},"2545":{"name":"stderr","text":"\r9.8%"},"2546":{"name":"stderr","text":"\r9.9%"},"2547":{"name":"stderr","text":"\r9.9%"},"2548":{"name":"stderr","text":"\r9.9%"},"2549":{"name":"stderr","text":"\r9.9%"},"255":{"name":"stderr","text":"\r1.0%"},"2550":{"name":"stderr","text":"\r9.9%"},"2551":{"name":"stderr","text":"\r9.9%"},"2552":{"name":"stderr","text":"\r9.9%"},"2553":{"name":"stderr","text":"\r9.9%"},"2554":{"name":"stderr","text":"\r9.9%"},"2555":{"name":"stderr","text":"\r9.9%"},"2556":{"name":"stderr","text":"\r9.9%"},"2557":{"name":"stderr","text":"\r9.9%"},"2558":{"name":"stderr","text":"\r9.9%"},"2559":{"name":"stderr","text":"\r9.9%"},"256":{"name":"stderr","text":"\r1.0%"},"2560":{"name":"stderr","text":"\r9.9%"},"2561":{"name":"stderr","text":"\r9.9%"},"2562":{"name":"stderr","text":"\r9.9%"},"2563":{"name":"stderr","text":"\r9.9%"},"2564":{"name":"stderr","text":"\r9.9%"},"2565":{"name":"stderr","text":"\r9.9%"},"2566":{"name":"stderr","text":"\r9.9%"},"2567":{"name":"stderr","text":"\r9.9%"},"2568":{"name":"stderr","text":"\r9.9%"},"2569":{"name":"stderr","text":"\r9.9%"},"257":{"name":"stderr","text":"\r1.0%"},"2570":{"name":"stderr","text":"\r9.9%"},"2571":{"name":"stderr","text":"\r9.9%"},"2572":{"name":"stderr","text":"\r10.0%"},"2573":{"name":"stderr","text":"\r10.0%"},"2574":{"name":"stderr","text":"\r10.0%"},"2575":{"name":"stderr","text":"\r10.0%"},"2576":{"name":"stderr","text":"\r10.0%"},"2577":{"name":"stderr","text":"\r10.0%"},"2578":{"name":"stderr","text":"\r10.0%"},"2579":{"name":"stderr","text":"\r10.0%"},"258":{"name":"stderr","text":"\r1.0%"},"2580":{"name":"stderr","text":"\r10.0%"},"2581":{"name":"stderr","text":"\r10.0%"},"2582":{"name":"stderr","text":"\r10.0%"},"2583":{"name":"stderr","text":"\r10.0%"},"2584":{"name":"stderr","text":"\r10.0%"},"2585":{"name":"stderr","text":"\r10.0%"},"2586":{"name":"stderr","text":"\r10.0%"},"2587":{"name":"stderr","text":"\r10.0%"},"2588":{"name":"stderr","text":"\r10.0%"},"2589":{"name":"stderr","text":"\r10.0%"},"259":{"name":"stderr","text":"\r1.0%"},"2590":{"name":"stderr","text":"\r10.0%"},"2591":{"name":"stderr","text":"\r10.0%"},"2592":{"name":"stderr","text":"\r10.0%"},"2593":{"name":"stderr","text":"\r10.0%"},"2594":{"name":"stderr","text":"\r10.0%"},"2595":{"name":"stderr","text":"\r10.0%"},"2596":{"name":"stderr","text":"\r10.0%"},"2597":{"name":"stderr","text":"\r10.0%"},"2598":{"name":"stderr","text":"\r10.1%"},"2599":{"name":"stderr","text":"\r10.1%"},"26":{"name":"stderr","text":"\r0.1%"},"260":{"name":"stderr","text":"\r1.0%"},"2600":{"name":"stderr","text":"\r10.1%"},"2601":{"name":"stderr","text":"\r10.1%"},"2602":{"name":"stderr","text":"\r10.1%"},"2603":{"name":"stderr","text":"\r10.1%"},"2604":{"name":"stderr","text":"\r10.1%"},"2605":{"name":"stderr","text":"\r10.1%"},"2606":{"name":"stderr","text":"\r10.1%"},"2607":{"name":"stderr","text":"\r10.1%"},"2608":{"name":"stderr","text":"\r10.1%"},"2609":{"name":"stderr","text":"\r10.1%"},"261":{"name":"stderr","text":"\r1.0%"},"2610":{"name":"stderr","text":"\r10.1%"},"2611":{"name":"stderr","text":"\r10.1%"},"2612":{"name":"stderr","text":"\r10.1%"},"2613":{"name":"stderr","text":"\r10.1%"},"2614":{"name":"stderr","text":"\r10.1%"},"2615":{"name":"stderr","text":"\r10.1%"},"2616":{"name":"stderr","text":"\r10.1%"},"2617":{"name":"stderr","text":"\r10.1%"},"2618":{"name":"stderr","text":"\r10.1%"},"2619":{"name":"stderr","text":"\r10.1%"},"262":{"name":"stderr","text":"\r1.0%"},"2620":{"name":"stderr","text":"\r10.1%"},"2621":{"name":"stderr","text":"\r10.1%"},"2622":{"name":"stderr","text":"\r10.1%"},"2623":{"name":"stderr","text":"\r10.2%"},"2624":{"name":"stderr","text":"\r10.2%"},"2625":{"name":"stderr","text":"\r10.2%"},"2626":{"name":"stderr","text":"\r10.2%"},"2627":{"name":"stderr","text":"\r10.2%"},"2628":{"name":"stderr","text":"\r10.2%"},"2629":{"name":"stderr","text":"\r10.2%"},"263":{"name":"stderr","text":"\r1.0%"},"2630":{"name":"stderr","text":"\r10.2%"},"2631":{"name":"stderr","text":"\r10.2%"},"2632":{"name":"stderr","text":"\r10.2%"},"2633":{"name":"stderr","text":"\r10.2%"},"2634":{"name":"stderr","text":"\r10.2%"},"2635":{"name":"stderr","text":"\r10.2%"},"2636":{"name":"stderr","text":"\r10.2%"},"2637":{"name":"stderr","text":"\r10.2%"},"2638":{"name":"stderr","text":"\r10.2%"},"2639":{"name":"stderr","text":"\r10.2%"},"264":{"name":"stderr","text":"\r1.0%"},"2640":{"name":"stderr","text":"\r10.2%"},"2641":{"name":"stderr","text":"\r10.2%"},"2642":{"name":"stderr","text":"\r10.2%"},"2643":{"name":"stderr","text":"\r10.2%"},"2644":{"name":"stderr","text":"\r10.2%"},"2645":{"name":"stderr","text":"\r10.2%"},"2646":{"name":"stderr","text":"\r10.2%"},"2647":{"name":"stderr","text":"\r10.2%"},"2648":{"name":"stderr","text":"\r10.2%"},"2649":{"name":"stderr","text":"\r10.3%"},"265":{"name":"stderr","text":"\r1.0%"},"2650":{"name":"stderr","text":"\r10.3%"},"2651":{"name":"stderr","text":"\r10.3%"},"2652":{"name":"stderr","text":"\r10.3%"},"2653":{"name":"stderr","text":"\r10.3%"},"2654":{"name":"stderr","text":"\r10.3%"},"2655":{"name":"stderr","text":"\r10.3%"},"2656":{"name":"stderr","text":"\r10.3%"},"2657":{"name":"stderr","text":"\r10.3%"},"2658":{"name":"stderr","text":"\r10.3%"},"2659":{"name":"stderr","text":"\r10.3%"},"266":{"name":"stderr","text":"\r1.0%"},"2660":{"name":"stderr","text":"\r10.3%"},"2661":{"name":"stderr","text":"\r10.3%"},"2662":{"name":"stderr","text":"\r10.3%"},"2663":{"name":"stderr","text":"\r10.3%"},"2664":{"name":"stderr","text":"\r10.3%"},"2665":{"name":"stderr","text":"\r10.3%"},"2666":{"name":"stderr","text":"\r10.3%"},"2667":{"name":"stderr","text":"\r10.3%"},"2668":{"name":"stderr","text":"\r10.3%"},"2669":{"name":"stderr","text":"\r10.3%"},"267":{"name":"stderr","text":"\r1.0%"},"2670":{"name":"stderr","text":"\r10.3%"},"2671":{"name":"stderr","text":"\r10.3%"},"2672":{"name":"stderr","text":"\r10.3%"},"2673":{"name":"stderr","text":"\r10.3%"},"2674":{"name":"stderr","text":"\r10.3%"},"2675":{"name":"stderr","text":"\r10.4%"},"2676":{"name":"stderr","text":"\r10.4%"},"2677":{"name":"stderr","text":"\r10.4%"},"2678":{"name":"stderr","text":"\r10.4%"},"2679":{"name":"stderr","text":"\r10.4%"},"268":{"name":"stderr","text":"\r1.0%"},"2680":{"name":"stderr","text":"\r10.4%"},"2681":{"name":"stderr","text":"\r10.4%"},"2682":{"name":"stderr","text":"\r10.4%"},"2683":{"name":"stderr","text":"\r10.4%"},"2684":{"name":"stderr","text":"\r10.4%"},"2685":{"name":"stderr","text":"\r10.4%"},"2686":{"name":"stderr","text":"\r10.4%"},"2687":{"name":"stderr","text":"\r10.4%"},"2688":{"name":"stderr","text":"\r10.4%"},"2689":{"name":"stderr","text":"\r10.4%"},"269":{"name":"stderr","text":"\r1.0%"},"2690":{"name":"stderr","text":"\r10.4%"},"2691":{"name":"stderr","text":"\r10.4%"},"2692":{"name":"stderr","text":"\r10.4%"},"2693":{"name":"stderr","text":"\r10.4%"},"2694":{"name":"stderr","text":"\r10.4%"},"2695":{"name":"stderr","text":"\r10.4%"},"2696":{"name":"stderr","text":"\r10.4%"},"2697":{"name":"stderr","text":"\r10.4%"},"2698":{"name":"stderr","text":"\r10.4%"},"2699":{"name":"stderr","text":"\r10.4%"},"27":{"name":"stderr","text":"\r0.1%"},"270":{"name":"stderr","text":"\r1.0%"},"2700":{"name":"stderr","text":"\r10.4%"},"2701":{"name":"stderr","text":"\r10.5%"},"2702":{"name":"stderr","text":"\r10.5%"},"2703":{"name":"stderr","text":"\r10.5%"},"2704":{"name":"stderr","text":"\r10.5%"},"2705":{"name":"stderr","text":"\r10.5%"},"2706":{"name":"stderr","text":"\r10.5%"},"2707":{"name":"stderr","text":"\r10.5%"},"2708":{"name":"stderr","text":"\r10.5%"},"2709":{"name":"stderr","text":"\r10.5%"},"271":{"name":"stderr","text":"\r1.0%"},"2710":{"name":"stderr","text":"\r10.5%"},"2711":{"name":"stderr","text":"\r10.5%"},"2712":{"name":"stderr","text":"\r10.5%"},"2713":{"name":"stderr","text":"\r10.5%"},"2714":{"name":"stderr","text":"\r10.5%"},"2715":{"name":"stderr","text":"\r10.5%"},"2716":{"name":"stderr","text":"\r10.5%"},"2717":{"name":"stderr","text":"\r10.5%"},"2718":{"name":"stderr","text":"\r10.5%"},"2719":{"name":"stderr","text":"\r10.5%"},"272":{"name":"stderr","text":"\r1.0%"},"2720":{"name":"stderr","text":"\r10.5%"},"2721":{"name":"stderr","text":"\r10.5%"},"2722":{"name":"stderr","text":"\r10.5%"},"2723":{"name":"stderr","text":"\r10.5%"},"2724":{"name":"stderr","text":"\r10.5%"},"2725":{"name":"stderr","text":"\r10.5%"},"2726":{"name":"stderr","text":"\r10.5%"},"2727":{"name":"stderr","text":"\r10.6%"},"2728":{"name":"stderr","text":"\r10.6%"},"2729":{"name":"stderr","text":"\r10.6%"},"273":{"name":"stderr","text":"\r1.0%"},"2730":{"name":"stderr","text":"\r10.6%"},"2731":{"name":"stderr","text":"\r10.6%"},"2732":{"name":"stderr","text":"\r10.6%"},"2733":{"name":"stderr","text":"\r10.6%"},"2734":{"name":"stderr","text":"\r10.6%"},"2735":{"name":"stderr","text":"\r10.6%"},"2736":{"name":"stderr","text":"\r10.6%"},"2737":{"name":"stderr","text":"\r10.6%"},"2738":{"name":"stderr","text":"\r10.6%"},"2739":{"name":"stderr","text":"\r10.6%"},"274":{"name":"stderr","text":"\r1.0%"},"2740":{"name":"stderr","text":"\r10.6%"},"2741":{"name":"stderr","text":"\r10.6%"},"2742":{"name":"stderr","text":"\r10.6%"},"2743":{"name":"stderr","text":"\r10.6%"},"2744":{"name":"stderr","text":"\r10.6%"},"2745":{"name":"stderr","text":"\r10.6%"},"2746":{"name":"stderr","text":"\r10.6%"},"2747":{"name":"stderr","text":"\r10.6%"},"2748":{"name":"stderr","text":"\r10.6%"},"2749":{"name":"stderr","text":"\r10.6%"},"275":{"name":"stderr","text":"\r1.1%"},"2750":{"name":"stderr","text":"\r10.6%"},"2751":{"name":"stderr","text":"\r10.6%"},"2752":{"name":"stderr","text":"\r10.7%"},"2753":{"name":"stderr","text":"\r10.7%"},"2754":{"name":"stderr","text":"\r10.7%"},"2755":{"name":"stderr","text":"\r10.7%"},"2756":{"name":"stderr","text":"\r10.7%"},"2757":{"name":"stderr","text":"\r10.7%"},"2758":{"name":"stderr","text":"\r10.7%"},"2759":{"name":"stderr","text":"\r10.7%"},"276":{"name":"stderr","text":"\r1.1%"},"2760":{"name":"stderr","text":"\r10.7%"},"2761":{"name":"stderr","text":"\r10.7%"},"2762":{"name":"stderr","text":"\r10.7%"},"2763":{"name":"stderr","text":"\r10.7%"},"2764":{"name":"stderr","text":"\r10.7%"},"2765":{"name":"stderr","text":"\r10.7%"},"2766":{"name":"stderr","text":"\r10.7%"},"2767":{"name":"stderr","text":"\r10.7%"},"2768":{"name":"stderr","text":"\r10.7%"},"2769":{"name":"stderr","text":"\r10.7%"},"277":{"name":"stderr","text":"\r1.1%"},"2770":{"name":"stderr","text":"\r10.7%"},"2771":{"name":"stderr","text":"\r10.7%"},"2772":{"name":"stderr","text":"\r10.7%"},"2773":{"name":"stderr","text":"\r10.7%"},"2774":{"name":"stderr","text":"\r10.7%"},"2775":{"name":"stderr","text":"\r10.7%"},"2776":{"name":"stderr","text":"\r10.7%"},"2777":{"name":"stderr","text":"\r10.7%"},"2778":{"name":"stderr","text":"\r10.8%"},"2779":{"name":"stderr","text":"\r10.8%"},"278":{"name":"stderr","text":"\r1.1%"},"2780":{"name":"stderr","text":"\r10.8%"},"2781":{"name":"stderr","text":"\r10.8%"},"2782":{"name":"stderr","text":"\r10.8%"},"2783":{"name":"stderr","text":"\r10.8%"},"2784":{"name":"stderr","text":"\r10.8%"},"2785":{"name":"stderr","text":"\r10.8%"},"2786":{"name":"stderr","text":"\r10.8%"},"2787":{"name":"stderr","text":"\r10.8%"},"2788":{"name":"stderr","text":"\r10.8%"},"2789":{"name":"stderr","text":"\r10.8%"},"279":{"name":"stderr","text":"\r1.1%"},"2790":{"name":"stderr","text":"\r10.8%"},"2791":{"name":"stderr","text":"\r10.8%"},"2792":{"name":"stderr","text":"\r10.8%"},"2793":{"name":"stderr","text":"\r10.8%"},"2794":{"name":"stderr","text":"\r10.8%"},"2795":{"name":"stderr","text":"\r10.8%"},"2796":{"name":"stderr","text":"\r10.8%"},"2797":{"name":"stderr","text":"\r10.8%"},"2798":{"name":"stderr","text":"\r10.8%"},"2799":{"name":"stderr","text":"\r10.8%"},"28":{"name":"stderr","text":"\r0.1%"},"280":{"name":"stderr","text":"\r1.1%"},"2800":{"name":"stderr","text":"\r10.8%"},"2801":{"name":"stderr","text":"\r10.8%"},"2802":{"name":"stderr","text":"\r10.8%"},"2803":{"name":"stderr","text":"\r10.8%"},"2804":{"name":"stderr","text":"\r10.9%"},"2805":{"name":"stderr","text":"\r10.9%"},"2806":{"name":"stderr","text":"\r10.9%"},"2807":{"name":"stderr","text":"\r10.9%"},"2808":{"name":"stderr","text":"\r10.9%"},"2809":{"name":"stderr","text":"\r10.9%"},"281":{"name":"stderr","text":"\r1.1%"},"2810":{"name":"stderr","text":"\r10.9%"},"2811":{"name":"stderr","text":"\r10.9%"},"2812":{"name":"stderr","text":"\r10.9%"},"2813":{"name":"stderr","text":"\r10.9%"},"2814":{"name":"stderr","text":"\r10.9%"},"2815":{"name":"stderr","text":"\r10.9%"},"2816":{"name":"stderr","text":"\r10.9%"},"2817":{"name":"stderr","text":"\r10.9%"},"2818":{"name":"stderr","text":"\r10.9%"},"2819":{"name":"stderr","text":"\r10.9%"},"282":{"name":"stderr","text":"\r1.1%"},"2820":{"name":"stderr","text":"\r10.9%"},"2821":{"name":"stderr","text":"\r10.9%"},"2822":{"name":"stderr","text":"\r10.9%"},"2823":{"name":"stderr","text":"\r10.9%"},"2824":{"name":"stderr","text":"\r10.9%"},"2825":{"name":"stderr","text":"\r10.9%"},"2826":{"name":"stderr","text":"\r10.9%"},"2827":{"name":"stderr","text":"\r10.9%"},"2828":{"name":"stderr","text":"\r10.9%"},"2829":{"name":"stderr","text":"\r10.9%"},"283":{"name":"stderr","text":"\r1.1%"},"2830":{"name":"stderr","text":"\r11.0%"},"2831":{"name":"stderr","text":"\r11.0%"},"2832":{"name":"stderr","text":"\r11.0%"},"2833":{"name":"stderr","text":"\r11.0%"},"2834":{"name":"stderr","text":"\r11.0%"},"2835":{"name":"stderr","text":"\r11.0%"},"2836":{"name":"stderr","text":"\r11.0%"},"2837":{"name":"stderr","text":"\r11.0%"},"2838":{"name":"stderr","text":"\r11.0%"},"2839":{"name":"stderr","text":"\r11.0%"},"284":{"name":"stderr","text":"\r1.1%"},"2840":{"name":"stderr","text":"\r11.0%"},"2841":{"name":"stderr","text":"\r11.0%"},"2842":{"name":"stderr","text":"\r11.0%"},"2843":{"name":"stderr","text":"\r11.0%"},"2844":{"name":"stderr","text":"\r11.0%"},"2845":{"name":"stderr","text":"\r11.0%"},"2846":{"name":"stderr","text":"\r11.0%"},"2847":{"name":"stderr","text":"\r11.0%"},"2848":{"name":"stderr","text":"\r11.0%"},"2849":{"name":"stderr","text":"\r11.0%"},"285":{"name":"stderr","text":"\r1.1%"},"2850":{"name":"stderr","text":"\r11.0%"},"2851":{"name":"stderr","text":"\r11.0%"},"2852":{"name":"stderr","text":"\r11.0%"},"2853":{"name":"stderr","text":"\r11.0%"},"2854":{"name":"stderr","text":"\r11.0%"},"2855":{"name":"stderr","text":"\r11.0%"},"2856":{"name":"stderr","text":"\r11.1%"},"2857":{"name":"stderr","text":"\r11.1%"},"2858":{"name":"stderr","text":"\r11.1%"},"2859":{"name":"stderr","text":"\r11.1%"},"286":{"name":"stderr","text":"\r1.1%"},"2860":{"name":"stderr","text":"\r11.1%"},"2861":{"name":"stderr","text":"\r11.1%"},"2862":{"name":"stderr","text":"\r11.1%"},"2863":{"name":"stderr","text":"\r11.1%"},"2864":{"name":"stderr","text":"\r11.1%"},"2865":{"name":"stderr","text":"\r11.1%"},"2866":{"name":"stderr","text":"\r11.1%"},"2867":{"name":"stderr","text":"\r11.1%"},"2868":{"name":"stderr","text":"\r11.1%"},"2869":{"name":"stderr","text":"\r11.1%"},"287":{"name":"stderr","text":"\r1.1%"},"2870":{"name":"stderr","text":"\r11.1%"},"2871":{"name":"stderr","text":"\r11.1%"},"2872":{"name":"stderr","text":"\r11.1%"},"2873":{"name":"stderr","text":"\r11.1%"},"2874":{"name":"stderr","text":"\r11.1%"},"2875":{"name":"stderr","text":"\r11.1%"},"2876":{"name":"stderr","text":"\r11.1%"},"2877":{"name":"stderr","text":"\r11.1%"},"2878":{"name":"stderr","text":"\r11.1%"},"2879":{"name":"stderr","text":"\r11.1%"},"288":{"name":"stderr","text":"\r1.1%"},"2880":{"name":"stderr","text":"\r11.1%"},"2881":{"name":"stderr","text":"\r11.2%"},"2882":{"name":"stderr","text":"\r11.2%"},"2883":{"name":"stderr","text":"\r11.2%"},"2884":{"name":"stderr","text":"\r11.2%"},"2885":{"name":"stderr","text":"\r11.2%"},"2886":{"name":"stderr","text":"\r11.2%"},"2887":{"name":"stderr","text":"\r11.2%"},"2888":{"name":"stderr","text":"\r11.2%"},"2889":{"name":"stderr","text":"\r11.2%"},"289":{"name":"stderr","text":"\r1.1%"},"2890":{"name":"stderr","text":"\r11.2%"},"2891":{"name":"stderr","text":"\r11.2%"},"2892":{"name":"stderr","text":"\r11.2%"},"2893":{"name":"stderr","text":"\r11.2%"},"2894":{"name":"stderr","text":"\r11.2%"},"2895":{"name":"stderr","text":"\r11.2%"},"2896":{"name":"stderr","text":"\r11.2%"},"2897":{"name":"stderr","text":"\r11.2%"},"2898":{"name":"stderr","text":"\r11.2%"},"2899":{"name":"stderr","text":"\r11.2%"},"29":{"name":"stderr","text":"\r0.1%"},"290":{"name":"stderr","text":"\r1.1%"},"2900":{"name":"stderr","text":"\r11.2%"},"2901":{"name":"stderr","text":"\r11.2%"},"2902":{"name":"stderr","text":"\r11.2%"},"2903":{"name":"stderr","text":"\r11.2%"},"2904":{"name":"stderr","text":"\r11.2%"},"2905":{"name":"stderr","text":"\r11.2%"},"2906":{"name":"stderr","text":"\r11.2%"},"2907":{"name":"stderr","text":"\r11.3%"},"2908":{"name":"stderr","text":"\r11.3%"},"2909":{"name":"stderr","text":"\r11.3%"},"291":{"name":"stderr","text":"\r1.1%"},"2910":{"name":"stderr","text":"\r11.3%"},"2911":{"name":"stderr","text":"\r11.3%"},"2912":{"name":"stderr","text":"\r11.3%"},"2913":{"name":"stderr","text":"\r11.3%"},"2914":{"name":"stderr","text":"\r11.3%"},"2915":{"name":"stderr","text":"\r11.3%"},"2916":{"name":"stderr","text":"\r11.3%"},"2917":{"name":"stderr","text":"\r11.3%"},"2918":{"name":"stderr","text":"\r11.3%"},"2919":{"name":"stderr","text":"\r11.3%"},"292":{"name":"stderr","text":"\r1.1%"},"2920":{"name":"stderr","text":"\r11.3%"},"2921":{"name":"stderr","text":"\r11.3%"},"2922":{"name":"stderr","text":"\r11.3%"},"2923":{"name":"stderr","text":"\r11.3%"},"2924":{"name":"stderr","text":"\r11.3%"},"2925":{"name":"stderr","text":"\r11.3%"},"2926":{"name":"stderr","text":"\r11.3%"},"2927":{"name":"stderr","text":"\r11.3%"},"2928":{"name":"stderr","text":"\r11.3%"},"2929":{"name":"stderr","text":"\r11.3%"},"293":{"name":"stderr","text":"\r1.1%"},"2930":{"name":"stderr","text":"\r11.3%"},"2931":{"name":"stderr","text":"\r11.3%"},"2932":{"name":"stderr","text":"\r11.3%"},"2933":{"name":"stderr","text":"\r11.4%"},"2934":{"name":"stderr","text":"\r11.4%"},"2935":{"name":"stderr","text":"\r11.4%"},"2936":{"name":"stderr","text":"\r11.4%"},"2937":{"name":"stderr","text":"\r11.4%"},"2938":{"name":"stderr","text":"\r11.4%"},"2939":{"name":"stderr","text":"\r11.4%"},"294":{"name":"stderr","text":"\r1.1%"},"2940":{"name":"stderr","text":"\r11.4%"},"2941":{"name":"stderr","text":"\r11.4%"},"2942":{"name":"stderr","text":"\r11.4%"},"2943":{"name":"stderr","text":"\r11.4%"},"2944":{"name":"stderr","text":"\r11.4%"},"2945":{"name":"stderr","text":"\r11.4%"},"2946":{"name":"stderr","text":"\r11.4%"},"2947":{"name":"stderr","text":"\r11.4%"},"2948":{"name":"stderr","text":"\r11.4%"},"2949":{"name":"stderr","text":"\r11.4%"},"295":{"name":"stderr","text":"\r1.1%"},"2950":{"name":"stderr","text":"\r11.4%"},"2951":{"name":"stderr","text":"\r11.4%"},"2952":{"name":"stderr","text":"\r11.4%"},"2953":{"name":"stderr","text":"\r11.4%"},"2954":{"name":"stderr","text":"\r11.4%"},"2955":{"name":"stderr","text":"\r11.4%"},"2956":{"more_output":true},"296":{"name":"stderr","text":"\r1.1%"},"297":{"name":"stderr","text":"\r1.1%"},"298":{"name":"stderr","text":"\r1.1%"},"299":{"name":"stderr","text":"\r1.1%"},"3":{"name":"stdout","text":"Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n"},"30":{"name":"stderr","text":"\r0.1%"},"300":{"name":"stderr","text":"\r1.1%"},"301":{"name":"stderr","text":"\r1.2%"},"302":{"name":"stderr","text":"\r1.2%"},"303":{"name":"stderr","text":"\r1.2%"},"304":{"name":"stderr","text":"\r1.2%"},"305":{"name":"stderr","text":"\r1.2%"},"306":{"name":"stderr","text":"\r1.2%"},"307":{"name":"stderr","text":"\r1.2%"},"308":{"name":"stderr","text":"\r1.2%"},"309":{"name":"stderr","text":"\r1.2%"},"31":{"name":"stderr","text":"\r0.1%"},"310":{"name":"stderr","text":"\r1.2%"},"311":{"name":"stderr","text":"\r1.2%"},"312":{"name":"stderr","text":"\r1.2%"},"313":{"name":"stderr","text":"\r1.2%"},"314":{"name":"stderr","text":"\r1.2%"},"315":{"name":"stderr","text":"\r1.2%"},"316":{"name":"stderr","text":"\r1.2%"},"317":{"name":"stderr","text":"\r1.2%"},"318":{"name":"stderr","text":"\r1.2%"},"319":{"name":"stderr","text":"\r1.2%"},"32":{"name":"stderr","text":"\r0.1%"},"320":{"name":"stderr","text":"\r1.2%"},"321":{"name":"stderr","text":"\r1.2%"},"322":{"name":"stderr","text":"\r1.2%"},"323":{"name":"stderr","text":"\r1.2%"},"324":{"name":"stderr","text":"\r1.2%"},"325":{"name":"stderr","text":"\r1.2%"},"326":{"name":"stderr","text":"\r1.2%"},"327":{"name":"stderr","text":"\r1.3%"},"328":{"name":"stderr","text":"\r1.3%"},"329":{"name":"stderr","text":"\r1.3%"},"33":{"name":"stderr","text":"\r0.1%"},"330":{"name":"stderr","text":"\r1.3%"},"331":{"name":"stderr","text":"\r1.3%"},"332":{"name":"stderr","text":"\r1.3%"},"333":{"name":"stderr","text":"\r1.3%"},"334":{"name":"stderr","text":"\r1.3%"},"335":{"name":"stderr","text":"\r1.3%"},"336":{"name":"stderr","text":"\r1.3%"},"337":{"name":"stderr","text":"\r1.3%"},"338":{"name":"stderr","text":"\r1.3%"},"339":{"name":"stderr","text":"\r1.3%"},"34":{"name":"stderr","text":"\r0.1%"},"340":{"name":"stderr","text":"\r1.3%"},"341":{"name":"stderr","text":"\r1.3%"},"342":{"name":"stderr","text":"\r1.3%"},"343":{"name":"stderr","text":"\r1.3%"},"344":{"name":"stderr","text":"\r1.3%"},"345":{"name":"stderr","text":"\r1.3%"},"346":{"name":"stderr","text":"\r1.3%"},"347":{"name":"stderr","text":"\r1.3%"},"348":{"name":"stderr","text":"\r1.3%"},"349":{"name":"stderr","text":"\r1.3%"},"35":{"name":"stderr","text":"\r0.1%"},"350":{"name":"stderr","text":"\r1.3%"},"351":{"name":"stderr","text":"\r1.3%"},"352":{"name":"stderr","text":"\r1.3%"},"353":{"name":"stderr","text":"\r1.4%"},"354":{"name":"stderr","text":"\r1.4%"},"355":{"name":"stderr","text":"\r1.4%"},"356":{"name":"stderr","text":"\r1.4%"},"357":{"name":"stderr","text":"\r1.4%"},"358":{"name":"stderr","text":"\r1.4%"},"359":{"name":"stderr","text":"\r1.4%"},"36":{"name":"stderr","text":"\r0.1%"},"360":{"name":"stderr","text":"\r1.4%"},"361":{"name":"stderr","text":"\r1.4%"},"362":{"name":"stderr","text":"\r1.4%"},"363":{"name":"stderr","text":"\r1.4%"},"364":{"name":"stderr","text":"\r1.4%"},"365":{"name":"stderr","text":"\r1.4%"},"366":{"name":"stderr","text":"\r1.4%"},"367":{"name":"stderr","text":"\r1.4%"},"368":{"name":"stderr","text":"\r1.4%"},"369":{"name":"stderr","text":"\r1.4%"},"37":{"name":"stderr","text":"\r0.1%"},"370":{"name":"stderr","text":"\r1.4%"},"371":{"name":"stderr","text":"\r1.4%"},"372":{"name":"stderr","text":"\r1.4%"},"373":{"name":"stderr","text":"\r1.4%"},"374":{"name":"stderr","text":"\r1.4%"},"375":{"name":"stderr","text":"\r1.4%"},"376":{"name":"stderr","text":"\r1.4%"},"377":{"name":"stderr","text":"\r1.4%"},"378":{"name":"stderr","text":"\r1.4%"},"379":{"name":"stderr","text":"\r1.5%"},"38":{"name":"stderr","text":"\r0.1%"},"380":{"name":"stderr","text":"\r1.5%"},"381":{"name":"stderr","text":"\r1.5%"},"382":{"name":"stderr","text":"\r1.5%"},"383":{"name":"stderr","text":"\r1.5%"},"384":{"name":"stderr","text":"\r1.5%"},"385":{"name":"stderr","text":"\r1.5%"},"386":{"name":"stderr","text":"\r1.5%"},"387":{"name":"stderr","text":"\r1.5%"},"388":{"name":"stderr","text":"\r1.5%"},"389":{"name":"stderr","text":"\r1.5%"},"39":{"name":"stderr","text":"\r0.1%"},"390":{"name":"stderr","text":"\r1.5%"},"391":{"name":"stderr","text":"\r1.5%"},"392":{"name":"stderr","text":"\r1.5%"},"393":{"name":"stderr","text":"\r1.5%"},"394":{"name":"stderr","text":"\r1.5%"},"395":{"name":"stderr","text":"\r1.5%"},"396":{"name":"stderr","text":"\r1.5%"},"397":{"name":"stderr","text":"\r1.5%"},"398":{"name":"stderr","text":"\r1.5%"},"399":{"name":"stderr","text":"\r1.5%"},"4":{"name":"stdout","text":"Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"},"40":{"name":"stderr","text":"\r0.1%"},"400":{"name":"stderr","text":"\r1.5%"},"401":{"name":"stderr","text":"\r1.5%"},"402":{"name":"stderr","text":"\r1.5%"},"403":{"name":"stderr","text":"\r1.5%"},"404":{"name":"stderr","text":"\r1.6%"},"405":{"name":"stderr","text":"\r1.6%"},"406":{"name":"stderr","text":"\r1.6%"},"407":{"name":"stderr","text":"\r1.6%"},"408":{"name":"stderr","text":"\r1.6%"},"409":{"name":"stderr","text":"\r1.6%"},"41":{"name":"stderr","text":"\r0.1%"},"410":{"name":"stderr","text":"\r1.6%"},"411":{"name":"stderr","text":"\r1.6%"},"412":{"name":"stderr","text":"\r1.6%"},"413":{"name":"stderr","text":"\r1.6%"},"414":{"name":"stderr","text":"\r1.6%"},"415":{"name":"stderr","text":"\r1.6%"},"416":{"name":"stderr","text":"\r1.6%"},"417":{"name":"stderr","text":"\r1.6%"},"418":{"name":"stderr","text":"\r1.6%"},"419":{"name":"stderr","text":"\r1.6%"},"42":{"name":"stderr","text":"\r0.1%"},"420":{"name":"stderr","text":"\r1.6%"},"421":{"name":"stderr","text":"\r1.6%"},"422":{"name":"stderr","text":"\r1.6%"},"423":{"name":"stderr","text":"\r1.6%"},"424":{"name":"stderr","text":"\r1.6%"},"425":{"name":"stderr","text":"\r1.6%"},"426":{"name":"stderr","text":"\r1.6%"},"427":{"name":"stderr","text":"\r1.6%"},"428":{"name":"stderr","text":"\r1.6%"},"429":{"name":"stderr","text":"\r1.6%"},"43":{"name":"stderr","text":"\r0.2%"},"430":{"name":"stderr","text":"\r1.7%"},"431":{"name":"stderr","text":"\r1.7%"},"432":{"name":"stderr","text":"\r1.7%"},"433":{"name":"stderr","text":"\r1.7%"},"434":{"name":"stderr","text":"\r1.7%"},"435":{"name":"stderr","text":"\r1.7%"},"436":{"name":"stderr","text":"\r1.7%"},"437":{"name":"stderr","text":"\r1.7%"},"438":{"name":"stderr","text":"\r1.7%"},"439":{"name":"stderr","text":"\r1.7%"},"44":{"name":"stderr","text":"\r0.2%"},"440":{"name":"stderr","text":"\r1.7%"},"441":{"name":"stderr","text":"\r1.7%"},"442":{"name":"stderr","text":"\r1.7%"},"443":{"name":"stderr","text":"\r1.7%"},"444":{"name":"stderr","text":"\r1.7%"},"445":{"name":"stderr","text":"\r1.7%"},"446":{"name":"stderr","text":"\r1.7%"},"447":{"name":"stderr","text":"\r1.7%"},"448":{"name":"stderr","text":"\r1.7%"},"449":{"name":"stderr","text":"\r1.7%"},"45":{"name":"stderr","text":"\r0.2%"},"450":{"name":"stderr","text":"\r1.7%"},"451":{"name":"stderr","text":"\r1.7%"},"452":{"name":"stderr","text":"\r1.7%"},"453":{"name":"stderr","text":"\r1.7%"},"454":{"name":"stderr","text":"\r1.7%"},"455":{"name":"stderr","text":"\r1.7%"},"456":{"name":"stderr","text":"\r1.8%"},"457":{"name":"stderr","text":"\r1.8%"},"458":{"name":"stderr","text":"\r1.8%"},"459":{"name":"stderr","text":"\r1.8%"},"46":{"name":"stderr","text":"\r0.2%"},"460":{"name":"stderr","text":"\r1.8%"},"461":{"name":"stderr","text":"\r1.8%"},"462":{"name":"stderr","text":"\r1.8%"},"463":{"name":"stderr","text":"\r1.8%"},"464":{"name":"stderr","text":"\r1.8%"},"465":{"name":"stderr","text":"\r1.8%"},"466":{"name":"stderr","text":"\r1.8%"},"467":{"name":"stderr","text":"\r1.8%"},"468":{"name":"stderr","text":"\r1.8%"},"469":{"name":"stderr","text":"\r1.8%"},"47":{"name":"stderr","text":"\r0.2%"},"470":{"name":"stderr","text":"\r1.8%"},"471":{"name":"stderr","text":"\r1.8%"},"472":{"name":"stderr","text":"\r1.8%"},"473":{"name":"stderr","text":"\r1.8%"},"474":{"name":"stderr","text":"\r1.8%"},"475":{"name":"stderr","text":"\r1.8%"},"476":{"name":"stderr","text":"\r1.8%"},"477":{"name":"stderr","text":"\r1.8%"},"478":{"name":"stderr","text":"\r1.8%"},"479":{"name":"stderr","text":"\r1.8%"},"48":{"name":"stderr","text":"\r0.2%"},"480":{"name":"stderr","text":"\r1.8%"},"481":{"name":"stderr","text":"\r1.8%"},"482":{"name":"stderr","text":"\r1.9%"},"483":{"name":"stderr","text":"\r1.9%"},"484":{"name":"stderr","text":"\r1.9%"},"485":{"name":"stderr","text":"\r1.9%"},"486":{"name":"stderr","text":"\r1.9%"},"487":{"name":"stderr","text":"\r1.9%"},"488":{"name":"stderr","text":"\r1.9%"},"489":{"name":"stderr","text":"\r1.9%"},"49":{"name":"stderr","text":"\r0.2%"},"490":{"name":"stderr","text":"\r1.9%"},"491":{"name":"stderr","text":"\r1.9%"},"492":{"name":"stderr","text":"\r1.9%"},"493":{"name":"stderr","text":"\r1.9%"},"494":{"name":"stderr","text":"\r1.9%"},"495":{"name":"stderr","text":"\r1.9%"},"496":{"name":"stderr","text":"\r1.9%"},"497":{"name":"stderr","text":"\r1.9%"},"498":{"name":"stderr","text":"\r1.9%"},"499":{"name":"stderr","text":"\r1.9%"},"5":{"name":"stderr","text":"\r0.0%"},"50":{"name":"stderr","text":"\r0.2%"},"500":{"name":"stderr","text":"\r1.9%"},"501":{"name":"stderr","text":"\r1.9%"},"502":{"name":"stderr","text":"\r1.9%"},"503":{"name":"stderr","text":"\r1.9%"},"504":{"name":"stderr","text":"\r1.9%"},"505":{"name":"stderr","text":"\r1.9%"},"506":{"name":"stderr","text":"\r1.9%"},"507":{"name":"stderr","text":"\r1.9%"},"508":{"name":"stderr","text":"\r2.0%"},"509":{"name":"stderr","text":"\r2.0%"},"51":{"name":"stderr","text":"\r0.2%"},"510":{"name":"stderr","text":"\r2.0%"},"511":{"name":"stderr","text":"\r2.0%"},"512":{"name":"stderr","text":"\r2.0%"},"513":{"name":"stderr","text":"\r2.0%"},"514":{"name":"stderr","text":"\r2.0%"},"515":{"name":"stderr","text":"\r2.0%"},"516":{"name":"stderr","text":"\r2.0%"},"517":{"name":"stderr","text":"\r2.0%"},"518":{"name":"stderr","text":"\r2.0%"},"519":{"name":"stderr","text":"\r2.0%"},"52":{"name":"stderr","text":"\r0.2%"},"520":{"name":"stderr","text":"\r2.0%"},"521":{"name":"stderr","text":"\r2.0%"},"522":{"name":"stderr","text":"\r2.0%"},"523":{"name":"stderr","text":"\r2.0%"},"524":{"name":"stderr","text":"\r2.0%"},"525":{"name":"stderr","text":"\r2.0%"},"526":{"name":"stderr","text":"\r2.0%"},"527":{"name":"stderr","text":"\r2.0%"},"528":{"name":"stderr","text":"\r2.0%"},"529":{"name":"stderr","text":"\r2.0%"},"53":{"name":"stderr","text":"\r0.2%"},"530":{"name":"stderr","text":"\r2.0%"},"531":{"name":"stderr","text":"\r2.0%"},"532":{"name":"stderr","text":"\r2.0%"},"533":{"name":"stderr","text":"\r2.1%"},"534":{"name":"stderr","text":"\r2.1%"},"535":{"name":"stderr","text":"\r2.1%"},"536":{"name":"stderr","text":"\r2.1%"},"537":{"name":"stderr","text":"\r2.1%"},"538":{"name":"stderr","text":"\r2.1%"},"539":{"name":"stderr","text":"\r2.1%"},"54":{"name":"stderr","text":"\r0.2%"},"540":{"name":"stderr","text":"\r2.1%"},"541":{"name":"stderr","text":"\r2.1%"},"542":{"name":"stderr","text":"\r2.1%"},"543":{"name":"stderr","text":"\r2.1%"},"544":{"name":"stderr","text":"\r2.1%"},"545":{"name":"stderr","text":"\r2.1%"},"546":{"name":"stderr","text":"\r2.1%"},"547":{"name":"stderr","text":"\r2.1%"},"548":{"name":"stderr","text":"\r2.1%"},"549":{"name":"stderr","text":"\r2.1%"},"55":{"name":"stderr","text":"\r0.2%"},"550":{"name":"stderr","text":"\r2.1%"},"551":{"name":"stderr","text":"\r2.1%"},"552":{"name":"stderr","text":"\r2.1%"},"553":{"name":"stderr","text":"\r2.1%"},"554":{"name":"stderr","text":"\r2.1%"},"555":{"name":"stderr","text":"\r2.1%"},"556":{"name":"stderr","text":"\r2.1%"},"557":{"name":"stderr","text":"\r2.1%"},"558":{"name":"stderr","text":"\r2.1%"},"559":{"name":"stderr","text":"\r2.2%"},"56":{"name":"stderr","text":"\r0.2%"},"560":{"name":"stderr","text":"\r2.2%"},"561":{"name":"stderr","text":"\r2.2%"},"562":{"name":"stderr","text":"\r2.2%"},"563":{"name":"stderr","text":"\r2.2%"},"564":{"name":"stderr","text":"\r2.2%"},"565":{"name":"stderr","text":"\r2.2%"},"566":{"name":"stderr","text":"\r2.2%"},"567":{"name":"stderr","text":"\r2.2%"},"568":{"name":"stderr","text":"\r2.2%"},"569":{"name":"stderr","text":"\r2.2%"},"57":{"name":"stderr","text":"\r0.2%"},"570":{"name":"stderr","text":"\r2.2%"},"571":{"name":"stderr","text":"\r2.2%"},"572":{"name":"stderr","text":"\r2.2%"},"573":{"name":"stderr","text":"\r2.2%"},"574":{"name":"stderr","text":"\r2.2%"},"575":{"name":"stderr","text":"\r2.2%"},"576":{"name":"stderr","text":"\r2.2%"},"577":{"name":"stderr","text":"\r2.2%"},"578":{"name":"stderr","text":"\r2.2%"},"579":{"name":"stderr","text":"\r2.2%"},"58":{"name":"stderr","text":"\r0.2%"},"580":{"name":"stderr","text":"\r2.2%"},"581":{"name":"stderr","text":"\r2.2%"},"582":{"name":"stderr","text":"\r2.2%"},"583":{"name":"stderr","text":"\r2.2%"},"584":{"name":"stderr","text":"\r2.2%"},"585":{"name":"stderr","text":"\r2.3%"},"586":{"name":"stderr","text":"\r2.3%"},"587":{"name":"stderr","text":"\r2.3%"},"588":{"name":"stderr","text":"\r2.3%"},"589":{"name":"stderr","text":"\r2.3%"},"59":{"name":"stderr","text":"\r0.2%"},"590":{"name":"stderr","text":"\r2.3%"},"591":{"name":"stderr","text":"\r2.3%"},"592":{"name":"stderr","text":"\r2.3%"},"593":{"name":"stderr","text":"\r2.3%"},"594":{"name":"stderr","text":"\r2.3%"},"595":{"name":"stderr","text":"\r2.3%"},"596":{"name":"stderr","text":"\r2.3%"},"597":{"name":"stderr","text":"\r2.3%"},"598":{"name":"stderr","text":"\r2.3%"},"599":{"name":"stderr","text":"\r2.3%"},"6":{"name":"stderr","text":"\r0.0%"},"60":{"name":"stderr","text":"\r0.2%"},"600":{"name":"stderr","text":"\r2.3%"},"601":{"name":"stderr","text":"\r2.3%"},"602":{"name":"stderr","text":"\r2.3%"},"603":{"name":"stderr","text":"\r2.3%"},"604":{"name":"stderr","text":"\r2.3%"},"605":{"name":"stderr","text":"\r2.3%"},"606":{"name":"stderr","text":"\r2.3%"},"607":{"name":"stderr","text":"\r2.3%"},"608":{"name":"stderr","text":"\r2.3%"},"609":{"name":"stderr","text":"\r2.3%"},"61":{"name":"stderr","text":"\r0.2%"},"610":{"name":"stderr","text":"\r2.3%"},"611":{"name":"stderr","text":"\r2.4%"},"612":{"name":"stderr","text":"\r2.4%"},"613":{"name":"stderr","text":"\r2.4%"},"614":{"name":"stderr","text":"\r2.4%"},"615":{"name":"stderr","text":"\r2.4%"},"616":{"name":"stderr","text":"\r2.4%"},"617":{"name":"stderr","text":"\r2.4%"},"618":{"name":"stderr","text":"\r2.4%"},"619":{"name":"stderr","text":"\r2.4%"},"62":{"name":"stderr","text":"\r0.2%"},"620":{"name":"stderr","text":"\r2.4%"},"621":{"name":"stderr","text":"\r2.4%"},"622":{"name":"stderr","text":"\r2.4%"},"623":{"name":"stderr","text":"\r2.4%"},"624":{"name":"stderr","text":"\r2.4%"},"625":{"name":"stderr","text":"\r2.4%"},"626":{"name":"stderr","text":"\r2.4%"},"627":{"name":"stderr","text":"\r2.4%"},"628":{"name":"stderr","text":"\r2.4%"},"629":{"name":"stderr","text":"\r2.4%"},"63":{"name":"stderr","text":"\r0.2%"},"630":{"name":"stderr","text":"\r2.4%"},"631":{"name":"stderr","text":"\r2.4%"},"632":{"name":"stderr","text":"\r2.4%"},"633":{"name":"stderr","text":"\r2.4%"},"634":{"name":"stderr","text":"\r2.4%"},"635":{"name":"stderr","text":"\r2.4%"},"636":{"name":"stderr","text":"\r2.4%"},"637":{"name":"stderr","text":"\r2.5%"},"638":{"name":"stderr","text":"\r2.5%"},"639":{"name":"stderr","text":"\r2.5%"},"64":{"name":"stderr","text":"\r0.2%"},"640":{"name":"stderr","text":"\r2.5%"},"641":{"name":"stderr","text":"\r2.5%"},"642":{"name":"stderr","text":"\r2.5%"},"643":{"name":"stderr","text":"\r2.5%"},"644":{"name":"stderr","text":"\r2.5%"},"645":{"name":"stderr","text":"\r2.5%"},"646":{"name":"stderr","text":"\r2.5%"},"647":{"name":"stderr","text":"\r2.5%"},"648":{"name":"stderr","text":"\r2.5%"},"649":{"name":"stderr","text":"\r2.5%"},"65":{"name":"stderr","text":"\r0.2%"},"650":{"name":"stderr","text":"\r2.5%"},"651":{"name":"stderr","text":"\r2.5%"},"652":{"name":"stderr","text":"\r2.5%"},"653":{"name":"stderr","text":"\r2.5%"},"654":{"name":"stderr","text":"\r2.5%"},"655":{"name":"stderr","text":"\r2.5%"},"656":{"name":"stderr","text":"\r2.5%"},"657":{"name":"stderr","text":"\r2.5%"},"658":{"name":"stderr","text":"\r2.5%"},"659":{"name":"stderr","text":"\r2.5%"},"66":{"name":"stderr","text":"\r0.2%"},"660":{"name":"stderr","text":"\r2.5%"},"661":{"name":"stderr","text":"\r2.5%"},"662":{"name":"stderr","text":"\r2.6%"},"663":{"name":"stderr","text":"\r2.6%"},"664":{"name":"stderr","text":"\r2.6%"},"665":{"name":"stderr","text":"\r2.6%"},"666":{"name":"stderr","text":"\r2.6%"},"667":{"name":"stderr","text":"\r2.6%"},"668":{"name":"stderr","text":"\r2.6%"},"669":{"name":"stderr","text":"\r2.6%"},"67":{"name":"stderr","text":"\r0.2%"},"670":{"name":"stderr","text":"\r2.6%"},"671":{"name":"stderr","text":"\r2.6%"},"672":{"name":"stderr","text":"\r2.6%"},"673":{"name":"stderr","text":"\r2.6%"},"674":{"name":"stderr","text":"\r2.6%"},"675":{"name":"stderr","text":"\r2.6%"},"676":{"name":"stderr","text":"\r2.6%"},"677":{"name":"stderr","text":"\r2.6%"},"678":{"name":"stderr","text":"\r2.6%"},"679":{"name":"stderr","text":"\r2.6%"},"68":{"name":"stderr","text":"\r0.2%"},"680":{"name":"stderr","text":"\r2.6%"},"681":{"name":"stderr","text":"\r2.6%"},"682":{"name":"stderr","text":"\r2.6%"},"683":{"name":"stderr","text":"\r2.6%"},"684":{"name":"stderr","text":"\r2.6%"},"685":{"name":"stderr","text":"\r2.6%"},"686":{"name":"stderr","text":"\r2.6%"},"687":{"name":"stderr","text":"\r2.6%"},"688":{"name":"stderr","text":"\r2.7%"},"689":{"name":"stderr","text":"\r2.7%"},"69":{"name":"stderr","text":"\r0.3%"},"690":{"name":"stderr","text":"\r2.7%"},"691":{"name":"stderr","text":"\r2.7%"},"692":{"name":"stderr","text":"\r2.7%"},"693":{"name":"stderr","text":"\r2.7%"},"694":{"name":"stderr","text":"\r2.7%"},"695":{"name":"stderr","text":"\r2.7%"},"696":{"name":"stderr","text":"\r2.7%"},"697":{"name":"stderr","text":"\r2.7%"},"698":{"name":"stderr","text":"\r2.7%"},"699":{"name":"stderr","text":"\r2.7%"},"7":{"name":"stderr","text":"\r0.0%"},"70":{"name":"stderr","text":"\r0.3%"},"700":{"name":"stderr","text":"\r2.7%"},"701":{"name":"stderr","text":"\r2.7%"},"702":{"name":"stderr","text":"\r2.7%"},"703":{"name":"stderr","text":"\r2.7%"},"704":{"name":"stderr","text":"\r2.7%"},"705":{"name":"stderr","text":"\r2.7%"},"706":{"name":"stderr","text":"\r2.7%"},"707":{"name":"stderr","text":"\r2.7%"},"708":{"name":"stderr","text":"\r2.7%"},"709":{"name":"stderr","text":"\r2.7%"},"71":{"name":"stderr","text":"\r0.3%"},"710":{"name":"stderr","text":"\r2.7%"},"711":{"name":"stderr","text":"\r2.7%"},"712":{"name":"stderr","text":"\r2.7%"},"713":{"name":"stderr","text":"\r2.7%"},"714":{"name":"stderr","text":"\r2.8%"},"715":{"name":"stderr","text":"\r2.8%"},"716":{"name":"stderr","text":"\r2.8%"},"717":{"name":"stderr","text":"\r2.8%"},"718":{"name":"stderr","text":"\r2.8%"},"719":{"name":"stderr","text":"\r2.8%"},"72":{"name":"stderr","text":"\r0.3%"},"720":{"name":"stderr","text":"\r2.8%"},"721":{"name":"stderr","text":"\r2.8%"},"722":{"name":"stderr","text":"\r2.8%"},"723":{"name":"stderr","text":"\r2.8%"},"724":{"name":"stderr","text":"\r2.8%"},"725":{"name":"stderr","text":"\r2.8%"},"726":{"name":"stderr","text":"\r2.8%"},"727":{"name":"stderr","text":"\r2.8%"},"728":{"name":"stderr","text":"\r2.8%"},"729":{"name":"stderr","text":"\r2.8%"},"73":{"name":"stderr","text":"\r0.3%"},"730":{"name":"stderr","text":"\r2.8%"},"731":{"name":"stderr","text":"\r2.8%"},"732":{"name":"stderr","text":"\r2.8%"},"733":{"name":"stderr","text":"\r2.8%"},"734":{"name":"stderr","text":"\r2.8%"},"735":{"name":"stderr","text":"\r2.8%"},"736":{"name":"stderr","text":"\r2.8%"},"737":{"name":"stderr","text":"\r2.8%"},"738":{"name":"stderr","text":"\r2.8%"},"739":{"name":"stderr","text":"\r2.8%"},"74":{"name":"stderr","text":"\r0.3%"},"740":{"name":"stderr","text":"\r2.9%"},"741":{"name":"stderr","text":"\r2.9%"},"742":{"name":"stderr","text":"\r2.9%"},"743":{"name":"stderr","text":"\r2.9%"},"744":{"name":"stderr","text":"\r2.9%"},"745":{"name":"stderr","text":"\r2.9%"},"746":{"name":"stderr","text":"\r2.9%"},"747":{"name":"stderr","text":"\r2.9%"},"748":{"name":"stderr","text":"\r2.9%"},"749":{"name":"stderr","text":"\r2.9%"},"75":{"name":"stderr","text":"\r0.3%"},"750":{"name":"stderr","text":"\r2.9%"},"751":{"name":"stderr","text":"\r2.9%"},"752":{"name":"stderr","text":"\r2.9%"},"753":{"name":"stderr","text":"\r2.9%"},"754":{"name":"stderr","text":"\r2.9%"},"755":{"name":"stderr","text":"\r2.9%"},"756":{"name":"stderr","text":"\r2.9%"},"757":{"name":"stderr","text":"\r2.9%"},"758":{"name":"stderr","text":"\r2.9%"},"759":{"name":"stderr","text":"\r2.9%"},"76":{"name":"stderr","text":"\r0.3%"},"760":{"name":"stderr","text":"\r2.9%"},"761":{"name":"stderr","text":"\r2.9%"},"762":{"name":"stderr","text":"\r2.9%"},"763":{"name":"stderr","text":"\r2.9%"},"764":{"name":"stderr","text":"\r2.9%"},"765":{"name":"stderr","text":"\r2.9%"},"766":{"name":"stderr","text":"\r3.0%"},"767":{"name":"stderr","text":"\r3.0%"},"768":{"name":"stderr","text":"\r3.0%"},"769":{"name":"stderr","text":"\r3.0%"},"77":{"name":"stderr","text":"\r0.3%"},"770":{"name":"stderr","text":"\r3.0%"},"771":{"name":"stderr","text":"\r3.0%"},"772":{"name":"stderr","text":"\r3.0%"},"773":{"name":"stderr","text":"\r3.0%"},"774":{"name":"stderr","text":"\r3.0%"},"775":{"name":"stderr","text":"\r3.0%"},"776":{"name":"stderr","text":"\r3.0%"},"777":{"name":"stderr","text":"\r3.0%"},"778":{"name":"stderr","text":"\r3.0%"},"779":{"name":"stderr","text":"\r3.0%"},"78":{"name":"stderr","text":"\r0.3%"},"780":{"name":"stderr","text":"\r3.0%"},"781":{"name":"stderr","text":"\r3.0%"},"782":{"name":"stderr","text":"\r3.0%"},"783":{"name":"stderr","text":"\r3.0%"},"784":{"name":"stderr","text":"\r3.0%"},"785":{"name":"stderr","text":"\r3.0%"},"786":{"name":"stderr","text":"\r3.0%"},"787":{"name":"stderr","text":"\r3.0%"},"788":{"name":"stderr","text":"\r3.0%"},"789":{"name":"stderr","text":"\r3.0%"},"79":{"name":"stderr","text":"\r0.3%"},"790":{"name":"stderr","text":"\r3.0%"},"791":{"name":"stderr","text":"\r3.1%"},"792":{"name":"stderr","text":"\r3.1%"},"793":{"name":"stderr","text":"\r3.1%"},"794":{"name":"stderr","text":"\r3.1%"},"795":{"name":"stderr","text":"\r3.1%"},"796":{"name":"stderr","text":"\r3.1%"},"797":{"name":"stderr","text":"\r3.1%"},"798":{"name":"stderr","text":"\r3.1%"},"799":{"name":"stderr","text":"\r3.1%"},"8":{"name":"stderr","text":"\r0.0%"},"80":{"name":"stderr","text":"\r0.3%"},"800":{"name":"stderr","text":"\r3.1%"},"801":{"name":"stderr","text":"\r3.1%"},"802":{"name":"stderr","text":"\r3.1%"},"803":{"name":"stderr","text":"\r3.1%"},"804":{"name":"stderr","text":"\r3.1%"},"805":{"name":"stderr","text":"\r3.1%"},"806":{"name":"stderr","text":"\r3.1%"},"807":{"name":"stderr","text":"\r3.1%"},"808":{"name":"stderr","text":"\r3.1%"},"809":{"name":"stderr","text":"\r3.1%"},"81":{"name":"stderr","text":"\r0.3%"},"810":{"name":"stderr","text":"\r3.1%"},"811":{"name":"stderr","text":"\r3.1%"},"812":{"name":"stderr","text":"\r3.1%"},"813":{"name":"stderr","text":"\r3.1%"},"814":{"name":"stderr","text":"\r3.1%"},"815":{"name":"stderr","text":"\r3.1%"},"816":{"name":"stderr","text":"\r3.1%"},"817":{"name":"stderr","text":"\r3.2%"},"818":{"name":"stderr","text":"\r3.2%"},"819":{"name":"stderr","text":"\r3.2%"},"82":{"name":"stderr","text":"\r0.3%"},"820":{"name":"stderr","text":"\r3.2%"},"821":{"name":"stderr","text":"\r3.2%"},"822":{"name":"stderr","text":"\r3.2%"},"823":{"name":"stderr","text":"\r3.2%"},"824":{"name":"stderr","text":"\r3.2%"},"825":{"name":"stderr","text":"\r3.2%"},"826":{"name":"stderr","text":"\r3.2%"},"827":{"name":"stderr","text":"\r3.2%"},"828":{"name":"stderr","text":"\r3.2%"},"829":{"name":"stderr","text":"\r3.2%"},"83":{"name":"stderr","text":"\r0.3%"},"830":{"name":"stderr","text":"\r3.2%"},"831":{"name":"stderr","text":"\r3.2%"},"832":{"name":"stderr","text":"\r3.2%"},"833":{"name":"stderr","text":"\r3.2%"},"834":{"name":"stderr","text":"\r3.2%"},"835":{"name":"stderr","text":"\r3.2%"},"836":{"name":"stderr","text":"\r3.2%"},"837":{"name":"stderr","text":"\r3.2%"},"838":{"name":"stderr","text":"\r3.2%"},"839":{"name":"stderr","text":"\r3.2%"},"84":{"name":"stderr","text":"\r0.3%"},"840":{"name":"stderr","text":"\r3.2%"},"841":{"name":"stderr","text":"\r3.2%"},"842":{"name":"stderr","text":"\r3.2%"},"843":{"name":"stderr","text":"\r3.3%"},"844":{"name":"stderr","text":"\r3.3%"},"845":{"name":"stderr","text":"\r3.3%"},"846":{"name":"stderr","text":"\r3.3%"},"847":{"name":"stderr","text":"\r3.3%"},"848":{"name":"stderr","text":"\r3.3%"},"849":{"name":"stderr","text":"\r3.3%"},"85":{"name":"stderr","text":"\r0.3%"},"850":{"name":"stderr","text":"\r3.3%"},"851":{"name":"stderr","text":"\r3.3%"},"852":{"name":"stderr","text":"\r3.3%"},"853":{"name":"stderr","text":"\r3.3%"},"854":{"name":"stderr","text":"\r3.3%"},"855":{"name":"stderr","text":"\r3.3%"},"856":{"name":"stderr","text":"\r3.3%"},"857":{"name":"stderr","text":"\r3.3%"},"858":{"name":"stderr","text":"\r3.3%"},"859":{"name":"stderr","text":"\r3.3%"},"86":{"name":"stderr","text":"\r0.3%"},"860":{"name":"stderr","text":"\r3.3%"},"861":{"name":"stderr","text":"\r3.3%"},"862":{"name":"stderr","text":"\r3.3%"},"863":{"name":"stderr","text":"\r3.3%"},"864":{"name":"stderr","text":"\r3.3%"},"865":{"name":"stderr","text":"\r3.3%"},"866":{"name":"stderr","text":"\r3.3%"},"867":{"name":"stderr","text":"\r3.3%"},"868":{"name":"stderr","text":"\r3.3%"},"869":{"name":"stderr","text":"\r3.4%"},"87":{"name":"stderr","text":"\r0.3%"},"870":{"name":"stderr","text":"\r3.4%"},"871":{"name":"stderr","text":"\r3.4%"},"872":{"name":"stderr","text":"\r3.4%"},"873":{"name":"stderr","text":"\r3.4%"},"874":{"name":"stderr","text":"\r3.4%"},"875":{"name":"stderr","text":"\r3.4%"},"876":{"name":"stderr","text":"\r3.4%"},"877":{"name":"stderr","text":"\r3.4%"},"878":{"name":"stderr","text":"\r3.4%"},"879":{"name":"stderr","text":"\r3.4%"},"88":{"name":"stderr","text":"\r0.3%"},"880":{"name":"stderr","text":"\r3.4%"},"881":{"name":"stderr","text":"\r3.4%"},"882":{"name":"stderr","text":"\r3.4%"},"883":{"name":"stderr","text":"\r3.4%"},"884":{"name":"stderr","text":"\r3.4%"},"885":{"name":"stderr","text":"\r3.4%"},"886":{"name":"stderr","text":"\r3.4%"},"887":{"name":"stderr","text":"\r3.4%"},"888":{"name":"stderr","text":"\r3.4%"},"889":{"name":"stderr","text":"\r3.4%"},"89":{"name":"stderr","text":"\r0.3%"},"890":{"name":"stderr","text":"\r3.4%"},"891":{"name":"stderr","text":"\r3.4%"},"892":{"name":"stderr","text":"\r3.4%"},"893":{"name":"stderr","text":"\r3.4%"},"894":{"name":"stderr","text":"\r3.4%"},"895":{"name":"stderr","text":"\r3.5%"},"896":{"name":"stderr","text":"\r3.5%"},"897":{"name":"stderr","text":"\r3.5%"},"898":{"name":"stderr","text":"\r3.5%"},"899":{"name":"stderr","text":"\r3.5%"},"9":{"name":"stderr","text":"\r0.0%"},"90":{"name":"stderr","text":"\r0.3%"},"900":{"name":"stderr","text":"\r3.5%"},"901":{"name":"stderr","text":"\r3.5%"},"902":{"name":"stderr","text":"\r3.5%"},"903":{"name":"stderr","text":"\r3.5%"},"904":{"name":"stderr","text":"\r3.5%"},"905":{"name":"stderr","text":"\r3.5%"},"906":{"name":"stderr","text":"\r3.5%"},"907":{"name":"stderr","text":"\r3.5%"},"908":{"name":"stderr","text":"\r3.5%"},"909":{"name":"stderr","text":"\r3.5%"},"91":{"name":"stderr","text":"\r0.3%"},"910":{"name":"stderr","text":"\r3.5%"},"911":{"name":"stderr","text":"\r3.5%"},"912":{"name":"stderr","text":"\r3.5%"},"913":{"name":"stderr","text":"\r3.5%"},"914":{"name":"stderr","text":"\r3.5%"},"915":{"name":"stderr","text":"\r3.5%"},"916":{"name":"stderr","text":"\r3.5%"},"917":{"name":"stderr","text":"\r3.5%"},"918":{"name":"stderr","text":"\r3.5%"},"919":{"name":"stderr","text":"\r3.5%"},"92":{"name":"stderr","text":"\r0.3%"},"920":{"name":"stderr","text":"\r3.6%"},"921":{"name":"stderr","text":"\r3.6%"},"922":{"name":"stderr","text":"\r3.6%"},"923":{"name":"stderr","text":"\r3.6%"},"924":{"name":"stderr","text":"\r3.6%"},"925":{"name":"stderr","text":"\r3.6%"},"926":{"name":"stderr","text":"\r3.6%"},"927":{"name":"stderr","text":"\r3.6%"},"928":{"name":"stderr","text":"\r3.6%"},"929":{"name":"stderr","text":"\r3.6%"},"93":{"name":"stderr","text":"\r0.3%"},"930":{"name":"stderr","text":"\r3.6%"},"931":{"name":"stderr","text":"\r3.6%"},"932":{"name":"stderr","text":"\r3.6%"},"933":{"name":"stderr","text":"\r3.6%"},"934":{"name":"stderr","text":"\r3.6%"},"935":{"name":"stderr","text":"\r3.6%"},"936":{"name":"stderr","text":"\r3.6%"},"937":{"name":"stderr","text":"\r3.6%"},"938":{"name":"stderr","text":"\r3.6%"},"939":{"name":"stderr","text":"\r3.6%"},"94":{"name":"stderr","text":"\r0.3%"},"940":{"name":"stderr","text":"\r3.6%"},"941":{"name":"stderr","text":"\r3.6%"},"942":{"name":"stderr","text":"\r3.6%"},"943":{"name":"stderr","text":"\r3.6%"},"944":{"name":"stderr","text":"\r3.6%"},"945":{"name":"stderr","text":"\r3.6%"},"946":{"name":"stderr","text":"\r3.7%"},"947":{"name":"stderr","text":"\r3.7%"},"948":{"name":"stderr","text":"\r3.7%"},"949":{"name":"stderr","text":"\r3.7%"},"95":{"name":"stderr","text":"\r0.4%"},"950":{"name":"stderr","text":"\r3.7%"},"951":{"name":"stderr","text":"\r3.7%"},"952":{"name":"stderr","text":"\r3.7%"},"953":{"name":"stderr","text":"\r3.7%"},"954":{"name":"stderr","text":"\r3.7%"},"955":{"name":"stderr","text":"\r3.7%"},"956":{"name":"stderr","text":"\r3.7%"},"957":{"name":"stderr","text":"\r3.7%"},"958":{"name":"stderr","text":"\r3.7%"},"959":{"name":"stderr","text":"\r3.7%"},"96":{"name":"stderr","text":"\r0.4%"},"960":{"name":"stderr","text":"\r3.7%"},"961":{"name":"stderr","text":"\r3.7%"},"962":{"name":"stderr","text":"\r3.7%"},"963":{"name":"stderr","text":"\r3.7%"},"964":{"name":"stderr","text":"\r3.7%"},"965":{"name":"stderr","text":"\r3.7%"},"966":{"name":"stderr","text":"\r3.7%"},"967":{"name":"stderr","text":"\r3.7%"},"968":{"name":"stderr","text":"\r3.7%"},"969":{"name":"stderr","text":"\r3.7%"},"97":{"name":"stderr","text":"\r0.4%"},"970":{"name":"stderr","text":"\r3.7%"},"971":{"name":"stderr","text":"\r3.7%"},"972":{"name":"stderr","text":"\r3.8%"},"973":{"name":"stderr","text":"\r3.8%"},"974":{"name":"stderr","text":"\r3.8%"},"975":{"name":"stderr","text":"\r3.8%"},"976":{"name":"stderr","text":"\r3.8%"},"977":{"name":"stderr","text":"\r3.8%"},"978":{"name":"stderr","text":"\r3.8%"},"979":{"name":"stderr","text":"\r3.8%"},"98":{"name":"stderr","text":"\r0.4%"},"980":{"name":"stderr","text":"\r3.8%"},"981":{"name":"stderr","text":"\r3.8%"},"982":{"name":"stderr","text":"\r3.8%"},"983":{"name":"stderr","text":"\r3.8%"},"984":{"name":"stderr","text":"\r3.8%"},"985":{"name":"stderr","text":"\r3.8%"},"986":{"name":"stderr","text":"\r3.8%"},"987":{"name":"stderr","text":"\r3.8%"},"988":{"name":"stderr","text":"\r3.8%"},"989":{"name":"stderr","text":"\r3.8%"},"99":{"name":"stderr","text":"\r0.4%"},"990":{"name":"stderr","text":"\r3.8%"},"991":{"name":"stderr","text":"\r3.8%"},"992":{"name":"stderr","text":"\r3.8%"},"993":{"name":"stderr","text":"\r3.8%"},"994":{"name":"stderr","text":"\r3.8%"},"995":{"name":"stderr","text":"\r3.8%"},"996":{"name":"stderr","text":"\r3.8%"},"997":{"name":"stderr","text":"\r3.8%"},"998":{"name":"stderr","text":"\r3.9%"},"999":{"name":"stderr","text":"\r3.9%"}},"pos":13,"start":1657048757594,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"0cb8dc","input":"matplotlib docs - https://matplotlib.org/stable/users/index","metadata":{"id":"4v-ygSOGM5Z2"},"pos":17,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"1dece6","input":"## Import Dependencies\nPyTorch, or torch, is the python deep learning library we use for our neural networks. \n\nTorchvision is an extension of this tool for computer vision specific functions such as transforming images and image datasets. \n\ntorch.nn is the neural network specific part of the torch library and torch.nn.functional has the activation functions we want to use (such as ReLU). ","metadata":{"id":"qAQI1JtBH6NX"},"pos":1,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"2b0a9f","input":"optim docs - https://pytorch.org/docs/stable/optim.html","metadata":{"id":"XBQj3ocPM5Z7"},"pos":27,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"4a262e","input":"<h1>Using our Model</h1>\n\nHere we simply pick a random image from our dataset and apply the model to it. ","metadata":{"id":"rXM79N49M5Z_"},"pos":36,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"4e80bb","input":"## Visualize our data","metadata":{"id":"PRFvX2ylDwaB"},"pos":16,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"544d84","input":"GPU boost training time. Why? Because it lets us do hundreds to thousands of calculations at the same time\n\n","metadata":{"id":"eOCymhEgDh09"},"pos":4,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"551d92","input":"## Configure cuda/gpu if available","metadata":{"id":"dYu9n0fTDfet"},"pos":3,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"5c1de3","input":"## Training our Model\n\nThe training process goes somewhat like this\ngo through the dataset [epoch] times<br>\n&ensp;  go through each image in the dataset<br>\n&ensp;&ensp; transfer inputs and labels to GPU<br>\n&ensp;&ensp; get prediction for input<br>\n&ensp;&ensp; check if prediction matches label, get loss<br>\n&ensp;&ensp; see which direction you have to change the weights<br>\n&ensp;&ensp; actually change weights using optimizer and learning rate\n&ensp;&ensp; Set the directions back to zero (optim.zero_grad())<br>\n&ensp;&ensp; add loss to total loss until reset<br>\n&ensp;&ensp; after some iterations, print out loss and reset","metadata":{"id":"S4nlQyOVNmkk"},"pos":30,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"66b7ae","input":"## Initializing the model, loss, and optimizer. \n\nFirst we initialize the model (our CNN). Next we transfer the CNN's weights over to the GPU. \n\nAfterward we initialize the cross entropy loss function, which is a loss function used for multi-class problems. Cross entropy heavily penalizes predictions that are far away from the true class distribution. ","metadata":{"id":"uzj67dly8t-s"},"pos":28,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"67bbe9","input":"##### H2 = (H1 - FH + 2P)/S + 1\n##### W2 = (W1 - FW + 2P)/S + 1","metadata":{"id":"-Teg5776K9ZF"},"pos":21,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"6ba59b","input":"# Applying the transforms\n\nWe are using the datasets.FashionMNIST function from torchvision to import the dataset. \n\n### Parameters to the function:\n\nThe root parameter sets the directory that we import the data to (and create it if it doesn't exist.)\n\nThe train parameter determines if we are importing training or testing fashion MNIST dataset. \n\nThe transform parameter determines the transforms we apply during preprocessing, which were defined above.\n\ndownload=True gives the function permission to download the data into the directory if it doesn't exist there. ","metadata":{"id":"4-lYAf_8Xtlu"},"pos":12,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"72276a","input":"CUDA is the API that we will use for GPU training. If CUDA is available we want to use it, and otherwise use the CPU. If you are using Google colab, it comes with a built in GPU for use so make sure to activate it by going to Runtime->Change runtime type->GPU","metadata":{"id":"fZOnBmNTM5Zt"},"pos":5,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"778410","input":"## Hyperparameters\n\ndatasets docs - https://pytorch.org/vision/stable/datasets.html\n\nThe input size is the number of pixels in each images, which is 28 by 28.\n\nThe number of classifcation (num_classes) is 10 because there are 10 possible classifications the model can make, such as T-shirt/top, trouser, bag, etc... \n\nThe number of epochs is how many times the training iterates over the dataset. For example, num_epochs of 8 means that the model will iterate over the dataset 10 times and each image is classified 10 times in training. More epochs means more training. If the loss is still going down after the last epoch, you should be training for more epochs. \n\nA batch size of 8 means 8 images at a time are passed into the model. A larger batch size means more images are passed at a time and there is faster training. Also, training is less variable, meaning it goes in a consistent image (think of it as using the average of the 8 images) while a batch size of 1 changes the weights for every image leading to more variable training. ","metadata":{"id":"gbhL7fcJM5Zz"},"pos":10,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"8ab541","input":"Data format: [batch_size, 1(grey) (3 if RGB but not applicable here), 28, 28]","metadata":{"id":"mpRX_NL4Ict7"},"pos":23,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"91cfc8","input":"## Convolutional Neural Network\n\nFirst we create two main groups at the beginning, the first convolutional group and the second convolutional group. Each convolutional group has a convolutional layer, an activation function (RELU) and a pooling layer. \n\nRemember, the convolutional layer passes multiple filters over each channel and can change the number of channels. Into the convolutional layer we pass in the number of input channels and the number of output channels. The kernel size is the size of our filter (e.g 5x5 filter) and the stride is how many pixels to the right we move everytime we apply a filter. Finally, the padding determines how many layers of zeroes we add to the border. \n\nThe second convolutional group (the one you have the code) is only different in the number of input and output channels of the convolutional layer. Create a convolutional layer with 16 input channels (output of the previous group) and 32 output channels. ","metadata":{"id":"HcwyOu_561Dl"},"pos":25,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"a6b051","input":"<h1>Creating Our Model</h1>","metadata":{"id":"cXZ3XWSbM5Z5"},"pos":22,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"b05f76","input":"## Dataloaders\n\nThe pytorch dataloader function creates an iterator that will give us one batch at a time as we iterate over the dataset. The parameters are self-explanatory, but the shuffle parameter shuffles the dataset. This is useful because we want to sample randomly and not go through all the data points from each classification one at a time. We wouldn't know the order of real world data. ","metadata":{"id":"DyE4L-A6bnaD"},"pos":14,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"b45946","input":"<h1>Evaluating our Model</h1>","metadata":{"id":"OcOQ9tIuM5Z9"},"pos":32,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"bebfdb","input":"Module docs - https://pytorch.org/docs/stable/generated/torch.nn.Module.html <br> nn docs - https://pytorch.org/docs/stable/nn.html","metadata":{"id":"JvjaQMVAM5Z6"},"pos":24,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"cdbef3","input":"transforms docs - https://pytorch.org/vision/stable/transforms.html\n\n","metadata":{"id":"E42dLkXJM5Zx"},"pos":8,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"d12a91","input":"## Evaluation Loop\nTurn off gradient descent using torch.no_grad. <br>\n&ensp; Iterate through every batch in the dataset<br>\n&ensp; Get the prediction of every image in the batch<br>\n&ensp; Add the number of images to the total<br>\n&ensp; Add the number of correctly classified images to a counter<br>\n&ensp; Get the accuracy through correct/total.<br>\n","metadata":{"id":"hJq4t5J_AvkE"},"pos":34,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"dcb833","input":"##### H2 = size of height after convolution\n##### W2 = size of width after convolution\n##### H1 = primitive height size\n##### FH = filter's height size\n##### W1 = primitive width size\n##### WH = filter's width size\n##### P = num padding\n##### S = stride","metadata":{"id":"3xjsirAPK5sL"},"pos":20,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"dd1cd6","input":"We first create a wrapper class for the pyplot imshow method, which is used to show RGB images on the screen. We first perform the opposite transformations as our initial normalization, to put the range of the pixels back between 0-1 (needed for pyplot. Next, we turn the tensor image into a numpy array and transpose the dimensions of image. \n\nWhy do we need to transpose the color channels? Because the tensor image is (28,28,3) so 28 by 28 which each unit having a third dimension of (R,G,B). We want it (3,28,28) so 3 distinct color channels with each channel being a different matrix (3 matrices). Finally, actually show the new image with plt.show(). \n\nThen we turn the train dataloader into an iterator and call the next() function to get the next batch of images and labels for display. Using the handy torch.utils.make_grid function, we can turn the images in the batch into one single row image, and at the end print out the classification/label for each image","metadata":{"id":"9H6KdGs7coRu"},"pos":18,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"e5601c","input":"# Applying a neural network to the Fashion MNIST dataset\n\nUsing a grayscale image of a clothing article, how can we classify the type of clothing article? E.g shirt, pants, etc...","metadata":{"id":"u0BK-Hd1M5Za"},"pos":0,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"e60793","input":"<h1>Preprocessing our Images</h1>\n\nThe transform that we will perform on our dataset is first converting all images to tensors. Tensors are the built in array datatype in pytorch, like numpy arrays. If you are interested, learn about why they are useful in keeping track of gradients here:\nhttps://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html .\n\nConverting to a tensor also converts an image with pixel values from 0 to 255 to a matrix with numbers from 0 to 1. \n\nIn addition, we are normalizing the data to a range between -1 and 1. If the range before is [0,1], subtracting 0.5 will give us [-0.5,0.5] and dividing by 0.5 will make the range wider to [-1,1]. torch.Normalize subtracts the first parameter from all the values in the image and divides by the second parameter. ","metadata":{"id":"Pz_N7lJRM5Zw"},"pos":7,"state":"done","type":"cell"}
{"id":0,"time":1657050794936,"type":"user"}
{"last_load":1656710286798,"type":"file"}